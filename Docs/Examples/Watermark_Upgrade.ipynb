{"cells":[{"cell_type":"code","source":["%pip install FabricSync --quiet"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"43d3b021-e0cc-4a7b-ab4c-5e700f8ac505"},{"cell_type":"code","source":["from delta import DeltaTable\n","from pyspark.sql.functions import lit, col, max\n","\n","from FabricSync.BQ.Model.Config import *\n","from FabricSync.BQ.Enum import *"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a582245-0796-48e1-85bb-bb4dd0cb6aba"},{"cell_type":"markdown","source":["### Configuration Set-Up\n","\n","Values should match the configuration provided. \n","\n","1. project_id - GCP project id\n","2. dataset - GCP dataset id\n","3. table_name - Table name\n","4. watermark_column - Table column to use for watermark\n","5. config_json_path - Path to configuration file"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"20e5e84e-4db8-43b0-897e-d08fee1167a4"},{"cell_type":"code","source":["project_id=\"<<GCP PROJECT ID>>\"\n","dataset=\"<<<GCP DATASET ID>>\"\n","table_name=\"<<<TABLE, VIEW, MATERIALIZED VIEW NAME>>>\"\n","watermark_column=\"<<WATERMARK COLUMN NAME>>>\"\n","\n","config_json_path = \"<<<PATH TO CONFIGURATION FILE>>>\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1e7a7973-eaa9-4d5b-983d-1de1c7b168e4"},{"cell_type":"code","source":["config = ConfigDataset.from_json(config_json_path)\n","\n","predicate = f\"sync_id='{config.ID}' AND project_id='{project_id}' AND dataset='{dataset}' AND table_name='{table_name}'\"\n","sync_cfg = spark.table(\"sync_configuration\").where(predicate)\n","\n","if sync_cfg.count() > 0:\n","    print(\"Updating configuration...\")\n","    c = next(sync_cfg.toLocalIterator(), None)\n","\n","    #For committed tables, get the max watermark\n","    if c[\"sync_state\"] == \"COMMIT\":\n","        print(\"Committed table setting watermark...\")\n","        sync_schedule = spark.table(\"sync_schedule\").where(predicate).orderBy(col(\"completed\").desc())\n","        s = next(sync_schedule.toLocalIterator(), None)\n","\n","        if s:\n","            df = spark.table(f\"{c['lakehouse']}.{c['lakehouse_table_name']}\")\n","            df = df.agg(max(watermark_column).alias(\"watermark\"))\n","\n","            w = next(df.toLocalIterator(), None)\n","\n","            if w:\n","                watermark = str(w[\"watermark\"])\n","                print(f\"Found max watermark: {watermark} ...\")  \n","\n","                deltaTable = DeltaTable.forName(spark, \"sync_schedule\")\n","                deltaTable.update(\n","                    condition = f\"sync_id='{config.ID}' AND schedule_id='{s['schedule_id']}'\",\n","                    set = { \n","                        'max_watermark': lit(watermark)\n","                    }\n","                )\n","    \n","    #Update sync_configuration metastore\n","    print(\"Updating sync_configuration...\")\n","    deltaTable = DeltaTable.forName(spark, \"sync_configuration\")\n","    deltaTable.update(\n","        condition = predicate,\n","        set = { \n","            'load_strategy': lit(SyncLoadStrategy.WATERMARK) ,\n","            'load_type': lit(SyncLoadType.APPEND),\n","            'watermark_column': lit(watermark_column)\n","        }\n","    )\n","\n","    #Update User Configuration File\n","    print(\"Updating user configuration...\")\n","    table = next((table for table in config.Tables if table.TableName == table_name), None)\n","\n","    if table:\n","        table.LoadStrategy = SyncLoadStrategy.WATERMARK\n","        table.LoadType = SyncLoadType.APPEND\n","        table.Keys = [ConfigTableColumn(column=watermark_column)]\n","\n","        config.Tables = [table if tbl.TableName == table.TableName else tbl for tbl in config.Tables]\n","        config.to_json(config_json_path)\n","\n","        print(\"Finished...\")\n","else:\n","    print(\"Configuration not found...\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"1ba75f40-709a-4a4f-816d-b83e5e80660c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}