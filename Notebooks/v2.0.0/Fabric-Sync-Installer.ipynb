{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6466d3e5-80a4-4a89-a9f8-74f49d6184a9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Fabric Sync Installer\n",
    "\n",
    "Before you begin:\n",
    "1. Attached this notebook to a lakehouse within your workspace \n",
    "    - If the workspace is new, create a new lakehouse for the Fabric Sync Metadata. For example: <code>FabricSync_Metdata</code>\n",
    "2. Start your sessions and walk-through the installer notebook\n",
    "3. Update the environment configuration data as required below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a4f49-058d-4216-97b0-d2f516441a6a",
   "metadata": {
    "editable": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "%pip install FabricSync --quiet --disable-pip-version-check --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547debc8-24e7-42a0-a750-15567a00204e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from FabricSync.BQ.Setup import Installer\n",
    "from FabricSync.BQ.Enum import FabricDestinationType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f2e921-5ddd-4b86-8e5f-8bc84aa76237",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Configure your Environment\n",
    "\n",
    "1. Loader Name - custom name for the data being loaded (ex: HR_Data_Sync, Sales_Data_Mirror)\n",
    "2. Metadata Lakehouse - name of the lakehouse used for Fabric Sync metadata (Created if it doesn't exists)\n",
    "3. Target Lakehouse - name of the lakehouse for the Fabric Sync mirrored data (Created if it doesn't exists)\n",
    "4. GCP Project ID - the GCP billing project id that contains the in-scope dataset\n",
    "5. GCP Dataset - the BQ dataset id containing the data to be synced\n",
    "6. GCP Service Credential JSON File\n",
    "    - Upload your GCP Credential file to the attached lakehouse\n",
    "    - The <code>gcp_credential_path</code> is the File API Path to your JSON credential file <br />\n",
    "    Example: <code>/lakehouse/default/Files/my-credential-file.json\"</code>\n",
    "7. Enable Schemas - flag to enable Fabric lakehouse schemas (Schemas REQUIRED for Mirrored Databases)\n",
    "8. Target Type - Fabric LAKEHOUSE or MIRRORED_DATABASE\n",
    "9. Create Spark Environment - flag to create a Fabric Spark environment as part of installation\n",
    "10. Spark Environment Name - name for Fabric Spark Environment item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a8854-71f4-4880-8f2b-3ea7a53cb50e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"loader_name\":\"Fabric_Sync\",\n",
    "    \"metadata_lakehouse\":\"<<<METADATA LAKEHOUSE NAME>>>\",\n",
    "    \"target_lakehouse\":\"<<<TARGET LAKEHOUSE NAME>>>\",\n",
    "    \"gcp_project_id\":\"<<<GCP PROJECT ID>>>\",\n",
    "    \"gcp_dataset_id\":\"<<<GCP DATASET ID>>>\",\n",
    "    \"gcp_credential_path\":\"<<<LAKEHOUSE PATH TO GCP CREDENTIALS>>>\",\n",
    "\n",
    "    \"enable_schemas\":False,\n",
    "    \"target_type\":FabricDestinationType.LAKEHOUSE,\n",
    "    \n",
    "    \"create_spark_environment\": False,\n",
    "    \"spark_environment_name\": \"<<<FABRIC SPARK ENVIRONMENT - IF ENABLED>>>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f82a6-3886-422c-a297-0354888f66f4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "credential_provider = notebookutils.credentials\n",
    "Installer(credential_provider).install(data)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {},
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
