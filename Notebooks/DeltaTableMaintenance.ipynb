{"cells":[{"cell_type":"code","source":["from pyspark.sql import *\n","from pyspark.sql.functions import * \n","from delta.tables import *\n","from typing import Tuple\n","\n","class DeltaTableMaintenance:\n","    \"\"\"\n","    Maintenance class that wraps  simple table metadata calls and \n","    all the heavy lifting for maintenance operations\n","\n","    1. Deep merge schema support:\n","        a. Detect schema changes that require a drop or data type update:\n","            - Drop column in destination\n","            - Update column data type in destination\n","        b. Drop table using file system shortcut\n","        c. Optimize\n","        d. Vacuum\n","\n","    \"\"\"\n","    __detail:Row = None\n","\n","    def __init__(self, table_nm:str):\n","        \"\"\"\n","        Init function, creates instance of DeltaTable class\n","        \"\"\"\n","        self.TableName = table_nm\n","        self.DeltaTable = DeltaTable.forName(spark, table_nm)\n","\n","    @property\n","    def CurrentTableVersion(self) -> int:\n","        \"\"\"\n","        Retrieves the max delta version for the table from the table history\n","        \"\"\"\n","        history = self.get_table_history() \\\n","            .select(max(col(\"version\")).alias(\"delta_version\"))\n","\n","        return [r[0] for r in history.collect()][0]\n","\n","    @property\n","    def Detail(self) -> DataFrame:\n","        \"\"\"\n","        Delta table metadata detail row\n","        \"\"\"\n","        if not self.__detail:\n","            self.__detail = self.DeltaTable.detail().collect()[0]\n","        \n","        return self.__detail\n","\n","\n","    @property\n","    def OneLakeLocation(self) -> str:\n","        \"\"\"\n","        OneLake file location for the table\n","        \"\"\"\n","        return self.Detail[\"location\"]\n","\n","    def get_table_history(self, only_current_day:bool = False) -> DataFrame:\n","        \"\"\"\n","        Gets the table history with optional filter for current day\n","        \"\"\"\n","        history = self.DeltaTable.history()\n","\n","        if only_current_day:\n","            history = history.filter(\"CAST(timestamp AS DATE) = current_date()\")\n","\n","        return history\n","\n","    def SchemaDiff(self, src: DataFrame, dest: DataFrame):\n","        \"\"\"\n","        Detects schema difference between the delta table and a source dataframe\n","        \"\"\"\n","        srcSchema = {x[0]:x[1] for x in src.dtypes}\n","        destSchema = {x[0]:x[1] for x in dest.dtypes}\n","            \n","        srcNotPresent = set(dest.columns) - set(src.columns)\n","        destNotPresent = set(src.columns) - set(dest.columns)\n","        \n","        diffSchema = {k:v for k,v in srcSchema.items() if k not in destNotPresent}\n","        \n","        typesChanged = {}\n","        for column_name in diffSchema:\n","            if diffSchema[column_name] != destSchema[column_name]:\n","                typesChanged[column_name] = srcSchema[column_name]\n","        \n","        \n","        return destNotPresent, srcNotPresent, typesChanged\n","    \n","    def evolve_schema(self, src:DataFrame):\n","        \"\"\"\n","        If schema differences are detected that between the source and delta table:\n","\n","        1. Drops required columns from the destination\n","        2. Updates column data types in the destination\n","\n","        Raises an error if a partitioning column is altered\n","        \"\"\"\n","        dest = self.DeltaTable.toDF()\n","\n","        new_columns, removed_columns, updated_columns  = self.SchemaDiff(src, dest)\n","\n","        if removed_columns or updated_columns:\n","            partition_columns = self.Detail[\"partitionColumns\"]\n","            tbl_properties = self.Detail[\"properties\"]\n","\n","            print(\"Resolving schema differences that require table overwrite...\")\n","            for c in removed_columns:\n","                if not c in partition_columns:\n","                    print(f\"Removing Column {c}...\")\n","                    dest = dest.drop(c)\n","                else:\n","                    raise Exception(\"Schema Evolution Violation: Partition Column cannot be dropped\")\n","\n","            for c in updated_columns:\n","                print(f\"Changing Column {c} type to {updated_columns[c]}...\")\n","                dest = dest.withColumn(c, col(c).cast(updated_columns[c]))\n","            \n","            write_options = {\"overwriteSchema\":\"true\"}\n","            write_options = {**write_options, **tbl_properties}\n","\n","            dest.write.options(**write_options) \\\n","                .partitionBy(partition_columns) \\\n","                .mode(\"overwrite\") \\\n","                .saveAsTable(\"test_table\")\n","    \n","    def drop_partition(self, partition_filter:str):\n","        \"\"\"\n","        Drops a partition of data from the delta table\n","        \"\"\"\n","        self.DeltaTable.delete(partition_filter)\n","\n","    def drop_table(self):\n","        \"\"\"\n","        Optimized table drop that directly deletes the table from storage\n","        \"\"\"\n","        mssparkutils.fs.rm(self.OneLakeLocation, recurse=True)\n","    \n","    def optimize_and_vacuum(self, partition_filter:str = None):\n","        \"\"\"\n","        Combined optimize and vacuum\n","        \"\"\"\n","        self.optimize(partition_filter)\n","        self.vacuum()\n","    \n","    def optimize(self, partition_filter:str = None):\n","        \"\"\"\n","        Table optimize to compact small files\n","        \"\"\"\n","        if partition_filter:\n","            self.DeltaTable.optimize().where(partition_filter).executeCompaction()\n","        else:\n","            self.DeltaTable.optimize().executeCompaction()\n","\n","    def vacuum(self):\n","        \"\"\"\n","        Sync vacuum with rentention forced to zero\n","        \"\"\"\n","        spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n","        self.DeltaTable.vacuum(0)\n","        spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n"],"outputs":[],"execution_count":null,"metadata":{},"id":"927021ff-5d84-4164-ab3a-e063049f1228"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"9c7403b4-606d-412c-9224-64ccdbff9cc3","default_lakehouse_name":"BQ_Metadata","default_lakehouse_workspace_id":"0fb2d96e-53e7-4c49-a594-beb0891ac121"},"environment":{"environmentId":"98eebb73-c49b-432b-bbff-85316ca4c796","workspaceId":"0fb2d96e-53e7-4c49-a594-beb0891ac121"}}},"nbformat":4,"nbformat_minor":5}