{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from delta.tables import *\n","from datetime import datetime, timezone\n","from json import JSONEncoder\n","import json\n","import base64\n","from pathlib import Path\n","import os\n","from typing import List, Tuple"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5205150e-59b3-44b3-932d-74e6715ee9a0","statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:31:27.9517562Z","session_start_time":null,"execution_start_time":"2024-04-18T13:31:28.3762968Z","execution_finish_time":"2024-04-18T13:31:28.732113Z","parent_msg_id":"4e9a8459-0d0c-41ce-9916-c95742a13f58"},"text/plain":"StatementMeta(, 5205150e-59b3-44b3-932d-74e6715ee9a0, 13, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{},"id":"4298ac5f-a210-45b0-a12a-349b9996cdc4"},{"cell_type":"code","source":["%run Config"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5205150e-59b3-44b3-932d-74e6715ee9a0","statement_id":19,"statement_ids":[14,15,16,17,18,19],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:31:28.043608Z","session_start_time":null,"execution_start_time":"2024-04-18T13:31:34.1891425Z","execution_finish_time":"2024-04-18T13:31:34.1893107Z","parent_msg_id":"26386b14-3f34-4c0e-bd27-96c879adc69b"},"text/plain":"StatementMeta(, 5205150e-59b3-44b3-932d-74e6715ee9a0, 19, Finished, Available)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0a3203b8-5c24-4acb-bc2f-e28d0d869526"},{"cell_type":"code","source":["%run DeltaTableMaintenance"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"560d356e-2961-4d97-8fa4-99e23f9d0a27"},{"cell_type":"code","source":["class ConfigMetadataLoader(ConfigBase):\n","    \"\"\"\n","    Class handles:\n","     \n","    1. Loads the table metadata from the BigQuery information schema tables to \n","        the Lakehouse Delta tables\n","    2. Autodetect table sync configuration based on defined metadata & heuristics\n","    \"\"\"\n","    def __init__(\n","            self, \n","            config_path : str):\n","        \"\"\"\n","        Calls the parent init to load the user config JSON file\n","        \"\"\"\n","        self.JSON_Config_Path = config_path\n","        \n","        super().__init__(config_path)\n","        spark.sql(f\"USE {self.UserConfig.MetadataLakehouse}\")\n","    \n","    def create_autodetect_view(self):\n","        \"\"\"\n","        Creates the autodetect temporary view that uses the BigQuery table metadata\n","        to determine default sync configuration based on defined heuristics\n","        \"\"\"\n","        sql = \"\"\"\n","        CREATE OR REPLACE TEMPORARY VIEW bq_table_metadata_autodetect\n","        AS\n","        WITH pkeys AS (    \n","            SELECT\n","                c.table_catalog, c.table_schema, c.table_name, \n","                k.column_name AS pk_col\n","            FROM bq_information_schema_table_constraints c\n","            JOIN bq_information_schema_key_column_usage k ON\n","                k.table_catalog = c.table_catalog AND\n","                k.table_schema = c.table_schema AND\n","                k.table_name = c.table_name AND\n","                k.constraint_name = c.constraint_name\n","            JOIN bq_information_schema_columns n ON\n","                n.table_catalog = k.table_catalog AND\n","                n.table_schema = k.table_schema AND\n","                n.table_name = k.table_name AND\n","                n.column_name = k.column_name\n","            JOIN bq_data_type_map m ON n.data_type = m.data_type\n","            WHERE c.constraint_type = 'PRIMARY KEY'\n","            AND m.is_watermark = 'YES'\n","        ),\n","        pkeys_cnt AS (\n","            SELECT \n","                table_catalog, table_schema, table_name, \n","                COUNT(*) as pk_cnt\n","            FROM pkeys\n","            GROUP BY table_catalog, table_schema, table_name\n","        ),\n","        watermark_cols AS (\n","            SELECT \n","                k.*\n","            FROM pkeys k\n","            JOIN pkeys_cnt c ON \n","                k.table_catalog = c.table_catalog AND\n","                k.table_schema = c.table_schema AND\n","                k.table_name = c.table_name\n","            WHERE c.pk_cnt = 1\n","        ),\n","        partitions AS (\n","            SELECT\n","                table_catalog, table_schema, table_name, \n","                count(*) as partition_count,\n","                avg(len(partition_id)) AS partition_id_len,\n","                sum(case when partition_id is NULL then 1 else 0 end) as null_partition_count\n","            FROM bq_information_schema_partitions\n","            GROUP BY table_catalog, table_schema, table_name\n","        ), \n","        partition_columns AS\n","        (\n","            SELECT\n","                table_catalog, table_schema, table_name,\n","                column_name, c.data_type,\n","                m.partition_type AS partitioning_type\n","            FROM bq_information_schema_columns c\n","            JOIN bq_data_type_map m ON c.data_type=m.data_type\n","            WHERE is_partitioning_column = 'YES'\n","        ),\n","        partition_cfg AS\n","        (\n","            SELECT\n","                p.*,\n","                CASE WHEN p.partition_count = 1 AND p.null_partition_count = 1 THEN FALSE ELSE TRUE END AS is_partitioned,\n","                c.column_name AS partition_col,\n","                c.data_type AS partition_data_type,\n","                c.partitioning_type,\n","                CASE WHEN (c.partitioning_type = 'TIME')\n","                    THEN \n","                        CASE WHEN (partition_id_len = 4) THEN 'YEAR'\n","                            WHEN (partition_id_len = 6) THEN 'MONTH'\n","                            WHEN (partition_id_len = 8) THEN 'DAY'\n","                            WHEN (partition_id_len = 10) THEN 'HOUR'\n","                            ELSE NULL END\n","                    ELSE NULL END AS partitioning_strategy\n","            FROM partitions p\n","            LEFT JOIN partition_columns c ON \n","                p.table_catalog = c.table_catalog AND\n","                p.table_schema = c.table_schema AND\n","                p.table_name = c.table_name\n","        )\n","\n","        SELECT \n","            t.table_catalog, t.table_schema, t.table_name, t.is_insertable_into,\n","            p.is_partitioned, p.partition_col, p.partition_data_type, p.partitioning_type, p.partitioning_strategy,\n","            w.pk_col\n","        FROM bq_information_schema_tables t\n","        LEFT JOIN watermark_cols w ON \n","            t.table_catalog = w.table_catalog AND\n","            t.table_schema = w.table_schema AND\n","            t.table_name = w.table_name\n","        LEFT JOIN partition_cfg p ON\n","            t.table_catalog = p.table_catalog AND\n","            t.table_schema = p.table_schema AND\n","            t.table_name = p.table_name\n","        \"\"\"\n","\n","        spark.sql(sql)\n","\n","    def sync_bq_information_schema_tables(self):\n","        \"\"\"\n","        Reads the INFORMATION_SCHEMA.TABLES from BigQuery for the configuration project_id \n","        and dataset returning only BASE TABLEs. Writes the results to the configured \n","        Metadata Lakehouse using a unique name based on project_id and dataset to allow \n","        for multiple datasets to be tracked independently.\n","        \"\"\"\n","        bq_table = self.UserConfig.get_bq_table_fullname(SyncConstants.INFORMATION_SCHEMA_TABLES)\n","        tbl_nm = self.UserConfig.flatten_3part_tablename(SyncConstants.INFORMATION_SCHEMA_TABLES.replace(\".\", \"_\"))\n","\n","        bql = f\"\"\"\n","        SELECT *\n","        FROM {bq_table}\n","        WHERE table_type='BASE TABLE'\n","        AND table_name NOT LIKE '_bqc_%'\n","        \"\"\"\n","\n","        df = self.read_bq_to_dataframe(bql)\n","\n","        if not self.UserConfig.LoadAllTables:\n","            filter_list = self.UserConfig.get_table_name_list()\n","            df = df.filter(col(\"table_name\").isin(filter_list))    \n","\n","        self.write_lakehouse_table(df, self.UserConfig.MetadataLakehouse, tbl_nm)\n","\n","    def sync_bq_information_schema_table_dependent(\n","            self, \n","            dependent_tbl : str):\n","        \"\"\"\n","        Reads a child INFORMATION_SCHEMA table from BigQuery for the configuration project_id \n","        and dataset. The child table is joined to the TABLES table to filter for BASE TABLEs.\n","        Writes the results to the configured Fabric Metadata Lakehouse using a unique \n","        name based on project_id and dataset to allow for multiple datasets to be tracked independently.\n","        \"\"\"\n","        bq_table = self.UserConfig.get_bq_table_fullname(SyncConstants.INFORMATION_SCHEMA_TABLES)\n","        bq_dependent_tbl = self.UserConfig.get_bq_table_fullname(dependent_tbl)\n","        tbl_nm = self.UserConfig.flatten_3part_tablename(dependent_tbl.replace(\".\", \"_\"))\n","\n","        bql = f\"\"\"\n","        SELECT c.*\n","        FROM {bq_dependent_tbl} c\n","        JOIN {bq_table} t ON \n","        t.table_catalog=c.table_catalog AND\n","        t.table_schema=c.table_schema AND\n","        t.table_name=c.table_name\n","        WHERE t.table_type='BASE TABLE'\n","        AND t.table_name NOT LIKE '_bqc_%'\n","        \"\"\"\n","\n","        df = self.read_bq_to_dataframe(bql)\n","\n","        if not self.UserConfig.LoadAllTables:\n","            filter_list = self.UserConfig.get_table_name_list()\n","            df = df.filter(col(\"table_name\").isin(filter_list)) \n","\n","        self.write_lakehouse_table(df, self.UserConfig.MetadataLakehouse, tbl_nm)\n","\n","    def sync_bq_metadata(self):\n","        \"\"\"\n","        Loads the required INFORMATION_SCHEMA tables from BigQuery:\n","\n","        1. TABLES\n","        2. PARTITIONS\n","        3. COLUMNS\n","        4. TABLE_CONSTRAINTS\n","        5. KEY_COLUMN_USAGE\n","        \"\"\"\n","        self.sync_bq_information_schema_tables()\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_PARTITIONS)\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_COLUMNS)\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_TABLE_CONSTRAINTS)\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_KEY_COLUMN_USAGE)\n","\n","    def create_proxy_views(self):\n","        \"\"\"\n","        Create the proxy views required to handle multiple project_id and datasets in the same lakehouse\n","        \"\"\"\n","        super().create_proxy_views()\n","\n","        if not spark.catalog.tableExists(\"bq_table_metadata_autodetect\"):\n","            self.create_autodetect_view()\n","\n","    def auto_detect_table_profiles(self):\n","        \"\"\"\n","        The autodetect provided the following capabilities:\n","         \n","        1. Uses the BigQuery metadata to determine a default config for each table\n","        2. If a user-defined table configuration is supplied it overrides the default configuration\n","        3. Write the configuration when the configuration is not locked\n","            a. The load configuration doesn't support changes without a reload of the data.\n","            b. The only changes that are support for locked configurations are:\n","                - Enabling and Disabling the table sync\n","                - Changing the table load Priority\n","                - Updating the table load Interval\n","        \"\"\"  \n","\n","        self.create_proxy_views()\n","\n","        sql = f\"\"\"\n","        WITH default_config AS (\n","            SELECT autodetect, target_lakehouse FROM user_config_json\n","        ),\n","        pk AS (\n","            SELECT\n","            a.table_catalog, a.table_schema, a.table_name, array_agg(COALESCE(a.pk_col, u.column)) as pk\n","            FROM bq_table_metadata_autodetect a\n","            LEFT JOIN user_config_table_keys u ON\n","                a.table_catalog = u.project_id AND\n","                a.table_schema = u.dataset AND\n","                a.table_name = u.table_name\n","            GROUP BY a.table_catalog, a.table_schema, a.table_name\n","        ),\n","        source AS (\n","            SELECT\n","                a.table_catalog as project_id,\n","                a.table_schema as dataset,\n","                a.table_name as table_name,\n","                COALESCE(u.enabled, TRUE) AS enabled,\n","                COALESCE(u.lakehouse, d.target_lakehouse) AS lakehouse,\n","                COALESCE(u.lakehouse_target_table, a.table_name) AS lakehouse_table_name,\n","                COALESCE(u.source_query, '') AS source_query,\n","                COALESCE(u.load_priority, '100') AS priority,\n","                CASE WHEN (COALESCE(u.watermark_column, a.pk_col) IS NOT NULL AND\n","                        COALESCE(u.watermark_column, a.pk_col) <> '') THEN 'WATERMARK' \n","                    WHEN (COALESCE(u.partition_enabled, a.is_partitioned) = TRUE) \n","                        AND COALESCE(u.partition_column, a.partition_col, '') NOT IN \n","                            ('_PARTITIONTIME', '_PARTITIONDATE') THEN 'PARTITION'\n","                    WHEN (COALESCE(u.partition_enabled, a.is_partitioned) = TRUE) \n","                        AND COALESCE(u.partition_column, a.partition_col, '') IN \n","                            ('_PARTITIONTIME', '_PARTITIONDATE') THEN 'TIME_INGESTION'\n","                    ELSE 'FULL' END AS load_strategy,\n","                CASE WHEN (COALESCE(u.watermark_column, a.pk_col) IS NOT NULL AND\n","                        COALESCE(u.watermark_column, a.pk_col) <> '') THEN 'APPEND' ELSE\n","                    'OVERWRITE' END AS load_type,\n","                COALESCE(u.interval, 'AUTO') AS interval,\n","                p.pk AS primary_keys,\n","                COALESCE(u.partition_enabled, a.is_partitioned) AS is_partitioned,\n","                COALESCE(u.partition_column, a.partition_col, '') AS partition_column,\n","                COALESCE(u.partition_type, a.partitioning_type, '') AS partition_type,\n","                COALESCE(u.partition_grain, a.partitioning_strategy, '') AS partition_grain,\n","                COALESCE(u.watermark_column, a.pk_col, '') AS watermark_column, \n","                d.autodetect,\n","                COALESCE(u.enforce_partition_expiration, FALSE) AS enforce_partition_expiration,\n","                COALESCE(u.enable_deletion_vectors, FALSE) AS enable_deletion_vectors,\n","                COALESCE(u.allow_schema_evoluton, FALSE) AS allow_schema_evoluton,\n","                COALESCE(u.table_maintenance_enabled, FALSE) AS table_maintenance_enabled,\n","                COALESCE(u.table_maintenance_interval, 'AUTO') AS table_maintenance_interval,\n","                CASE WHEN u.table_name IS NULL THEN FALSE ELSE TRUE END AS config_override,\n","                'INIT' AS sync_state,\n","                CURRENT_TIMESTAMP() as created_dt,\n","                NULL as last_updated_dt\n","            FROM bq_table_metadata_autodetect a\n","            JOIN pk p ON\n","                a.table_catalog = p.table_catalog AND\n","                a.table_schema = p.table_schema AND\n","                a.table_name = p.table_name\n","            LEFT JOIN user_config_tables u ON \n","                a.table_catalog = u.project_id AND\n","                a.table_schema = u.dataset AND\n","                a.table_name = u.table_name\n","            CROSS JOIN default_config d\n","        )\n","\n","        MERGE INTO {SyncConstants.SQL_TBL_SYNC_CONFIG} t\n","        USING source s\n","        ON t.project_id = s.project_id AND\n","            t.dataset = s.dataset AND\n","            t.table_name = s.table_name\n","        WHEN MATCHED AND t.sync_state <> 'INIT' THEN\n","            UPDATE SET\n","                t.enabled = s.enabled,\n","                t.interval = s.interval,\n","                t.priority = s.priority,\n","                t.enforce_partition_expiration = s.enforce_partition_expiration,\n","                t.enable_deletion_vectors = s.enable_deletion_vectors,\n","                t.allow_schema_evoluton = s.allow_schema_evoluton,\n","                t.table_maintenance_enabled = s.table_maintenance_enabled,\n","                t.table_maintenance_interval = s.table_maintenance_interval,\n","                t.last_updated_dt = CURRENT_TIMESTAMP()\n","        WHEN MATCHED AND t.sync_state = 'INIT' THEN\n","            UPDATE SET *\n","        WHEN NOT MATCHED THEN\n","            INSERT *\n","        \"\"\"\n","\n","        spark.sql(sql)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5205150e-59b3-44b3-932d-74e6715ee9a0","statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:31:28.1403909Z","session_start_time":null,"execution_start_time":"2024-04-18T13:31:34.6731685Z","execution_finish_time":"2024-04-18T13:31:35.0833974Z","parent_msg_id":"b10d5ef1-e0cc-45f2-bc9a-d376446e78e5"},"text/plain":"StatementMeta(, 5205150e-59b3-44b3-932d-74e6715ee9a0, 20, Finished, Available)"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b30797b9-49eb-43fc-b7bb-95f358014a57"},{"cell_type":"code","source":["class SyncSetup(ConfigBase):\n","    \"\"\"\n","    Configuration-driven utility to set-up the BQ Sync environment in the Fabric Lakehouse\n","\n","    1. Creates the Metadata & Target Lakehouse if they do not exists\n","    2. Drops & Recreates the required metadata and any supporting tables required\n","    \"\"\"\n","    def __init__(\n","            self, \n","            config_path : str):\n","        \"\"\"\n","        Calls the parent init to load the User Config JSON file\n","        \"\"\"\n","        if spark.catalog.tableExists(\"user_config_json\"):\n","            spark.catalog.dropTempView(\"user_config_json\")\n","\n","        super().__init__(config_path)\n","\n","    def get_fabric_lakehouse(\n","            self, \n","            nm : str):\n","        \"\"\"\n","        Returns a Fabric Lakehouse by name or None if it does not exists\n","        \"\"\"\n","        lakehouse = None\n","\n","        try:\n","            lakehouse = mssparkutils.lakehouse.get(nm)\n","        except Exception:\n","            print(\"Lakehouse not found: {0}\".format(nm))\n","\n","        return lakehouse\n","\n","    def create_fabric_lakehouse(\n","            self, \n","            nm : str):\n","        \"\"\"\n","        Creates a Fabric Lakehouse if it does not exists\n","        \"\"\"\n","        lakehouse = self.get_fabric_lakehouse(nm)\n","\n","        if (lakehouse is None):\n","            print(\"Creating Lakehouse {0}...\".format(nm))\n","            mssparkutils.lakehouse.create(nm)\n","\n","    def setup(self):\n","        \"\"\"\n","        Set-up method to ensure required Lakehouse exists and create required tables\n","        \"\"\"\n","        self.create_fabric_lakehouse(self.UserConfig.MetadataLakehouse)\n","        self.create_fabric_lakehouse(self.UserConfig.TargetLakehouse)\n","        spark.sql(f\"USE {self.UserConfig.MetadataLakehouse}\")\n","        self.create_all_tables()\n","\n","    def drop_table(\n","            self, \n","            tbl : str):\n","        \"\"\"\n","        Drops an existing table from the Lakehouse if it exists\n","        \"\"\"\n","        sql = f\"DROP TABLE IF EXISTS {tbl}\"\n","        spark.sql(sql)\n","\n","    def get_tbl_name(\n","            self, \n","            tbl : str) -> str:\n","        \"\"\"\n","        Returns the table name with two-part format for the configuration Metadata Lakehouse\n","        \"\"\"\n","        return self.UserConfig.get_lakehouse_tablename(self.UserConfig.MetadataLakehouse, tbl)\n","\n","    def create_data_type_map_tbl(self):\n","        \"\"\"\n","        Creates the BQ Data Type Mapping table and loads the default data. The csv data must be supplied in the \n","        the following path of the configured Metadata Lakehouse:\n","\n","        Files/data/bq_data_types.csv\n","        \"\"\"\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_DATA_TYPE_MAP)\n","        self.drop_table(tbl_nm)\n","\n","        sql = f\"\"\"CREATE TABLE IF NOT EXISTS {tbl_nm} (data_type STRING, partition_type STRING, is_watermark STRING)\"\"\"\n","        spark.sql(sql)\n","\n","        df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/data/bq_data_types.csv\")\n","        df.write.mode(\"OVERWRITE\").saveAsTable(tbl_nm)\n","\n","    def create_sync_config_tbl(self):\n","        \"\"\"\n","        Create the BQ Sync Configuration metadata table\n","        \"\"\"\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_SYNC_CONFIG)\n","        self.drop_table(tbl_nm)\n","\n","        sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {tbl_nm}\n","        (\n","            project_id STRING,\n","            dataset STRING,\n","            table_name STRING,\n","            enabled BOOLEAN,\n","            lakehouse STRING,\n","            lakehouse_table_name STRING,\n","            source_query STRING,\n","            priority INTEGER,\n","            load_strategy STRING,\n","            load_type STRING,\n","            interval STRING,\n","            primary_keys ARRAY<STRING>,\n","            is_partitioned BOOLEAN,\n","            partition_column STRING,\n","            partition_type STRING,\n","            partition_grain STRING,\n","            watermark_column STRING,\n","            autodetect BOOLEAN,\n","            enforce_partition_expiration BOOLEAN,\n","            enable_deletion_vectors BOOLEAN,\n","            allow_schema_evoluton BOOLEAN,\n","            table_maintenance_enabled BOOLEAN,\n","            table_maintenance_interval STRING,\n","            config_override BOOLEAN,\n","            sync_state STRING,\n","            created_dt TIMESTAMP,\n","            last_updated_dt TIMESTAMP\n","        )\n","        \"\"\"\n","        spark.sql(sql)\n","    \n","    def create_sync_schedule_tbl(self):\n","        \"\"\"\n","        Create the BQ Sync Schedule metadata table\n","        \"\"\"\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_SYNC_SCHEDULE)\n","        self.drop_table(tbl_nm)\n","\n","        sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {tbl_nm} (\n","            group_schedule_id STRING,\n","            schedule_id STRING,\n","            project_id STRING,\n","            dataset STRING,\n","            table_name STRING,\n","            scheduled TIMESTAMP,\n","            status STRING,\n","            started TIMESTAMP,\n","            completed TIMESTAMP,\n","            completed_activities INT,\n","            failed_activities INT,\n","            max_watermark STRING,\n","            priority INTEGER\n","        )\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def create_sync_schedule_telemetry_tbl(self):\n","        \"\"\"\n","        Create the BQ Sync Schedule Telemetry metadata table\n","        \"\"\"\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_SYNC_SCHEDULE_TELEMETRY)\n","        self.drop_table(tbl_nm)\n","\n","        sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {tbl_nm} (\n","            schedule_id STRING,\n","            project_id STRING,\n","            dataset STRING,\n","            table_name STRING,\n","            partition_id STRING,\n","            status STRING,\n","            started TIMESTAMP,\n","            completed TIMESTAMP,\n","            src_row_count BIGINT,\n","            dest_row_count BIGINT,\n","            inserted_row_count BIGINT,\n","            updated_row_count BIGINT,\n","            delta_version BIGINT,\n","            spark_application_id STRING,\n","            max_watermark STRING,\n","            summary_load STRING\n","        )\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def create_all_tables(self):\n","        \"\"\"\n","        Create all required metadata tables\n","        \"\"\"\n","        self.create_data_type_map_tbl()\n","        self.create_sync_config_tbl()\n","        self.create_sync_schedule_tbl()\n","        self.create_sync_schedule_telemetry_tbl()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5205150e-59b3-44b3-932d-74e6715ee9a0","statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:31:28.3417402Z","session_start_time":null,"execution_start_time":"2024-04-18T13:31:35.5790274Z","execution_finish_time":"2024-04-18T13:31:36.0398046Z","parent_msg_id":"7e635774-81fe-4f8b-95aa-9b11351e51b7"},"text/plain":"StatementMeta(, 5205150e-59b3-44b3-932d-74e6715ee9a0, 21, Finished, Available)"},"metadata":{}}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"10d7ab50-ea8e-4aa6-9945-3d0081a84def"},{"cell_type":"code","source":["class Scheduler(ConfigBase):\n","    \"\"\"\n","    Class responsible for calculating the to-be run schedule based on the sync config and \n","    the most recent BigQuery table metadata. Schedule is persisted to the Sync Schedule\n","    Delta table. When tables are scheduled but no updates are detected on the BigQuery side \n","    a SKIPPED record is created for tracking purposes.\n","    \"\"\"\n","    def __init__(\n","            self, \n","            config_path : str):\n","        \"\"\"\n","        Calls the parent init to load the user config JSON file\n","        \"\"\"\n","        super().__init__(config_path)\n","        spark.sql(f\"USE {self.UserConfig.MetadataLakehouse}\")\n","\n","    def run(self):\n","        \"\"\"\n","        Process responsible for creating and saving the sync schedule\n","        \"\"\"\n","        sql = f\"\"\"\n","        WITH new_schedule AS ( \n","            SELECT UUID() AS group_schedule_id, CURRENT_TIMESTAMP() as scheduled\n","        ),\n","        last_bq_tbl_updates AS (\n","            SELECT table_catalog, table_schema, table_name, max(last_modified_time) as last_bq_tbl_update\n","            FROM bq_information_schema_partitions\n","            GROUP BY table_catalog, table_schema, table_name\n","        ),\n","        last_load AS (\n","            SELECT project_id, dataset, table_name, MAX(started) AS last_load_update\n","            FROM {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","            WHERE status='COMPLETE'\n","            GROUP BY project_id, dataset, table_name\n","        ),\n","        schedule AS (\n","            SELECT\n","                n.group_schedule_id,\n","                UUID() AS schedule_id,\n","                c.project_id,\n","                c.dataset,\n","                c.table_name,\n","                n.scheduled,\n","                CASE WHEN ((l.last_load_update IS NULL) OR\n","                     (b.last_bq_tbl_update >= l.last_load_update))\n","                    THEN 'SCHEDULED' ELSE 'SKIPPED' END as status,\n","                NULL as started,\n","                NULL as completed,   \n","                NULL as completed_activities,\n","                NULL as failed_activities,\n","                NULL as max_watermark,\n","                c.priority                \n","            FROM {SyncConstants.SQL_TBL_SYNC_CONFIG} c \n","            LEFT JOIN {SyncConstants.SQL_TBL_SYNC_SCHEDULE} s ON \n","                c.project_id= s.project_id AND\n","                c.dataset = s.dataset AND\n","                c.table_name = s.table_name AND\n","                s.status = 'SCHEDULED'\n","            LEFT JOIN last_bq_tbl_updates b ON\n","                c.project_id= b.table_catalog AND\n","                c.dataset = b.table_schema AND\n","                c.table_name = b.table_name\n","            LEFT JOIN last_load l ON \n","                c.project_id= l.project_id AND\n","                c.dataset = l.dataset AND\n","                c.table_name = l.table_name\n","            CROSS JOIN new_schedule n\n","            WHERE s.schedule_id IS NULL\n","            AND c.enabled = TRUE\n","        )\n","\n","        INSERT INTO {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","        SELECT * FROM schedule s\n","        WHERE s.project_id = '{self.UserConfig.ProjectID}'\n","        AND s.dataset = '{self.UserConfig.Dataset}'\n","        \"\"\"\n","        spark.sql(sql)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5205150e-59b3-44b3-932d-74e6715ee9a0","statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:31:28.4281093Z","session_start_time":null,"execution_start_time":"2024-04-18T13:31:36.6374668Z","execution_finish_time":"2024-04-18T13:31:37.0186809Z","parent_msg_id":"43282319-ac05-45b1-8862-5113fd5b6bfc"},"text/plain":"StatementMeta(, 5205150e-59b3-44b3-932d-74e6715ee9a0, 22, Finished, Available)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c68ac0fb-bac4-43e9-b4ac-cfe1052fa7bb"},{"cell_type":"code","source":["class BQScheduleLoader(ConfigBase):\n","    \"\"\"\n","    Class repsonsible for processing the sync schedule and handling data movement \n","    from BigQuery to Fabric Lakehouse based on each table's configuration\n","    \"\"\"\n","    def __init__(\n","            self, \n","            config_path : str, \n","            load_proxy_views : bool =True, \n","            force_config_reload : bool = False):\n","        \"\"\"\n","        Calls parent init to load User Config from JSON file\n","        \"\"\"\n","        super().__init__(config_path, force_config_reload)\n","        spark.sql(f\"USE {self.UserConfig.MetadataLakehouse}\")\n","\n","        if load_proxy_views:\n","            super().create_proxy_views()\n","\n","    def save_schedule_telemetry(\n","            self, \n","            schedule : SyncSchedule):\n","        \"\"\"\n","        Write status and telemetry from sync schedule to Sync Schedule Telemetry Delta table\n","        \"\"\"\n","        tbl = f\"{SyncConstants.SQL_TBL_SYNC_SCHEDULE_TELEMETRY}\"\n","\n","        schema = spark.table(tbl).schema\n","\n","        rdd = spark.sparkContext.parallelize([Row(\n","            schedule_id=schedule.ScheduleId,\n","            project_id=schedule.ProjectId,\n","            dataset=schedule.Dataset,\n","            table_name=schedule.TableName,\n","            partition_id=schedule.PartitionId,\n","            status=\"COMPLETE\",\n","            started=schedule.StartTime,\n","            completed=schedule.EndTime,\n","            src_row_count=schedule.SourceRows,\n","            dest_row_count=schedule.DestRows,\n","            inserted_row_count=schedule.InsertedRows,\n","            updated_row_count=schedule.UpdatedRows,\n","            delta_version=schedule.DeltaVersion,\n","            spark_application_id=schedule.SparkAppId,\n","            max_watermark=schedule.MaxWatermark,\n","            summary_load=schedule.SummaryLoadType\n","        )])\n","\n","        df = spark.createDataFrame(rdd, schema)\n","        df.write.mode(SyncConstants.APPEND).saveAsTable(tbl)\n","\n","    def get_delta_merge_row_counts(\n","            self, \n","            schedule:SyncSchedule) -> Tuple[int, int, int]:\n","        \"\"\"\n","        Gets the rows affected by merge operation, filters on partition id when table is partitioned\n","        \"\"\"\n","        telemetry = spark.sql(f\"DESCRIBE HISTORY {schedule.LakehouseTableName}\")\n","\n","        telemetry = telemetry \\\n","            .filter(\"operation = 'MERGE' AND CAST(timestamp AS DATE) = current_date()\") \\\n","            .orderBy(\"version\", ascending=False)\n","\n","        inserts = 0\n","        updates = 0\n","        deletes = 0\n","\n","        for t in telemetry.collect():\n","            op_metrics = None\n","\n","            if schedule.FabricPartitionColumn and schedule.PartitionId:\n","                if \"predicate\" in t[\"operationParameters\"] and \\\n","                    schedule.PartitionId in t[\"operationParameters\"][\"predicate\"]:\n","                        op_metrics = t[\"operationMetrics\"]\n","            else:\n","                op_metrics = t[\"operationMetrics\"]\n","\n","            if op_metrics:\n","                inserts = int(op_metrics[\"numTargetRowsInserted\"])\n","                updates = int(op_metrics[\"numTargetRowsUpdated\"])\n","                deletes = int(op_metrics[\"numTargetRowsDeleted\"])\n","\n","                continue\n","\n","        return (inserts, updates, deletes)\n","    \n","    def get_schedule(self):\n","        \"\"\"\n","        Gets the schedule activities that need to be run based on the configuration and metadat\n","        \"\"\"\n","        sql = f\"\"\"\n","        WITH last_completed_schedule AS (\n","            SELECT schedule_id, project_id, dataset, table_name, max_watermark, started AS last_schedule_dt\n","            FROM (\n","                SELECT schedule_id, project_id, dataset, table_name, started, max_watermark,\n","                ROW_NUMBER() OVER(PARTITION BY project_id, dataset, table_name ORDER BY scheduled DESC) AS row_num\n","                FROM {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","                WHERE status='COMPLETE'\n","            )\n","            WHERE row_num = 1\n","        ),\n","        tbl_partitions AS (\n","            SELECT\n","                sp.table_catalog, sp.table_schema, sp.table_name, sp.partition_id\n","            FROM bq_information_schema_partitions sp\n","            JOIN {SyncConstants.SQL_TBL_SYNC_CONFIG} c ON\n","                sp.table_catalog = c.project_id AND \n","                sp.table_schema = c.dataset AND\n","                sp.table_name = c.table_name\n","            LEFT JOIN last_completed_schedule s ON \n","                sp.table_catalog = s.project_id AND \n","                sp.table_schema = s.dataset AND\n","                sp.table_name = s.table_name\n","            WHERE ((sp.last_modified_time >= s.last_schedule_dt) OR (s.last_schedule_dt IS NULL))\n","            AND \n","                ((c.load_strategy = 'PARTITION' AND s.last_schedule_dt IS NOT NULL) OR\n","                    c.load_strategy = 'TIME_INGESTION')\n","        )\n","\n","        SELECT c.*, \n","            p.partition_id,\n","            s.group_schedule_id,\n","            s.schedule_id,\n","            h.max_watermark,\n","            h.last_schedule_dt,\n","            CASE WHEN (h.schedule_id IS NULL) THEN TRUE ELSE FALSE END AS initial_load\n","        FROM {SyncConstants.SQL_TBL_SYNC_CONFIG} c\n","        JOIN {SyncConstants.SQL_TBL_SYNC_SCHEDULE} s ON \n","            c.project_id = s.project_id AND\n","            c.dataset = s.dataset AND\n","            c.table_name = s.table_name\n","        LEFT JOIN last_completed_schedule h ON\n","            c.project_id = h.project_id AND\n","            c.dataset = h.dataset AND\n","            c.table_name = h.table_name\n","        LEFT JOIN tbl_partitions p ON\n","            p.table_catalog = c.project_id AND \n","            p.table_schema = c.dataset AND\n","            p.table_name = c.table_name\n","        LEFT JOIN {SyncConstants.SQL_TBL_SYNC_SCHEDULE_TELEMETRY} t ON\n","            s.schedule_id = t.schedule_id AND\n","            c.project_id = t.project_id AND\n","            c.dataset = t.dataset AND\n","            c.table_name = t.table_name AND\n","            COALESCE(p.partition_id, '0') = COALESCE(t.partition_id, '0') AND\n","            t.status = 'COMPLETE'\n","        WHERE s.status = 'SCHEDULED'\n","            AND c.enabled = TRUE\n","            AND t.schedule_id IS NULL\n","            AND c.project_id = '{self.UserConfig.ProjectID}' \n","            AND c.dataset = '{self.UserConfig.Dataset}'\n","        ORDER BY c.priority\n","        \"\"\"\n","        df = spark.sql(sql)\n","        df.createOrReplaceTempView(\"LoaderQueue\")\n","        df.cache()\n","\n","        return df\n","\n","    def get_max_watermark(\n","            self, \n","            lakehouse_tbl : str, \n","            watermark_col : str) -> str:\n","        \"\"\"\n","        Get the max value for the supplied table and column\n","        \"\"\"\n","        df = spark.table(lakehouse_tbl) \\\n","            .select(max(col(watermark_col)).alias(\"watermark\"))\n","\n","        for row in df.collect():\n","            return row[\"watermark\"]\n","\n","    def get_bq_partition_date_format(\n","            self, \n","            schedule:SyncSchedule) -> str:\n","        \"\"\"\n","        Resolve the BigQuery datetime format based on the partition grain\n","        \"\"\"\n","        part_format = None\n","\n","        match schedule.PartitionGrain:\n","            case \"DAY\":\n","                part_format = \"%Y%m%d\"\n","            case \"MONTH\":\n","                part_format = \"%Y%m\"\n","            case \"YEAR\":\n","                part_format = \"%Y\"\n","            case \"HOUR\":\n","                part_format = \"%Y%m%d%H\"\n","            case _:\n","                raise Exception(\"Unsupported Partition Grain in Table Config\")\n","        \n","        return part_format\n","\n","    def resolve_fabric_partition_column(\n","            self, \n","            schedule:SyncSchedule, \n","            df_bq:DataFrame) -> Tuple[str, DataFrame]:\n","        \"\"\"\n","        Resolves the fabric partition approach using a proxy column when required\n","        \"\"\"\n","        part_format = \"\"\n","        part_col_name = f\"__bq_part_{schedule.PartitionColumn}\"\n","        use_proxy_col = False\n","        partition = None\n","\n","        match schedule.PartitionGrain:\n","            case \"DAY\":\n","                part_format = \"yyyyMMdd\"\n","\n","                if dict(df_bq.dtypes)[schedule.PartitionColumn] == \"date\":\n","                    partition = schedule.PartitionColumn\n","                else:\n","                    partition = f\"{part_col_name}_DAY\"\n","                    use_proxy_col = True\n","            case \"MONTH\":\n","                part_format = \"yyyyMM\"\n","                partition = f\"{part_col_name}_MONTH\"\n","                use_proxy_col = True\n","            case \"YEAR\":\n","                part_format = \"yyyy\"\n","                partition = f\"{part_col_name}_YEAR\"\n","                use_proxy_col = True\n","            case \"HOUR\":\n","                part_format = \"yyyyMMddHH\"\n","                partition = f\"{part_col_name}_HOUR\"\n","                use_proxy_col = True\n","            case _:\n","                raise Exception(\"Unsupported partition grain\")\n","    \n","        print(\"{0} partitioning - partitioned by {1} (Requires Proxy Column: {2})\".format( \\\n","            schedule.PartitionGrain, \\\n","            partition, \\\n","            use_proxy_col))\n","        \n","        if use_proxy_col:\n","            df_bq = df_bq.withColumn(partition, date_format(col(schedule.PartitionColumn), part_format))\n","        \n","        return (partition, df_bq)\n","    \n","    def merge_table(self, schedule:SyncSchedule, src:DataFrame) -> SyncSchedule:\n","        \"\"\"\n","        Merge into Lakehouse Table based on User Configuration. Only supports Insert/Update All\n","        \"\"\"\n","        spark.conf.set(\"spark.databricks.delta.merge.repartitionBeforeWrite.enabled\", \"true\")\n","\n","        constraints = []\n","\n","        for p in schedule.Keys:\n","            constraints.append(f\"s.{p} = d.{p}\")\n","\n","        if not constraints:\n","            raise ValueError(\"One or more keys must be specified for a MERGE operation\")\n","        \n","        if schedule.FabricPartitionColumn and schedule.PartitionId:\n","            constraints.append(f\"d.{schedule.FabricPartitionColumn} = '{schedule.PartitionId}'\")\n","\n","        predicate = \" AND \".join(constraints)\n","\n","        if (schedule.AllowSchemaEvolution):\n","            spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n","\n","        dest = DeltaTable.forName(spark, tableOrViewName=schedule.LakehouseTableName)\n","\n","        dest.alias('d') \\\n","        .merge(\n","            src.alias('s'),\n","            predicate\n","        ) \\\n","        .whenMatchedUpdateAll() \\\n","        .whenNotMatchedInsertAll() \\\n","        .execute()\n","\n","        if (schedule.AllowSchemaEvolution):\n","            spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"false\")\n","\n","        results = self.get_delta_merge_row_counts(schedule)\n","\n","        schedule.UpdateRowCounts(src=0, dest=0, \\\n","                                 insert=results[0], update=results[1])\n","        \n","        return schedule\n","        \n","    def sync_bq_table(\n","            self, \n","            schedule:SyncSchedule):\n","        \"\"\"\n","        Sync the data for a table from BigQuery to the target Fabric Lakehouse based on configuration\n","\n","        1. Determines how to retrieve the data from BigQuery\n","            a. PARTITION & TIME_INGESTION\n","                - Data is loaded by partition using the partition filter option of the spark connector\n","            b. FULL & WATERMARK\n","                - Loaded using the table name or source query and any relevant predicates\n","        2. Resolve BigQuery to Fabric partition mapping\n","            a. BigQuery supports TIME and RANGE based partitioning\n","                - TIME based partitioning support YEAR, MONTH, DAY & HOUR grains\n","                    - When the grain doesn't exist or a psuedo column is used, a proxy column is added\n","                        on the Fabric Lakehouse side\n","                - RANGE partitioning is a backlog feature\n","        3. Write data to the Fabric Lakehouse\n","            a. PARTITION write use replaceWhere to overwrite the specific Delta partition\n","            b. All other writes respect the configure MODE against the write destination\n","        4. Collect and save telemetry\n","        \"\"\"\n","        print(\"{0} {1}...\".format(schedule.SummaryLoadType, schedule.TableName))\n","        table_maint = DeltaTableMaintenance(schedule.LakehouseTableName)\n","\n","        if schedule.IsTimePartitionedStrategy and schedule.PartitionId is not None:\n","            print(\"Load for BQ by partition...\")\n","            src = f\"{schedule.BQTableName}\"\n","\n","            part_format = self.get_bq_partition_date_format(schedule)\n","\n","            if schedule.IsTimeIngestionPartitioned:                   \n","                part_filter = f\"timestamp_trunc({schedule.PartitionColumn}, {schedule.PartitionGrain}) = PARSE_TIMESTAMP('{part_format}', '{schedule.PartitionId}')\"\n","            else:\n","                part_filter = f\"date_trunc({schedule.PartitionColumn}, {schedule.PartitionGrain}) = PARSE_DATETIME('{part_format}', '{schedule.PartitionId}')\"\n","\n","            df_bq = super().read_bq_partition_to_dataframe(src, part_filter)\n","        else:\n","            print(\"Load for BQ by table or query...\")\n","            src = schedule.BQTableName     \n","\n","            if schedule.SourceQuery != \"\":\n","                src = schedule.SourceQuery\n","\n","            df_bq = super().read_bq_to_dataframe(src)\n","\n","        predicate = None\n","\n","        if schedule.LoadStrategy == SyncConstants.WATERMARK and not schedule.InitialLoad:\n","            print(\"Use watermark for differential load...\")\n","            pk = schedule.PrimaryKey\n","            max_watermark = schedule.MaxWatermark\n","\n","            if max_watermark.isdigit():\n","                predicate = f\"{pk} > {max_watermark}\"\n","            else:\n","                predicate = f\"{pk} > '{max_watermark}'\"\n","            \n","        if predicate is not None:\n","            df_bq = df_bq.where(predicate)\n","\n","        df_bq.cache()\n","\n","        partition = None\n","\n","        if schedule.IsPartitioned:\n","            print('Resolving Fabric partitioning...')\n","            if schedule.PartitionType == SyncConstants.TIME:\n","                partition = schedule.PartitionColumn\n","\n","                if not schedule.IsTimeIngestionPartitioned:\n","                    partition, df_bq = self.resolve_fabric_partition_column(schedule, df_bq)\n","                else:\n","                    part_format = self.get_bq_partition_date_format(schedule)\n","                    partition = f\"__bq{schedule.PartitionColumn}\"\n","\n","                    print(\"Ingestion time partitioning - partitioned by {0} ({1})\".format(partition, schedule.PartitionId))\n","                    df_bq = df_bq.withColumn(partition, lit(schedule.PartitionId))\n","                \n","                schedule.FabricPartitionColumn = partition\n","\n","        write_config = { \"delta.enableDeletionVectors\" : str(schedule.EnableDeletionVectors).lower() }\n","\n","        #Schema Evolution\n","        if schedule.AllowSchemaEvolution:\n","            table_maint.evolve_schema(df_bq)\n","            write_config[\"mergeSchema\"] = \"true\"\n","\n","        if not schedule.LoadStrategy == SyncConstants.MERGE:\n","            if schedule.IsTimePartitionedStrategy and schedule.PartitionId is not None:\n","                print(f\"Writing {schedule.TableName}${schedule.PartitionId} partition to Lakehouse...\")\n","\n","                write_config[\"replaceWhere\"] = f\"{schedule.FabricPartitionColumn} = '{schedule.PartitionId}'\"}\n","\n","                df_bq.write \\\n","                    .mode(SyncConstants.OVERWRITE) \\\n","                    .options(**write_config) \\\n","                    .saveAsTable(schedule.LakehouseTableName)\n","            else:\n","                print(\"Writing dataframe to Lakehouse...\")\n","                if partition is None:\n","                    df_bq.write \\\n","                        .mode(schedule.Mode) \\\n","                        .options( **write_config) \\\n","                        .saveAsTable(schedule.LakehouseTableName)\n","                else:\n","                    df_bq.write \\\n","                        .partitionBy(partition) \\\n","                        .mode(schedule.Mode) \\\n","                        .options( **write_config) \\\n","                        .saveAsTable(schedule.LakehouseTableName)\n","        else:\n","            print(\"Merging dataframe to Lakehouse...\")\n","            schedule = self.merge_table(schedule, df_bq)\n","\n","        if schedule.LoadStrategy == SyncConstants.WATERMARK:\n","            schedule.MaxWatermark = self.get_max_watermark(schedule.LakehouseTableName, schedule.PrimaryKey)\n","\n","        src_cnt = df_bq.count()\n","\n","        if (schedule.LoadStrategy == SyncConstants.PARTITION or \\\n","                schedule.LoadStrategy == SyncConstants.TIME_INGESTION)  and schedule.PartitionId is not None:\n","            dest_cnt = src_cnt\n","        else:\n","            dest_cnt = spark.table(schedule.LakehouseTableName).count()\n","\n","        schedule.UpdateRowCounts(src_cnt, dest_cnt, 0, 0)    \n","        schedule.SparkAppId = spark.sparkContext.applicationId\n","        schedule.DeltaVersion = table_maint.CurrentTableVersion\n","        schedule.EndTime = datetime.now(timezone.utc)\n","\n","        df_bq.unpersist()\n","\n","        return schedule\n","\n","    def process_load_group_telemetry(\n","            self, \n","            load_grp : str = None):\n","        \"\"\"\n","        When a load group is complete, summarizes the telemetry to close out the schedule\n","        \"\"\"\n","        load_grp_filter = \"\"\n","\n","        if load_grp is not None:\n","            load_grp_filter = f\"AND r.priority = '{load_grp}'\"\n","\n","        sql = f\"\"\"\n","        WITH schedule_telemetry AS (\n","                SELECT\n","                        schedule_id,\n","                        project_id,\n","                        dataset,\n","                        table_name,\n","                        SUM(CASE WHEN status='COMPLETE' THEN 1 ELSE 0 END) AS completed_activities,\n","                        SUM(CASE WHEN status='FAILED' THEN 1 ELSE 0 END) AS failed_activities,\n","                        MIN(started) as started,\n","                        MAX(completed) as completed\n","                FROM bq_sync_schedule_telemetry\n","                GROUP BY\n","                schedule_id,\n","                project_id,\n","                dataset,\n","                table_name\n","        ),\n","        schedule_watermarks AS (\n","                SELECT\n","                        schedule_id,\n","                        project_id,\n","                        dataset,\n","                        table_name,\n","                        max_watermark,\n","                        ROW_NUMBER() OVER(PARTITION BY schedule_id,\n","                                project_id,\n","                                dataset,\n","                                table_name ORDER BY completed DESC) AS row_num\n","                FROM bq_sync_schedule_telemetry\n","                WHERE max_watermark IS NOT NULL\n","        ),\n","        schedule_results AS (\n","                SELECT\n","                        s.schedule_id,\n","                        s.project_id,\n","                        s.dataset,\n","                        s.table_name,\n","                        s.status,\n","                        CASE WHEN t.failed_activities = 0 THEN 'COMPLETE' ELSE 'FAILED' END AS result_status,\n","                        t.started,\n","                        t.completed,\n","                        t.completed_activities,\n","                        t.failed_activities,\n","                        w.max_watermark,\n","                        s.priority \n","                FROM bq_sync_schedule s\n","                JOIN schedule_telemetry t ON \n","                        s.schedule_id = t.schedule_id AND\n","                        s.project_id = t.project_id AND\n","                        s.dataset = t.dataset AND\n","                        s.table_name = t.table_name\n","                LEFT JOIN schedule_watermarks w ON\n","                        s.schedule_id = w.schedule_id AND\n","                        s.project_id = w.project_id AND\n","                        s.dataset = w.dataset AND\n","                        s.table_name = w.table_name\n","        )  \n","\n","        MERGE INTO bq_sync_schedule s\n","        USING ( \n","                SELECT *\n","                FROM schedule_results r\n","                WHERE r.status='SCHEDULED'\n","                {load_grp_filter}\n","        ) r\n","        ON s.schedule_id = r.schedule_id AND\n","                s.project_id = r.project_id AND\n","                s.dataset = r.dataset AND\n","                s.table_name = r.table_name\n","        WHEN MATCHED THEN\n","                UPDATE SET\n","                        s.status = r.result_status,\n","                        s.started = r.started,\n","                        s.completed = r.completed,\n","                        s.completed_activities = r.completed_activities,\n","                        s.failed_activities = r.failed_activities,\n","                        s.max_watermark = r.max_watermark\n","\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def commit_table_configuration(self):\n","        \"\"\"\n","        After an initial load, locks the table configuration so no changes can occur when reprocessing metadata\n","        \"\"\"\n","        sql = \"\"\"\n","        WITH committed AS (\n","            SELECT project_id, dataset, table_name, MAX(started) as started\n","            FROM bq_sync_schedule\n","            WHERE status='COMPLETE'\n","            GROUP BY project_id, dataset, table_name\n","        )\n","\n","        MERGE INTO bq_sync_configuration t\n","        USING committed c\n","        ON t.project_id=c.project_id\n","        AND t.dataset=c.dataset\n","        AND t.table_name=c.table_name\n","        WHEN MATCHED AND t.sync_state='INIT' THEN\n","            UPDATE SET\n","                t.sync_state='COMMIT'\n","        \"\"\"\n","        spark.sql(sql)\n","\n","\n","    def run_sequential_schedule(self):\n","        \"\"\"\n","        Run the schedule activities sequentially based on priority order\n","        \"\"\"\n","        df_schedule = self.get_schedule()\n","\n","        for row in df_schedule.collect():\n","            schedule = SyncSchedule(row)\n","\n","            self.sync_bq_table(schedule)\n","\n","            self.save_schedule_telemetry(schedule)  \n","\n","        self.process_load_group_telemetry()\n","        self.commit_table_configuration()\n","    \n","    def run_aync_schedule(self):\n","        \"\"\"\n","        Runs the schedule activities in parallel using the runMultiple \n","        capabilities of the Fabric Spark Notebook\n","\n","        - Utilitizes the priority to define load groups to respect priority\n","        - Parallelism is control from the User Config JSON file\n","        \"\"\"\n","        dag = ScheduleDAG(timeout=self.UserConfig.Async.NotebookTimeout, \\\n","            concurrency=self.UserConfig.Async.Parallelism)\n","\n","        schedule = self.get_schedule()\n","\n","        load_grps = [i[\"priority\"] for i in schedule.select(\"priority\").distinct().orderBy(\"priority\").collect()]\n","\n","        grp_dependency = None\n","\n","        for grp in load_grps:\n","            checkpoint_dependencies = []\n","            grp_nm = \"GROUP_{0}\".format(grp)\n","            grp_df = schedule.where(f\"priority = '{grp}'\")\n","\n","            for tbl in grp_df.collect():\n","                nm = \"{0}.{1}\".format(tbl[\"dataset\"], tbl[\"table_name\"])\n","                dependencies = []\n","\n","                if tbl[\"partition_id\"] is not None:\n","                    nm = \"{0}${1}\".format(nm, tbl[\"partition_id\"])\n","\n","                if grp_dependency is not None:\n","                    dependencies.append(grp_dependency)\n","                \n","                dag.activities.append( \\\n","                    DAGActivity(nm, \"BQ_TBL_PART_LOADER\", \\\n","                        self.UserConfig.Async.CellTimeout, \\\n","                        None, None, \\\n","                        dependencies, \\\n","                        schedule_id=tbl[\"schedule_id\"], \\\n","                        project_id=tbl[\"project_id\"], \\\n","                        dataset=tbl[\"dataset\"], \\\n","                        table_name=tbl[\"table_name\"], \\\n","                        partition_id=tbl[\"partition_id\"], \\\n","                        config_json_path=config_json_path))\n","\n","                checkpoint_dependencies.append(nm)\n","                print(f\"Load Activity: {nm}\")\n","\n","            \n","            grp_dependency = grp_nm\n","            print(f\"Load Group Checkpoint: {grp_nm}\")\n","            dag.activities.append( \\\n","                DAGActivity(grp_nm, \"BQ_LOAD_GROUP_CHECKPOINT\", \\\n","                    self.UserConfig.Async.CellTimeout, \\\n","                    None, None, \\\n","                    checkpoint_dependencies, \\\n","                    load_group=grp, \\\n","                    config_json_path=self.ConfigPath))\n","        \n","        dag_json = json.dumps(dag, indent=4, cls=ScheduleDAGEncoder)\n","        #print(dag_json)\n","        schedule_dag = json.loads(dag_json)\n","\n","        dag_result = mssparkutils.notebook.runMultiple(schedule_dag, {\"displayDAGViaGraphviz\":True, \"DAGLayout\":\"spectral\", \"DAGSize\":8})\n","\n","        self.commit_table_configuration()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5205150e-59b3-44b3-932d-74e6715ee9a0","statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:31:28.5590037Z","session_start_time":null,"execution_start_time":"2024-04-18T13:31:37.4371015Z","execution_finish_time":"2024-04-18T13:31:37.8285799Z","parent_msg_id":"bf3a6f76-120d-43bf-909d-65449ca811a6"},"text/plain":"StatementMeta(, 5205150e-59b3-44b3-932d-74e6715ee9a0, 23, Finished, Available)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c0e8422c-123d-4c5c-938b-8d50ad956281"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"environment":{"environmentId":"98eebb73-c49b-432b-bbff-85316ca4c796","workspaceId":"0fb2d96e-53e7-4c49-a594-beb0891ac121"}}},"nbformat":4,"nbformat_minor":5}