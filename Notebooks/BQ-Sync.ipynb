{"cells":[{"cell_type":"markdown","source":["## BigQuery Spark Connector\n","\n","Python does not currently support jar files loaded in an Environment. We load the required library into the session.\n","\n","<mark>PLEASE VERIFY THE LAKEHOUSE (ABFSS) PATH TO THE JAR IS CORRECT FOR YOUR WORKSPACE</mark>"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5165ca45-cb7a-4223-a3c0-7a79279a2236"},{"cell_type":"code","source":["%%configure -f: \n","{\n","    \"conf\": {\n","        \"spark.jars\": \"<<<PATH_SPARK_BQ_JAR>>>\"\n","    }\n","}  "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language_group":"synapse_pyspark"}},"id":"aed6109e-d15f-4b72-8b48-fd909a85dc89"},{"cell_type":"markdown","source":["If you are not going to leverage an environment, the BQ Sync package needs to be installed at runtime. \n","\n","<strong>Please note that if you are scheduling this notebook to run from a pipeline, you must provide the <code>_inlineInstallationEnabled</code> parameter to the pipeline for pip install support.</strong>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"31a02142-e2aa-4acf-b6d9-568f2ed7755e"},{"cell_type":"code","source":["%pip install --upgrade --force-reinstall <<<PATH_TO_BQ_SYNC_PACKAGE>>>"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language_group":"synapse_pyspark"}},"id":"e712fb75-3b1c-46ce-9026-e8fea610833d"},{"cell_type":"markdown","source":["The set-up process creates a minimal config file based on the parameters provided. You can update the config file at anytime and manually upload to the either the notebook resources or environment resources path below."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da6ba8e3-9c37-49d2-8310-9fb0d0d4b3cb"},{"cell_type":"code","source":["config_json_path = \"<<<PATH_TO_USER_CONFIG>>>\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"],"microsoft":{"language_group":"synapse_pyspark"}},"id":"8c479e7b-120c-45b7-8363-adb7b9450ee6"},{"cell_type":"code","source":["from FabricSync.BQ.Loader import *\n","from FabricSync.DeltaTableUtility import *"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language_group":"synapse_pyspark"}},"id":"fde4eedb-e50c-467b-bf69-f27821ce125a"},{"cell_type":"markdown","source":["# Metadata Sync"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4dbed78a-f6cf-479f-b5be-cd0317e52acc"},{"cell_type":"markdown","source":["This step loads the user-supplied configuration and retrieves the relevant BQ metadata. Once the metadata has been synced to the Fabric lakehouse, the auto-detect process estimates the most optimal way to load the BQ data and persist to the the metadata lakehouse. You can modified any of the configuration data as needed by either:\n","-  Updating the <code>bq_sync_configuration</code> table directly\n","-  Specifying overrides in the user configuration file\n","\n","Once the schedule and loader has run for the first time for any given table the configuration is locked. To make a change or fix an error or if any configuration modification is needed after a table has been loaded. Take the following steps:\n","1. Delete the configuration record from the <code>bq_sync_configuration</code> table\n","2. Delete any related sync metadata from the <code>bq_sync_schedule</code> and <code>bq_sync_schedule_telemetry</code> tables.\n","3. Remove the target BQ table data from the target lakehouse manually or with <code>mssparkutils.fs.rm(\"\\<PATH TO BQ TABLE>\\\", recurse=True)</code> "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"16ab0de8-0c81-4336-a891-9cd5aac8be78"},{"cell_type":"markdown","source":["# Scheduler"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"24b2bc01-e6dd-4461-986b-339ae373920a"},{"cell_type":"markdown","source":["Schedule builder for the tables configured in the above step. Currently only AUTO is supported for loading which evaluates all enabled tables for every load. If a table is static and needs to be skipped, add an entry to the user configuration file to disabled the table.\n","\n","<code>\n","\t\"tables\": [ \n","\t{\n","\t\t\"table_name\": \"<MY BQ TABLE>\",\n","\t\t\"enabled\": false\n","\t}\n","\t]\n","</code>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ffdd072d-efa4-447a-be73-e4c63ee49935"},{"cell_type":"markdown","source":["# Loader"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6cc6ae65-fb9b-4ace-af30-a57f2656c765"},{"cell_type":"markdown","source":["The async scheduler uses python threading to saturate the configured spark cluster resource to optimize load efficiencies for your BQ tables/partitions. Python thread parallelism is controlled by the user configuration file. \n","\n","<code>\n","\"async\": {\n","\t\"enabled\": true,\n","\t\"parallelism\": 5,\n","\t\"cell_timeout\": 36000,\n","\t\"notebook_timeout\": 72000\n","\t}\n","</code>\n","\n","Choose a sensible number of threads based on your configured spark environment (the default is 5). Setting the degree of parallelism too high will slow the whole process down."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3d2a77a8-79f7-4300-8e49-4c224638172d"},{"cell_type":"markdown","source":["# Metadata Table Maintenance"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0e414ac6-7902-4e66-bd85-e7234e569ac5"},{"cell_type":"markdown","source":["Hygiene for the BQ sync process metadata tables:\n","- Optimize\n","- Vacuum with Retention 0 to minimize data size"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"24f5812f-e043-42f7-98de-a9c12d4eec90"},{"cell_type":"code","source":["bq_sync = BQSync(spark, config_json_path)\n","group_schedule_id = bq_sync.build_schedule()\n","bq_sync.run_schedule(group_schedule_id)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language_group":"synapse_pyspark"}},"id":"d76a0dda-8c58-4eb6-85e8-c67e9ec86442"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}