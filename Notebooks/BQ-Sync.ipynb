{"cells":[{"cell_type":"markdown","source":["## <mark>Notebook must be attached to metadata Lakehouse created during set-up</mark>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e90cf8dd-3523-4899-bc6e-6780cf487914"},{"cell_type":"markdown","source":["## BigQuery Spark Connector"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5165ca45-cb7a-4223-a3c0-7a79279a2236"},{"cell_type":"code","source":["%%configure -f: \n","{\n","    \"defaultLakehouse\": {\n","        \"name\": \"<<<METADATA_LAKEHOUSE_NAME>>>\",\n","        \"id\": \"<<<METADATA_LAKEHOUSE_ID>>>\",\n","        \"workspaceId\": \"<<<FABRIC_WORKSPACE_ID>>>\"\n","    },\n","    \"conf\": {\n","        \"spark.jars\": \"<<<PATH_SPARK_BQ_JAR>>>\"\n","    }\n","}  "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language_group":"synapse_pyspark","language":"python"}},"id":"aed6109e-d15f-4b72-8b48-fd909a85dc89"},{"cell_type":"markdown","source":["## BQ Sync Python Package\n","If you are not going to leverage an environment, the BQ Sync package needs to be installed at runtime. \n","\n","<strong>Options for Loading/Using the BQ Sync Package</strong>\n","1. Runtime from OneLake (stable): <br />\n","    <code>%pip install /lakehouse/default/Files/BQ_Sync_Process/libs/FabricSync-0.1.0-py3-none-any.whl</code>\n","2. Runtime from GitHub (latest version): <br/>\n","    <code>%pip install https://github.com/microsoft/FabricBQSync/raw/main/Packages/FabricSync/dist/FabricSync-0.1.0-py3-none-any.whl</code>\n","3. From Spark Environment \n","\n","<strong>Please note that if you are scheduling this notebook to run from a pipeline, you must provide the <code>_inlineInstallationEnabled</code> parameter to the pipeline for pip install support.</strong>"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"31a02142-e2aa-4acf-b6d9-568f2ed7755e"},{"cell_type":"code","source":["%pip install --upgrade --force-reinstall <<<PATH_TO_BQ_SYNC_PACKAGE>>>"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language_group":"synapse_pyspark","language":"python"}},"id":"e712fb75-3b1c-46ce-9026-e8fea610833d"},{"cell_type":"markdown","source":["# Sync Loader Documentation\n","\n","## Config File Path\n","The set-up process creates a minimal config file based on the parameters provided. \n","\n","You can update the config file at anytime and manually upload to an alternate path. \n","\n","Note: If you upload to a OneLake destination, it must be in the default Lakehouse and the <code>config_json_path</code> should point to the File API path (example: <code>/lakehouse/default/Files/myconfigfile.json</code>)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da6ba8e3-9c37-49d2-8310-9fb0d0d4b3cb"},{"cell_type":"code","source":["config_json_path = \"<<<PATH_TO_USER_CONFIG>>>\"\n","schedule_type = \"AUTO\"\n","optimize_metadata = False"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"],"microsoft":{"language_group":"synapse_pyspark","language":"python"}},"id":"8c479e7b-120c-45b7-8363-adb7b9450ee6"},{"cell_type":"code","source":["from FabricSync.BQ.Loader import *\n","from FabricSync.DeltaTableUtility import *"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language_group":"synapse_pyspark","language":"python"}},"id":"fde4eedb-e50c-467b-bf69-f27821ce125a"},{"cell_type":"markdown","source":["# Metadata Sync\nThis step loads the user-supplied configuration and retrieves the relevant BQ metadata. Once the metadata has been synced to the Fabric lakehouse, the auto-detect process estimates the most optimal way to load the BQ data and persist to the the metadata lakehouse. You can modified any of the configuration data as needed by either:\n-  Updating the <code>bq_sync_configuration</code> table directly\n-  Specifying overrides in the user configuration file\n\nOnce the schedule and loader has run for the first time for any given table the configuration is locked. To make a change or fix an error or if any configuration modification is needed after a table has been loaded. Take the following steps:\n1. Delete the configuration record from the <code>bq_sync_configuration</code> table\n2. Delete any related sync metadata from the <code>bq_sync_schedule</code> and <code>bq_sync_schedule_telemetry</code> tables.\n3. Remove the target BQ table data from the target lakehouse manually or with <code>mssparkutils.fs.rm(\"\\<PATH TO BQ TABLE>\\\", recurse=True)</code> \n\n# Scheduler\nSchedule builder for the tables configured in the above step. Currently only AUTO is supported for loading which evaluates all enabled tables for every load. If a table is static and needs to be skipped, add an entry to the user configuration file to disabled the table.\n\n<code>\n\t\"tables\": [ \n\t{\n\t\t\"table_name\": \"<MY BQ TABLE>\",\n\t\t\"enabled\": false\n\t}\n\t]\n</code>\n\n# Loader\nThe async scheduler uses python threading to saturate the configured spark cluster resource to optimize load efficiencies for your BQ tables/partitions. Python thread parallelism is controlled by the user configuration file. \n\n<code>\n\"async\": {\n\t\"enabled\": true,\n\t\"parallelism\": 5,\n\t\"cell_timeout\": 36000,\n\t\"notebook_timeout\": 72000\n\t}\n</code>\n\nChoose a sensible number of threads based on your configured spark environment (the default is 5). Setting the degree of parallelism too high will slow the whole process down.\n\n# Metadata Table Maintenance\nHygiene for the BQ sync process metadata tables:\n- Optimize\n- Vacuum with Retention 0 to minimize data size"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1aca9025-8c1b-4bd9-8437-477db7592f3d"},{"cell_type":"markdown","source":["# Running BQ Sync"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b57f6f37-1733-4eee-a280-d67a2864ac13"},{"cell_type":"code","source":["bq_sync = BQSync(spark, config_json_path)\n","bq_sync.sync_metadata()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language_group":"synapse_pyspark","language":"python"}},"id":"d76a0dda-8c58-4eb6-85e8-c67e9ec86442"},{"cell_type":"markdown","source":["If you would like to export the auto-discovered BQ sync configuration to a new user configuration file, un-comment and run the following lines of code. \n","\n","This step will generate a potentially verbose configuration file with all tables discovered from your BQ Dataset. You can tweak and override any of the autodiscovered settings. Please note, there is currently no validation on the configuration set-up. An invalid configuration may potentially break the sync process.\n","\n","If configuration changes are made and your re-target the sync process within an existing session please run the following line of code to clean-up any cached configuration before re-running the sync process.\n","\n","<code>bq_sync.cleanup_session(spark)</code>\n","\n","After the session is invalidated, it is necessary to re-run the <code>BQSync()</code> constructor to force a reload of the user configuration"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28931d0d-5fe6-4239-a51f-63361d747d63"},{"cell_type":"markdown","source":["Before you continue, carefully evaluate your config for correctness.\n","\n","Once you run the next step, your load configuration is locked and cannot be changed without manually resetting the sync metadata and sync'd data."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1f126c75-f27d-4528-a5ac-069afc07aa9d"},{"cell_type":"code","source":["schedule_id = bq_sync.build_schedule(sync_metadata=False, schedule_type=schedule_type)\n","bq_sync.run_schedule(group_schedule_id=schedule_id, optimize_metadata=optimize_metadata)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a866194e-7779-4983-8b1e-7add099d7eb1"},{"cell_type":"code","source":["display(spark.sql(f\"\"\"\n","SELECT * FROM (\n","    SELECT status\n","    FROM bq_sync_schedule\n","    WHERE group_schedule_id='{group_schedule_id}'\n",")\n","PIVOT (\n","  COUNT(*)\n","  FOR status in (\n","    'COMPLETE', 'SKIPPED', 'FAILED', 'SCHEDULED'\n","  )\n",")\n","\"\"\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e281bbb7-e7cc-486d-a1d6-cbde62266187"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}