{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from delta.tables import *\n","from datetime import datetime, timezone\n","import json\n","from json import JSONEncoder\n","import base64\n","from pathlib import Path\n","import os"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"72b59d90-3d7f-4824-80fe-c61ea97dcd8a","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-16T19:29:49.248005Z","session_start_time":"2024-04-16T19:29:49.4742202Z","execution_start_time":"2024-04-16T19:29:59.2435654Z","execution_finish_time":"2024-04-16T19:30:01.2775295Z","parent_msg_id":"2b7e559a-6e75-440a-ac50-8128d4d47cbf"},"text/plain":"StatementMeta(, 72b59d90-3d7f-4824-80fe-c61ea97dcd8a, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"51d29545-50c9-44b9-a5c6-7d1d674fee14"},{"cell_type":"code","source":["class SyncConstants:\n","    OVERWRITE = \"OVERWRITE\"\n","    APPEND = \"APPEND\"\n","    FULL = \"FULL\"\n","    PARTITION = \"PARTITION\"\n","    WATERMARK = \"WATERMARK\"\n","    TIME_INGESTION = \"TIME_INGESTION\"\n","    AUTO = \"AUTO\"\n","    TIME = \"TIME\"\n","    INITIAL_FULL_OVERWRITE = \"INITIAL_FULL_OVERWRITE\"\n","    INFORMATION_SCHEMA_TABLES = \"INFORMATION_SCHEMA.TABLES\"\n","    INFORMATION_SCHEMA_PARTITIONS = \"INFORMATION_SCHEMA.PARTITIONS\"\n","    INFORMATION_SCHEMA_COLUMNS = \"INFORMATION_SCHEMA.COLUMNS\"\n","    INFORMATION_SCHEMA_TABLE_CONSTRAINTS = \"INFORMATION_SCHEMA.TABLE_CONSTRAINTS\"\n","    INFORMATION_SCHEMA_KEY_COLUMN_USAGE = \"INFORMATION_SCHEMA.KEY_COLUMN_USAGE\"\n","\n","    SQL_TBL_SYNC_SCHEDULE = \"bq_sync_schedule\"\n","    SQL_TBL_SYNC_CONFIG = \"bq_sync_configuration\"\n","    SQL_TBL_DATA_TYPE_MAP = \"bq_data_type_map\"\n","    SQL_TBL_SYNC_SCHEDULE_TELEMETRY = \"bq_sync_schedule_telemetry\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"72b59d90-3d7f-4824-80fe-c61ea97dcd8a","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-16T19:30:12.7021043Z","session_start_time":null,"execution_start_time":"2024-04-16T19:30:13.0790625Z","execution_finish_time":"2024-04-16T19:30:13.4224782Z","parent_msg_id":"bf04cf9d-ca2a-43e3-9d60-13d49d3a54ae"},"text/plain":"StatementMeta(, 72b59d90-3d7f-4824-80fe-c61ea97dcd8a, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{},"id":"27ba34e9-1354-477a-9680-ee29cebba906"},{"cell_type":"code","source":["class ConfigBase():\n","    def __init__(self, config_path, force_reload_config = False):\n","        if config_path is None:\n","            raise ValueError(\"Missing Path to JSON User Config\")\n","\n","        self.ConfigPath = config_path\n","        self.UserConfig = None\n","        self.GCPCredential = None\n","\n","        self.UserConfig = self.ensure_user_config(force_reload_config)\n","\n","\n","        if self.UserConfig.GCPCredentialPath is None:\n","            raise ValueError(\"Missing GCP Credentials  path in JSON User Config\")\n","\n","        self.GCPCredential = self.load_gcp_credential()\n","    \n","    def ensure_user_config(self, reload_config):\n","        if (self.UserConfig is None or reload_config) and self.ConfigPath is not None:\n","            config = self.load_user_config(self.ConfigPath, reload_config)\n","\n","            cfg = ConfigDataset(config)\n","\n","            self.validate_user_config(cfg)\n","            \n","            return cfg\n","        else:\n","            return self.UserConfig\n","    \n","    def load_user_config(self, config_path, reload_config):\n","        config_df = None\n","\n","        if not spark.catalog.tableExists(\"user_config_json\") or reload_config:\n","            config_df = spark.read.option(\"multiline\",\"true\").json(config_path)\n","            config_df.createOrReplaceTempView(\"user_config_json\")\n","            config_df.cache()\n","        else:\n","            config_df = spark.table(\"user_config_json\")\n","            \n","        return json.loads(config_df.toJSON().first())\n","\n","    def validate_user_config(self, cfg):\n","        if cfg is None:\n","            raise RuntimeError(\"Invalid User Config\")    \n","        return True\n","\n","    def load_gcp_credential(self):\n","        cred = None\n","\n","        if self.is_base64(self.UserConfig.GCPCredentialPath):\n","            cred = self.UserConfig.GCPCredentialPath\n","        else:\n","            credential = f\"{mssparkutils.nbResPath}{self.UserConfig.GCPCredentialPath}\"\n","\n","            if os.path.exists(credential):\n","                file_contents = self.read_credential_file(credential)\n","                cred = self.convert_to_base64string(file_contents)\n","            else:\n","                raise ValueError(\"Invalid GCP Credential path supplied.\")\n","\n","        return cred\n","\n","    def read_credential_file(self, credential_path):\n","        txt = Path(credential_path).read_text()\n","        txt = txt.replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n","\n","        return txt\n","\n","    def convert_to_base64string(self, credential_val):\n","        credential_val_bytes = credential_val.encode(\"ascii\") \n","        \n","        base64_bytes = base64.b64encode(credential_val_bytes) \n","        base64_string = base64_bytes.decode(\"ascii\") \n","\n","        return base64_string\n","\n","    def is_base64(self, val):\n","        try:\n","                if isinstance(val, str):\n","                        sb_bytes = bytes(val, 'ascii')\n","                elif isinstance(val, bytes):\n","                        sb_bytes = val\n","                else:\n","                        raise ValueError(\"Argument must be string or bytes\")\n","                return base64.b64encode(base64.b64decode(sb_bytes)) == sb_bytes\n","        except Exception:\n","                return False\n","\n","    def read_bq_partition_to_dataframe(self, table, partition_filter, cache_results=False):\n","        df = spark.read \\\n","            .format(\"bigquery\") \\\n","            .option(\"parentProject\", self.UserConfig.ProjectID) \\\n","            .option(\"credentials\", self.GCPCredential) \\\n","            .option(\"viewsEnabled\", \"true\") \\\n","            .option(\"materializationDataset\", self.UserConfig.Dataset) \\\n","            .option(\"table\", table) \\\n","            .option(\"filter\", partition_filter) \\\n","            .load()\n","        \n","        if cache_results:\n","            df.cache()\n","        \n","        return df\n","\n","    def read_bq_to_dataframe(self, query, cache_results=False):\n","        df = spark.read \\\n","            .format(\"bigquery\") \\\n","            .option(\"parentProject\", self.UserConfig.ProjectID) \\\n","            .option(\"credentials\", self.GCPCredential) \\\n","            .option(\"viewsEnabled\", \"true\") \\\n","            .option(\"materializationDataset\", self.UserConfig.Dataset) \\\n","            .load(query)\n","        \n","        if cache_results:\n","            df.cache()\n","        \n","        return df\n","\n","    def write_lakehouse_table(self, df, lakehouse, tbl_nm, mode=SyncConstants.OVERWRITE):\n","        dest_table = self.UserConfig.get_lakehouse_tablename(lakehouse, tbl_nm)\n","\n","        df.write \\\n","            .mode(mode) \\\n","            .saveAsTable(dest_table)\n","    \n","    def create_infosys_proxy_view(self, trgt):\n","        clean_nm = trgt.replace(\".\", \"_\")\n","        vw_nm = f\"BQ_{clean_nm}\"\n","\n","        if not spark.catalog.tableExists(vw_nm):\n","            tbl = self.UserConfig.flatten_3part_tablename(clean_nm)\n","            lakehouse_tbl = self.UserConfig.get_lakehouse_tablename(self.UserConfig.MetadataLakehouse, tbl)\n","\n","            sql = f\"\"\"\n","            CREATE OR REPLACE TEMPORARY VIEW {vw_nm}\n","            AS\n","            SELECT *\n","            FROM {lakehouse_tbl}\n","            \"\"\"\n","            spark.sql(sql)\n","\n","    def create_userconfig_tables_proxy_view(self):\n","        sql = \"\"\"\n","            CREATE OR REPLACE TEMPORARY VIEW user_config_tables\n","            AS\n","            SELECT\n","                project_id, dataset, tbl.table_name,\n","                tbl.enabled,tbl.load_priority,tbl.source_query,\n","                tbl.load_strategy,tbl.load_type,tbl.interval,\n","                tbl.watermark.column as watermark_column,\n","                tbl.partitioned.enabled as partition_enabled,\n","                tbl.partitioned.type as partition_type,\n","                tbl.partitioned.column as partition_column,\n","                tbl.partitioned.partition_grain,\n","                tbl.lakehouse_target.lakehouse,\n","                tbl.lakehouse_target.table_name AS lakehouse_target_table,\n","                tbl.keys\n","            FROM (SELECT project_id, dataset, EXPLODE(tables) AS tbl FROM user_config_json)\n","        \"\"\"\n","        spark.sql (sql)\n","\n","    def create_userconfig_tables_cols_proxy_view(self):\n","        sql = \"\"\"\n","            CREATE OR REPLACE TEMPORARY VIEW user_config_table_keys\n","            AS\n","            SELECT\n","                project_id, dataset, table_name, pkeys.column\n","            FROM (\n","                SELECT\n","                    project_id, dataset, tbl.table_name, EXPLODE(tbl.keys) AS pkeys\n","                FROM (SELECT project_id, dataset, EXPLODE(tables) AS tbl FROM user_config_json)\n","            )\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def create_proxy_views(self):\n","        if not spark.catalog.tableExists(\"user_config_tables\"):\n","            self.create_userconfig_tables_proxy_view()\n","        \n","        if not spark.catalog.tableExists(\"user_config_table_keys\"):\n","            self.create_userconfig_tables_cols_proxy_view()\n","\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_TABLES)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_PARTITIONS)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_COLUMNS)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_TABLE_CONSTRAINTS)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_KEY_COLUMN_USAGE)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"72b59d90-3d7f-4824-80fe-c61ea97dcd8a","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-16T19:30:14.4804089Z","session_start_time":null,"execution_start_time":"2024-04-16T19:30:14.8777047Z","execution_finish_time":"2024-04-16T19:30:15.251977Z","parent_msg_id":"5478cd16-bd03-4cab-ab5a-ff981353d561"},"text/plain":"StatementMeta(, 72b59d90-3d7f-4824-80fe-c61ea97dcd8a, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0c6a4adf-5b5a-44e1-840f-553c14cf5b8a"},{"cell_type":"code","source":["class Scheduler(ConfigBase):\n","    def __init__(self, config_path):\n","        super().__init__(config_path)\n","        spark.sql(f\"USE {self.UserConfig.MetadataLakehouse}\")\n","\n","    def run(self):\n","        sql = f\"\"\"\n","        WITH new_schedule AS ( \n","            SELECT UUID() AS group_schedule_id, CURRENT_TIMESTAMP() as scheduled\n","        ),\n","        last_bq_tbl_updates AS (\n","            SELECT table_catalog, table_schema, table_name, max(last_modified_time) as last_bq_tbl_update\n","            FROM bq_information_schema_partitions\n","            GROUP BY table_catalog, table_schema, table_name\n","        ),\n","        last_load AS (\n","            SELECT project_id, dataset, table_name, MAX(started) AS last_load_update\n","            FROM {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","            WHERE status='COMPLETE'\n","            GROUP BY project_id, dataset, table_name\n","        ),\n","        schedule AS (\n","            SELECT\n","                n.group_schedule_id,\n","                UUID() AS schedule_id,\n","                c.project_id,\n","                c.dataset,\n","                c.table_name,\n","                n.scheduled,\n","                CASE WHEN ((l.last_load_update IS NULL) OR\n","                     (b.last_bq_tbl_update >= l.last_load_update))\n","                    THEN 'SCHEDULED' ELSE 'SKIPPED' END as status,\n","                NULL as started,\n","                NULL as completed,   \n","                NULL as completed_activities,\n","                NULL as failed_activities,\n","                NULL as max_watermark,\n","                c.priority                \n","            FROM {SyncConstants.SQL_TBL_SYNC_CONFIG} c \n","            LEFT JOIN {SyncConstants.SQL_TBL_SYNC_SCHEDULE} s ON \n","                c.project_id= s.project_id AND\n","                c.dataset = s.dataset AND\n","                c.table_name = s.table_name AND\n","                s.status = 'SCHEDULED'\n","            LEFT JOIN last_bq_tbl_updates b ON\n","                c.project_id= b.table_catalog AND\n","                c.dataset = b.table_schema AND\n","                c.table_name = b.table_name\n","            LEFT JOIN last_load l ON \n","                c.project_id= l.project_id AND\n","                c.dataset = l.dataset AND\n","                c.table_name = l.table_name\n","            CROSS JOIN new_schedule n\n","            WHERE s.schedule_id IS NULL\n","            AND c.enabled = TRUE\n","        )\n","\n","        INSERT INTO {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","        SELECT * FROM schedule s\n","        WHERE s.project_id = '{self.UserConfig.ProjectID}'\n","        AND s.dataset = '{self.UserConfig.Dataset}'\n","        \"\"\"\n","        spark.sql(sql)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"72b59d90-3d7f-4824-80fe-c61ea97dcd8a","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-16T19:30:16.7906498Z","session_start_time":null,"execution_start_time":"2024-04-16T19:30:17.1724895Z","execution_finish_time":"2024-04-16T19:30:17.5130646Z","parent_msg_id":"630af058-4500-4275-900e-be73ff4d837d"},"text/plain":"StatementMeta(, 72b59d90-3d7f-4824-80fe-c61ea97dcd8a, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fdb1ac1d-8bb9-4a40-94a2-87721b3844be"},{"cell_type":"code","source":["class ScheduleDAG:\n","    def __init__(self, timeout=7200, concurrency=5):\n","        self.activities = []\n","        self.timeoutInSeconds = timeout\n","        self.concurrency = concurrency\n","\n","class ScheduleDAGEncoder(JSONEncoder):\n","        def default(self, o):\n","            return o.__dict__\n","            \n","class DAGActivity:\n","    def __init__(self, name, path, timeout = 3600, retry =  None, retryInterval = None, dependencies = [], **keyword_args):\n","        self.name = name\n","        self.path = path\n","        self.timeoutPerCellInSeconds = timeout\n","        self.retry = retry\n","        self.retryIntervalInSeconds = retryInterval\n","        self.dependencies = dependencies\n","        self.args = keyword_args"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ec5f5cd8-89ba-4a57-99d8-e67bb39a857f"},{"cell_type":"code","source":["class SyncSchedule:\n","    EndTime = None\n","    SourceRows = 0\n","    DestRows = 0\n","    InsertedRows = 0\n","    UpdatedRows = 0\n","    DeltaVersion = None\n","    SparkAppId = None\n","    MaxWatermark = None\n","    Status = None\n","\n","    def __init__(self, row):\n","        self.Row = row\n","        self.StartTime = datetime.now(timezone.utc)\n","        self.GroupScheduleId = row[\"group_schedule_id\"]\n","        self.ScheduleId = row[\"schedule_id\"]\n","        self.LoadStrategy = row[\"load_strategy\"]\n","        self.LoadType = row[\"load_type\"]\n","        self.InitialLoad = row[\"initial_load\"]\n","        self.ProjectId = row[\"project_id\"]\n","        self.Dataset = row[\"dataset\"]\n","        self.TableName = row[\"table_name\"]\n","        self.SourceQuery = row[\"source_query\"]\n","        self.MaxWatermark = row[\"max_watermark\"]\n","        self.IsPartitioned = row[\"is_partitioned\"]\n","        self.PartitionColumn = row[\"partition_column\"]\n","        self.PartitionType = row[\"partition_type\"]\n","        self.PartitionGrain = row[\"partition_grain\"]\n","        self.WatermarkColumn = row[\"watermark_column\"]\n","        self.LastScheduleLoadDate = row[\"last_schedule_dt\"]\n","        self.Lakehouse = row[\"lakehouse\"]\n","        self.DestinationTableName = row[\"lakehouse_table_name\"]\n","        self.PartitionId = row[\"partition_id\"]\n","    \n","    @property\n","    def SummaryLoadType(self):\n","        if self.InitialLoad:\n","            return SyncConstants.INITIAL_FULL_OVERWRITE\n","        else:\n","            return \"{0}_{1}\".format(self.LoadStrategy, self.LoadType)\n","    \n","    @property\n","    def Mode(self):\n","        if self.InitialLoad:\n","            return SyncConstants.OVERWRITE\n","        else:\n","            return self.LoadType\n","    \n","    @property\n","    def PrimaryKey(self):\n","        if self.Row[\"primary_keys\"]:\n","            return self.Row[\"primary_keys\"][0]\n","        else:\n","            return None\n","    \n","    @property\n","    def LakehouseTableName(self):\n","        return \"{0}.{1}\".format(self.Lakehouse, self.DestinationTableName)\n","        \n","    @property\n","    def BQTableName(self):\n","        return \"{0}.{1}.{2}\".format(self.ProjectId, self.Dataset, self.TableName)\n","\n","    @property\n","    def IsTimeIngestionPartitioned(self):\n","        is_time = False\n","\n","        if self.PartitionColumn == \"_PARTITIONTIME\" or self.PartitionColumn == \"_PARTITIONDATE\":\n","            is_time = True;\n","\n","        return is_time;\n","\n","\n","    def UpdateRowCounts(self, src, dest, insert, update):\n","        self.SourceRows += src\n","        self.DestRows += dest\n","\n","        match self.LoadStrategy:\n","            case SyncConstants.WATERMARK:\n","                self.InsertedRows += src     \n","            case SyncConstants.PARTITION:\n","                self.InsertedRows += dest  \n","            case _:\n","                self.InsertedRows += dest\n","\n","        self.UpdatedRows = 0"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"72b59d90-3d7f-4824-80fe-c61ea97dcd8a","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-16T19:30:18.2717076Z","session_start_time":null,"execution_start_time":"2024-04-16T19:30:18.6484574Z","execution_finish_time":"2024-04-16T19:30:18.9955501Z","parent_msg_id":"326d7ad1-4118-488c-99d5-709a4b0f2604"},"text/plain":"StatementMeta(, 72b59d90-3d7f-4824-80fe-c61ea97dcd8a, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f8235f42-25df-451d-8083-f8b4f7cc8876"},{"cell_type":"code","source":["class ConfigDataset:\n","    def __init__(self, json_config):\n","        self.ProjectID = self.get_json_conf_val(json_config, \"project_id\", None)\n","        self.Dataset = self.get_json_conf_val(json_config, \"dataset\", None)\n","        self.LoadAllTables = self.get_json_conf_val(json_config, \"load_all_tables\", True)\n","        self.Autodetect = self.get_json_conf_val(json_config, \"autodetect\", True)\n","        self.MasterReset = self.get_json_conf_val(json_config, \"master_reset\", False)\n","        self.MetadataLakehouse = self.get_json_conf_val(json_config, \"metadata_lakehouse\", None)\n","        self.TargetLakehouse = self.get_json_conf_val(json_config, \"target_lakehouse\", None)\n","        self.GCPCredentialPath = self.get_json_conf_val(json_config, \"gcp_credential_path\", None)\n","        self.Tables = []\n","\n","        if \"async\" in json_config:\n","            self.Async = ConfigAsync(\n","                self.get_json_conf_val(json_config[\"async\"], \"enabled\", False),\n","                self.get_json_conf_val(json_config[\"async\"], \"parallelism\", None),\n","                self.get_json_conf_val(json_config[\"async\"], \"notebook_timeout\", None),\n","                self.get_json_conf_val(json_config[\"async\"], \"cell_timeout\", None)\n","            )\n","        else:\n","            self.Async = ConfigAsync()\n","\n","        if \"tables\" in json_config:\n","            for t in json_config[\"tables\"]:\n","                self.Tables.append(ConfigBQTable(t))\n","    \n","    def get_delimited_tables_list(self):\n","            return ''.join([str(x.TableName) for x in self.Tables])\n","\n","    def get_table_name_list(self):\n","        return [str(x.TableName) for x in self.Tables]\n","\n","    def get_bq_table_fullname(self, tbl_name):\n","        return f\"{self.ProjectID}.{self.Dataset}.{tbl_name}\"\n","\n","    def get_lakehouse_tablename(self, lakehouse, tbl_name):\n","        return f\"{lakehouse}.{tbl_name}\"\n","\n","    def flatten_3part_tablename(self, tbl_name):\n","        clean_project_id = self.ProjectID.replace(\"-\", \"_\")\n","        return f\"{clean_project_id}_{self.Dataset}_{tbl_name}\"\n","    \n","    def get_json_conf_val(self, json, config_key, default_val = None):\n","        if config_key in json:\n","            return json[config_key]\n","        else:\n","            return default_val\n","\n","class ConfigAsync:\n","    def __init__(self, enabled = False, parallelism = 5, notebook_timeout = 1800, cell_timeout = 300):\n","        self.Enabled = enabled\n","        self.Parallelism = parallelism\n","        self.NotebookTimeout = notebook_timeout\n","        self.CellTimeout = cell_timeout\n","\n","class ConfigTableColumn:\n","    def __init__(self, col = \"\"):\n","        self.Column = col\n","\n","class ConfigLakehouseTarget:\n","    def __init__(self, lakehouse = \"\", table = \"\"):\n","        self.Lakehouse = lakehouse\n","        self.Table = table\n","\n","class ConfigPartition:\n","    def __init__(self, enabled = False, partition_type = \"\", col = ConfigTableColumn(), grain = \"\"):\n","        self.Enabled = enabled\n","        self.PartitionType = partition_type\n","        self.PartitionColumn = col\n","        self.Granularity = grain\n","\n","class ConfigBQTable:\n","    def __str__(self):\n","        return str(self.TableName)\n","\n","    def __init__(self, json_config):\n","        self.TableName = self.get_json_conf_val(json_config, \"table_name\", \"\")\n","        self.Priority = self.get_json_conf_val(json_config, \"priority\", 100)\n","        self.SourceQuery = self.get_json_conf_val(json_config, \"source_query\", \"\")\n","        self.LoadStrategy = self.get_json_conf_val(json_config, \"load_strategy\" , SyncConstants.FULL)\n","        self.LoadType = self.get_json_conf_val(json_config, \"load_type\", SyncConstants.OVERWRITE)\n","        self.Interval =  self.get_json_conf_val(json_config, \"interval\", SyncConstants.AUTO)\n","        self.Enabled =  self.get_json_conf_val(json_config, \"enabled\", True)\n","\n","        if \"lakehouse_target\" in json_config:\n","            self.LakehouseTarget = ConfigLakehouseTarget( \\\n","                self.get_json_conf_val(json_config[\"lakehouse_target\"], \"lakehouse\", \"\"), \\\n","                self.get_json_conf_val(json_config[\"lakehouse_target\"], \"table_name\", \"\"))\n","        else:\n","            self.LakehouseTarget = ConfigLakehouseTarget()\n","        \n","        if \"watermark\" in json_config:\n","            self.Watermark = ConfigTableColumn( \\\n","                self.get_json_conf_val(json_config[\"watermark\"], \"column\", \"\"))\n","        else:\n","            self.Watermark = ConfigTableColumn()\n","\n","        if \"partitioned\" in json_config:\n","            self.Partitioned = ConfigPartition( \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"enabled\", False), \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"type\", \"\"), \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"column\", \"\"), \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"partition_grain\", \"\"))\n","        else:\n","            self.Partitioned = ConfigPartition()\n","        \n","        self.Keys = []\n","\n","        if \"keys\" in json_config:\n","            for c in json_config[\"keys\"]:\n","                self.Keys.append(ConfigTableColumn( \\\n","                    self.get_json_conf_val(c, \"column\", \"\")))\n","        \n","    def get_json_conf_val(self, json, config_key, default_val = None):\n","        if config_key in json:\n","            return json[config_key]\n","        else:\n","            return default_val"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"72b59d90-3d7f-4824-80fe-c61ea97dcd8a","statement_id":8,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-16T19:30:19.8565594Z","session_start_time":null,"execution_start_time":"2024-04-16T19:30:20.2371415Z","execution_finish_time":"2024-04-16T19:30:20.6051896Z","parent_msg_id":"aa419288-fef2-4c93-bb6e-65a72ebe8875"},"text/plain":"StatementMeta(, 72b59d90-3d7f-4824-80fe-c61ea97dcd8a, 8, Finished, Available)"},"metadata":{}}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e052e1da-4963-41ff-ac98-e56b3eb1ca5d"},{"cell_type":"code","source":["class ConfigMetadataLoader(ConfigBase):\n","    def __init__(self, config_path):\n","        self.JSON_Config_Path = config_path\n","        \n","        super().__init__(config_path)\n","        spark.sql(f\"USE {self.UserConfig.MetadataLakehouse}\")\n","    \n","    def create_autodetect_view(self):\n","        sql = \"\"\"\n","        CREATE OR REPLACE TEMPORARY VIEW bq_table_metadata_autodetect\n","        AS\n","        WITH pkeys AS (    \n","            SELECT\n","                c.table_catalog, c.table_schema, c.table_name, \n","                k.column_name AS pk_col\n","            FROM bq_information_schema_table_constraints c\n","            JOIN bq_information_schema_key_column_usage k ON\n","                k.table_catalog = c.table_catalog AND\n","                k.table_schema = c.table_schema AND\n","                k.table_name = c.table_name AND\n","                k.constraint_name = c.constraint_name\n","            JOIN bq_information_schema_columns n ON\n","                n.table_catalog = k.table_catalog AND\n","                n.table_schema = k.table_schema AND\n","                n.table_name = k.table_name AND\n","                n.column_name = k.column_name\n","            JOIN bq_data_type_map m ON n.data_type = m.data_type\n","            WHERE c.constraint_type = 'PRIMARY KEY'\n","            AND m.is_watermark = 'YES'\n","        ),\n","        pkeys_cnt AS (\n","            SELECT \n","                table_catalog, table_schema, table_name, \n","                COUNT(*) as pk_cnt\n","            FROM pkeys\n","            GROUP BY table_catalog, table_schema, table_name\n","        ),\n","        watermark_cols AS (\n","            SELECT \n","                k.*\n","            FROM pkeys k\n","            JOIN pkeys_cnt c ON \n","                k.table_catalog = c.table_catalog AND\n","                k.table_schema = c.table_schema AND\n","                k.table_name = c.table_name\n","            WHERE c.pk_cnt = 1\n","        ),\n","        partitions AS (\n","            SELECT\n","                table_catalog, table_schema, table_name, \n","                count(*) as partition_count,\n","                avg(len(partition_id)) AS partition_id_len,\n","                sum(case when partition_id is NULL then 1 else 0 end) as null_partition_count\n","            FROM bq_information_schema_partitions\n","            GROUP BY table_catalog, table_schema, table_name\n","        ), \n","        partition_columns AS\n","        (\n","            SELECT\n","                table_catalog, table_schema, table_name,\n","                column_name, c.data_type,\n","                m.partition_type AS partitioning_type\n","            FROM bq_information_schema_columns c\n","            JOIN bq_data_type_map m ON c.data_type=m.data_type\n","            WHERE is_partitioning_column = 'YES'\n","        ),\n","        partition_cfg AS\n","        (\n","            SELECT\n","                p.*,\n","                CASE WHEN p.partition_count = 1 AND p.null_partition_count = 1 THEN FALSE ELSE TRUE END AS is_partitioned,\n","                c.column_name AS partition_col,\n","                c.data_type AS partition_data_type,\n","                c.partitioning_type,\n","                CASE WHEN (c.partitioning_type = 'TIME')\n","                    THEN \n","                        CASE WHEN (partition_id_len = 4) THEN 'YEAR'\n","                            WHEN (partition_id_len = 6) THEN 'MONTH'\n","                            WHEN (partition_id_len = 8) THEN 'DAY'\n","                            WHEN (partition_id_len = 10) THEN 'HOUR'\n","                            ELSE NULL END\n","                    ELSE NULL END AS partitioning_strategy\n","            FROM partitions p\n","            LEFT JOIN partition_columns c ON \n","                p.table_catalog = c.table_catalog AND\n","                p.table_schema = c.table_schema AND\n","                p.table_name = c.table_name\n","        )\n","\n","        SELECT \n","            t.table_catalog, t.table_schema, t.table_name, t.is_insertable_into,\n","            p.is_partitioned, p.partition_col, p.partition_data_type, p.partitioning_type, p.partitioning_strategy,\n","            w.pk_col\n","        FROM bq_information_schema_tables t\n","        LEFT JOIN watermark_cols w ON \n","            t.table_catalog = w.table_catalog AND\n","            t.table_schema = w.table_schema AND\n","            t.table_name = w.table_name\n","        LEFT JOIN partition_cfg p ON\n","            t.table_catalog = p.table_catalog AND\n","            t.table_schema = p.table_schema AND\n","            t.table_name = p.table_name\n","        \"\"\"\n","\n","        spark.sql(sql)\n","\n","    def sync_bq_information_schema_tables(self):\n","        bq_table = self.UserConfig.get_bq_table_fullname(SyncConstants.INFORMATION_SCHEMA_TABLES)\n","        tbl_nm = self.UserConfig.flatten_3part_tablename(SyncConstants.INFORMATION_SCHEMA_TABLES.replace(\".\", \"_\"))\n","\n","        bql = f\"\"\"\n","        SELECT *\n","        FROM {bq_table}\n","        WHERE table_type='BASE TABLE'\n","        AND table_name NOT LIKE '_bqc_%'\n","        \"\"\"\n","\n","        df = self.read_bq_to_dataframe(bql)\n","\n","        if not self.UserConfig.LoadAllTables:\n","            filter_list = self.UserConfig.get_table_name_list()\n","            df = df.filter(col(\"table_name\").isin(filter_list))    \n","\n","        self.write_lakehouse_table(df, self.UserConfig.MetadataLakehouse, tbl_nm)\n","\n","    def sync_bq_information_schema_table_dependent(self, dependent_tbl):\n","        bq_table = self.UserConfig.get_bq_table_fullname(SyncConstants.INFORMATION_SCHEMA_TABLES)\n","        bq_dependent_tbl = self.UserConfig.get_bq_table_fullname(dependent_tbl)\n","        tbl_nm = self.UserConfig.flatten_3part_tablename(dependent_tbl.replace(\".\", \"_\"))\n","\n","        bql = f\"\"\"\n","        SELECT c.*\n","        FROM {bq_dependent_tbl} c\n","        JOIN {bq_table} t ON \n","        t.table_catalog=c.table_catalog AND\n","        t.table_schema=c.table_schema AND\n","        t.table_name=c.table_name\n","        WHERE t.table_type='BASE TABLE'\n","        AND t.table_name NOT LIKE '_bqc_%'\n","        \"\"\"\n","\n","        df = self.read_bq_to_dataframe(bql)\n","\n","        if not self.UserConfig.LoadAllTables:\n","            filter_list = self.UserConfig.get_table_name_list()\n","            df = df.filter(col(\"table_name\").isin(filter_list)) \n","\n","        self.write_lakehouse_table(df, self.UserConfig.MetadataLakehouse, tbl_nm)\n","\n","    def sync_bq_metadata(self):\n","        self.sync_bq_information_schema_tables()\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_PARTITIONS)\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_COLUMNS)\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_TABLE_CONSTRAINTS)\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_KEY_COLUMN_USAGE)\n","\n","    def create_proxy_views(self):\n","        super().create_proxy_views()\n","\n","        if not spark.catalog.tableExists(\"bq_table_metadata_autodetect\"):\n","            self.create_autodetect_view()\n","\n","    def auto_detect_table_profiles(self):        \n","        sql = f\"\"\"\n","        WITH default_config AS (\n","            SELECT autodetect, target_lakehouse FROM user_config_json\n","        ),\n","        pk AS (\n","            SELECT\n","            a.table_catalog, a.table_schema, a.table_name, array_agg(COALESCE(a.pk_col, u.column)) as pk\n","            FROM bq_table_metadata_autodetect a\n","            LEFT JOIN user_config_table_keys u ON\n","                a.table_catalog = u.project_id AND\n","                a.table_schema = u.dataset AND\n","                a.table_name = u.table_name\n","            GROUP BY a.table_catalog, a.table_schema, a.table_name\n","        ),\n","        source AS (\n","            SELECT\n","                a.table_catalog as project_id,\n","                a.table_schema as dataset,\n","                a.table_name as table_name,\n","                COALESCE(u.enabled, TRUE) AS enabled,\n","                COALESCE(u.lakehouse, d.target_lakehouse) AS lakehouse,\n","                COALESCE(u.lakehouse_target_table, a.table_name) AS lakehouse_table_name,\n","                COALESCE(u.source_query, '') AS source_query,\n","                COALESCE(u.load_priority, '100') AS priority,\n","                CASE WHEN (COALESCE(u.watermark_column, a.pk_col) IS NOT NULL AND\n","                        COALESCE(u.watermark_column, a.pk_col) <> '') THEN 'WATERMARK' \n","                    WHEN (COALESCE(u.partition_enabled, a.is_partitioned) = TRUE) \n","                        AND COALESCE(u.partition_column, a.partition_col, '') NOT IN \n","                            ('_PARTITIONTIME', '_PARTITIONDATE') THEN 'PARTITION'\n","                    WHEN (COALESCE(u.partition_enabled, a.is_partitioned) = TRUE) \n","                        AND COALESCE(u.partition_column, a.partition_col, '') IN \n","                            ('_PARTITIONTIME', '_PARTITIONDATE') THEN 'TIME_INGESTION'\n","                    ELSE 'FULL' END AS load_strategy,\n","                CASE WHEN (COALESCE(u.watermark_column, a.pk_col) IS NOT NULL AND\n","                        COALESCE(u.watermark_column, a.pk_col) <> '') THEN 'APPEND' ELSE\n","                    'OVERWRITE' END AS load_type,\n","                COALESCE(u.interval, 'AUTO') AS interval,\n","                p.pk AS primary_keys,\n","                COALESCE(u.partition_enabled, a.is_partitioned) AS is_partitioned,\n","                COALESCE(u.partition_column, a.partition_col, '') AS partition_column,\n","                COALESCE(u.partition_type, a.partitioning_type, '') AS partition_type,\n","                COALESCE(u.partition_grain, a.partitioning_strategy, '') AS partition_grain,\n","                COALESCE(u.watermark_column, a.pk_col, '') AS watermark_column, \n","                d.autodetect,\n","                CASE WHEN u.table_name IS NULL THEN FALSE ELSE TRUE END AS config_override,\n","                'INIT' AS sync_state,\n","                CURRENT_TIMESTAMP() as created_dt,\n","                NULL as last_updated_dt\n","            FROM bq_table_metadata_autodetect a\n","            JOIN pk p ON\n","                a.table_catalog = p.table_catalog AND\n","                a.table_schema = p.table_schema AND\n","                a.table_name = p.table_name\n","            LEFT JOIN user_config_tables u ON \n","                a.table_catalog = u.project_id AND\n","                a.table_schema = u.dataset AND\n","                a.table_name = u.table_name\n","            CROSS JOIN default_config d\n","        )\n","\n","        MERGE INTO {SyncConstants.SQL_TBL_SYNC_CONFIG} t\n","        USING source s\n","        ON t.project_id = s.project_id AND\n","            t.dataset = s.dataset AND\n","            t.table_name = s.table_name\n","        WHEN MATCHED AND t.sync_state <> 'INIT' THEN\n","            UPDATE SET\n","                t.enabled = s.enabled,\n","                t.interval = s.interval,\n","                t.priority = s.priority,\n","                t.last_updated_dt = CURRENT_TIMESTAMP()\n","        WHEN MATCHED AND t.sync_state = 'INIT' THEN\n","            UPDATE SET *\n","        WHEN NOT MATCHED THEN\n","            INSERT *\n","        \"\"\"\n","\n","        spark.sql(sql)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"72b59d90-3d7f-4824-80fe-c61ea97dcd8a","statement_id":9,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-16T19:30:23.3418245Z","session_start_time":null,"execution_start_time":"2024-04-16T19:30:23.7257526Z","execution_finish_time":"2024-04-16T19:30:24.0644591Z","parent_msg_id":"9f4a1cf2-a7bb-45e9-8b17-507ef0e9e6b5"},"text/plain":"StatementMeta(, 72b59d90-3d7f-4824-80fe-c61ea97dcd8a, 9, Finished, Available)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a30a6ece-4ffa-47aa-bb2f-08d5dac28f83"},{"cell_type":"code","source":["class BQScheduleLoader(ConfigBase):\n","    def __init__(self, config_path, load_proxy_views=True, force_config_reload = False):\n","        super().__init__(config_path, force_config_reload)\n","        spark.sql(f\"USE {self.UserConfig.MetadataLakehouse}\")\n","\n","        if load_proxy_views:\n","            super().create_proxy_views()\n","\n","    def save_schedule_telemetry(self, schedule: SyncSchedule):\n","        tbl = f\"{SyncConstants.SQL_TBL_SYNC_SCHEDULE_TELEMETRY}\"\n","\n","        schema = spark.table(tbl).schema\n","\n","        rdd = spark.sparkContext.parallelize([Row(\n","            schedule_id=schedule.ScheduleId,\n","            project_id=schedule.ProjectId,\n","            dataset=schedule.Dataset,\n","            table_name=schedule.TableName,\n","            partition_id=schedule.PartitionId,\n","            status=\"COMPLETE\",\n","            started=schedule.StartTime,\n","            completed=schedule.EndTime,\n","            src_row_count=schedule.SourceRows,\n","            dest_row_count=schedule.DestRows,\n","            inserted_row_count=schedule.InsertedRows,\n","            updated_row_count=schedule.UpdatedRows,\n","            delta_version=schedule.DeltaVersion,\n","            spark_application_id=schedule.SparkAppId,\n","            max_watermark=schedule.MaxWatermark,\n","            summary_load=schedule.SummaryLoadType\n","        )])\n","\n","        df = spark.createDataFrame(rdd, schema)\n","        df.write.mode(SyncConstants.APPEND).saveAsTable(tbl)\n","\n","\n","    def get_table_delta_version(self, tbl):\n","        sql = f\"DESCRIBE HISTORY {tbl}\"\n","        df = spark.sql(sql) \\\n","            .select(max(col(\"version\")).alias(\"delta_version\"))\n","\n","        for row in df.collect():\n","            return row[\"delta_version\"]\n","\n","    def update_sync_config_state(self, project_id, dataset, table_name):\n","        sql = f\"\"\"\n","        UPDATE {SyncConstants.SQL_TBL_SYNC_CONFIG} \n","        SET sync_state='COMMIT' \n","        WHERE\n","            project_id='{project_id}' AND\n","            dataset='{dataset}' AND\n","            table_name='{table_name}'\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def get_schedule(self):\n","        sql = f\"\"\"\n","        WITH last_completed_schedule AS (\n","            SELECT schedule_id, project_id, dataset, table_name, max_watermark, started AS last_schedule_dt\n","            FROM (\n","                SELECT schedule_id, project_id, dataset, table_name, started, max_watermark,\n","                ROW_NUMBER() OVER(PARTITION BY project_id, dataset, table_name ORDER BY scheduled DESC) AS row_num\n","                FROM {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","                WHERE status='COMPLETE'\n","            )\n","            WHERE row_num = 1\n","        ),\n","        tbl_partitions AS (\n","            SELECT\n","                sp.table_catalog, sp.table_schema, sp.table_name, sp.partition_id\n","            FROM bq_information_schema_partitions sp\n","            JOIN {SyncConstants.SQL_TBL_SYNC_CONFIG} c ON\n","                sp.table_catalog = c.project_id AND \n","                sp.table_schema = c.dataset AND\n","                sp.table_name = c.table_name\n","            LEFT JOIN last_completed_schedule s ON \n","                sp.table_catalog = s.project_id AND \n","                sp.table_schema = s.dataset AND\n","                sp.table_name = s.table_name\n","            WHERE ((sp.last_modified_time >= s.last_schedule_dt) OR (s.last_schedule_dt IS NULL))\n","            AND \n","                ((c.load_strategy = 'PARTITION' AND s.last_schedule_dt IS NOT NULL) OR\n","                    c.load_strategy = 'TIME_INGESTION')\n","        )\n","\n","        SELECT c.*, \n","            p.partition_id,\n","            s.group_schedule_id,\n","            s.schedule_id,\n","            h.max_watermark,\n","            h.last_schedule_dt,\n","            CASE WHEN (h.schedule_id IS NULL) THEN TRUE ELSE FALSE END AS initial_load\n","        FROM {SyncConstants.SQL_TBL_SYNC_CONFIG} c\n","        JOIN {SyncConstants.SQL_TBL_SYNC_SCHEDULE} s ON \n","            c.project_id = s.project_id AND\n","            c.dataset = s.dataset AND\n","            c.table_name = s.table_name\n","        LEFT JOIN last_completed_schedule h ON\n","            c.project_id = h.project_id AND\n","            c.dataset = h.dataset AND\n","            c.table_name = h.table_name\n","        LEFT JOIN tbl_partitions p ON\n","            p.table_catalog = c.project_id AND \n","            p.table_schema = c.dataset AND\n","            p.table_name = c.table_name\n","        LEFT JOIN {SyncConstants.SQL_TBL_SYNC_SCHEDULE_TELEMETRY} t ON\n","            s.schedule_id = t.schedule_id AND\n","            c.project_id = t.project_id AND\n","            c.dataset = t.dataset AND\n","            c.table_name = t.table_name AND\n","            COALESCE(p.partition_id, '0') = COALESCE(t.partition_id, '0') AND\n","            t.status = 'COMPLETE'\n","        WHERE s.status = 'SCHEDULED'\n","            AND c.enabled = TRUE\n","            AND t.schedule_id IS NULL\n","            AND c.project_id = '{self.UserConfig.ProjectID}' \n","            AND c.dataset = '{self.UserConfig.Dataset}'\n","        ORDER BY c.priority\n","        \"\"\"\n","        df = spark.sql(sql)\n","        df.createOrReplaceTempView(\"LoaderQueue\")\n","        df.cache()\n","\n","        return df\n","\n","    def get_max_watermark(self, lakehouse_tbl, watermark_col):\n","        df = spark.table(lakehouse_tbl) \\\n","            .select(max(col(watermark_col)).alias(\"watermark\"))\n","\n","        for row in df.collect():\n","            return row[\"watermark\"]\n","\n","    def sync_bq_table(self, schedule:SyncSchedule):\n","        print(\"{0} {1}...\".format(schedule.SummaryLoadType, schedule.TableName))\n","\n","        if (schedule.LoadStrategy == SyncConstants.PARTITION or \\\n","                schedule.LoadStrategy == SyncConstants.TIME_INGESTION) and schedule.PartitionId is not None:\n","            print(\"Load by partition...\")\n","            src = f\"{schedule.BQTableName}\"\n","\n","            match schedule.PartitionGrain:\n","                case \"DAY\":\n","                    part_format = \"%Y%m%d\"\n","                case \"MONTH\":\n","                    part_format = \"%Y%m\"\n","                case \"YEAR\":\n","                    part_format = \"%Y\"\n","                case \"HOUR\":\n","                    part_format = \"%Y%m%d%H\"\n","                case _:\n","                    raise Exception(\"Unsupported Partition Grain in Table Config\")\n","\n","            if schedule.PartitionColumn == \"_PARTITIONTIME\":                   \n","                part_filter = f\"timestamp_trunc({schedule.PartitionColumn}, {schedule.PartitionGrain}) = PARSE_TIMESTAMP('{part_format}', '{schedule.PartitionId}')\"\n","            else:\n","                part_filter = f\"date_trunc({schedule.PartitionColumn}, {schedule.PartitionGrain}) = PARSE_DATETIME('{part_format}', '{schedule.PartitionId}')\"\n","\n","            df_bq = super().read_bq_partition_to_dataframe(src, part_filter)\n","        else:\n","            src = schedule.BQTableName     \n","\n","            if schedule.SourceQuery != \"\":\n","                src = schedule.SourceQuery\n","\n","            df_bq = super().read_bq_to_dataframe(src)\n","\n","        predicate = None\n","\n","        if schedule.LoadStrategy == SyncConstants.WATERMARK and not schedule.InitialLoad:\n","            pk = schedule.PrimaryKey\n","            max_watermark = schedule.MaxWatermark\n","\n","            if max_watermark.isdigit():\n","                predicate = f\"{pk} > {max_watermark}\"\n","            else:\n","                predicate = f\"{pk} > '{max_watermark}'\"\n","            \n","        if predicate is not None:\n","            df_bq = df_bq.where(predicate)\n","\n","        df_bq.cache()\n","\n","        partition = None\n","\n","        if schedule.IsPartitioned:\n","            print('Resolving Fabric partitioning...')\n","            if schedule.PartitionType == SyncConstants.TIME:\n","                partition_col = schedule.PartitionColumn\n","                if not schedule.IsTimeIngestionPartitioned:\n","                    part_format = \"\"\n","                    part_col_name = f\"__bq_part_{partition_col}\"\n","                    use_proxy_col = False\n","\n","                    match schedule.PartitionGrain:\n","                        case \"DAY\":\n","                            part_format = \"yyyyMMdd\"\n","\n","                            if dict(df_bq.dtypes)[partition_col] == \"date\":\n","                                partition = partition_col\n","                            else:\n","                                partition = f\"{part_col_name}_DAY\"\n","                                use_proxy_col = True\n","                        case \"MONTH\":\n","                            part_format = \"yyyyMM\"\n","                            partition = f\"{part_col_name}_MONTH\"\n","                            use_proxy_col = True\n","                        case \"YEAR\":\n","                            part_format = \"yyyy\"\n","                            partition = f\"{part_col_name}_YEAR\"\n","                            use_proxy_col = True\n","                        case \"HOUR\":\n","                            part_format = \"yyyyMMddHH\"\n","                            partition = f\"{part_col_name}_HOUR\"\n","                            use_proxy_col = True\n","                        case _:\n","                            print('Unsupported partition grain...')\n","                \n","                    print(\"{0} partitioning - partitioned by {1} (Requires Proxy Column: {2})\".format( \\\n","                        schedule.PartitionGrain, \\\n","                        partition, \\\n","                        use_proxy_col))\n","                    \n","                    if use_proxy_col:\n","                        df_bq = df_bq.withColumn(partition, date_format(col(partition_col), part_format))\n","                else:\n","                    part_format = \"\"\n","                    partition = f\"__bq{partition_col}\"\n","\n","                    match schedule.PartitionGrain:\n","                        case \"DAY\":\n","                            part_format = \"%Y%m%d\"\n","                        case \"MONTH\":\n","                            part_format = \"%Y%m\"\n","                        case \"YEAR\":\n","                            part_format = \"%Y\"\n","                        case \"HOUR\":\n","                            part_format = \"%Y%m%d%H\"\n","                        case _:\n","                            print('Unsupported partition grain...')\n","                    \n","                    print(\"Ingestion time partitioning - partitioned by {0} ({1})\".format(partition, schedule.PartitionId))\n","                    df_bq = df_bq.withColumn(partition, lit(schedule.PartitionId))\n","\n","\n","        if (schedule.LoadStrategy == SyncConstants.PARTITION or \\\n","                schedule.LoadStrategy == SyncConstants.TIME_INGESTION) and schedule.PartitionId is not None:\n","            print(f\"Writing {schedule.TableName}${schedule.PartitionId} partition...\")\n","            part_filter = f\"{partition} = '{schedule.PartitionId}'\"\n","\n","            df_bq.write \\\n","                .mode(SyncConstants.OVERWRITE) \\\n","                .option(\"replaceWhere\", part_filter) \\\n","                .saveAsTable(schedule.LakehouseTableName)\n","        else:\n","            if partition is None:\n","                df_bq.write \\\n","                    .mode(schedule.Mode) \\\n","                    .saveAsTable(schedule.LakehouseTableName)\n","            else:\n","                df_bq.write \\\n","                    .partitionBy(partition) \\\n","                    .mode(schedule.Mode) \\\n","                    .saveAsTable(schedule.LakehouseTableName)\n","\n","        if schedule.LoadStrategy == SyncConstants.WATERMARK:\n","            schedule.MaxWatermark = self.get_max_watermark(schedule.LakehouseTableName, schedule.PrimaryKey)\n","\n","        src_cnt = df_bq.count()\n","\n","        if (schedule.LoadStrategy == SyncConstants.PARTITION or \\\n","                schedule.LoadStrategy == SyncConstants.TIME_INGESTION)  and schedule.PartitionId is not None:\n","            dest_cnt = src_cnt\n","        else:\n","            dest_cnt = spark.table(schedule.LakehouseTableName).count()\n","\n","        schedule.UpdateRowCounts(src_cnt, dest_cnt, 0, 0)    \n","        schedule.SparkAppId = spark.sparkContext.applicationId\n","        schedule.DeltaVersion = self.get_table_delta_version(schedule.LakehouseTableName)\n","        schedule.EndTime = datetime.now(timezone.utc)\n","\n","        df_bq.unpersist()\n","\n","        return schedule\n","\n","    def process_load_group_telemetry(self, load_grp = None):\n","        load_grp_filter = \"\"\n","\n","        if load_grp is not None:\n","            load_grp_filter = f\"AND r.priority = '{load_grp}'\"\n","\n","        sql = f\"\"\"\n","        WITH schedule_telemetry AS (\n","                SELECT\n","                        schedule_id,\n","                        project_id,\n","                        dataset,\n","                        table_name,\n","                        SUM(CASE WHEN status='COMPLETE' THEN 1 ELSE 0 END) AS completed_activities,\n","                        SUM(CASE WHEN status='FAILED' THEN 1 ELSE 0 END) AS failed_activities,\n","                        MIN(started) as started,\n","                        MAX(completed) as completed\n","                FROM bq_sync_schedule_telemetry\n","                GROUP BY\n","                schedule_id,\n","                project_id,\n","                dataset,\n","                table_name\n","        ),\n","        schedule_watermarks AS (\n","                SELECT\n","                        schedule_id,\n","                        project_id,\n","                        dataset,\n","                        table_name,\n","                        max_watermark,\n","                        ROW_NUMBER() OVER(PARTITION BY schedule_id,\n","                                project_id,\n","                                dataset,\n","                                table_name ORDER BY completed DESC) AS row_num\n","                FROM bq_sync_schedule_telemetry\n","                WHERE max_watermark IS NOT NULL\n","        ),\n","        schedule_results AS (\n","                SELECT\n","                        s.schedule_id,\n","                        s.project_id,\n","                        s.dataset,\n","                        s.table_name,\n","                        s.status,\n","                        CASE WHEN t.failed_activities = 0 THEN 'COMPLETE' ELSE 'FAILED' END AS result_status,\n","                        t.started,\n","                        t.completed,\n","                        t.completed_activities,\n","                        t.failed_activities,\n","                        w.max_watermark,\n","                        s.priority \n","                FROM bq_sync_schedule s\n","                JOIN schedule_telemetry t ON \n","                        s.schedule_id = t.schedule_id AND\n","                        s.project_id = t.project_id AND\n","                        s.dataset = t.dataset AND\n","                        s.table_name = t.table_name\n","                LEFT JOIN schedule_watermarks w ON\n","                        s.schedule_id = w.schedule_id AND\n","                        s.project_id = w.project_id AND\n","                        s.dataset = w.dataset AND\n","                        s.table_name = w.table_name\n","        )  \n","\n","        MERGE INTO bq_sync_schedule s\n","        USING ( \n","                SELECT *\n","                FROM schedule_results r\n","                WHERE r.status='SCHEDULED'\n","                {load_grp_filter}\n","        ) r\n","        ON s.schedule_id = r.schedule_id AND\n","                s.project_id = r.project_id AND\n","                s.dataset = r.dataset AND\n","                s.table_name = r.table_name\n","        WHEN MATCHED THEN\n","                UPDATE SET\n","                        s.status = r.result_status,\n","                        s.started = r.started,\n","                        s.completed = r.completed,\n","                        s.completed_activities = r.completed_activities,\n","                        s.failed_activities = r.failed_activities,\n","                        s.max_watermark = r.max_watermark\n","\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def commit_table_configuration(self):\n","        sql = \"\"\"\n","        WITH committed AS (\n","            SELECT project_id, dataset, table_name, MAX(started) as started\n","            FROM bq_sync_schedule\n","            WHERE status='COMPLETE'\n","            GROUP BY project_id, dataset, table_name\n","        )\n","\n","        MERGE INTO bq_sync_configuration t\n","        USING committed c\n","        ON t.project_id=c.project_id\n","        AND t.dataset=c.dataset\n","        AND t.table_name=c.table_name\n","        WHEN MATCHED AND t.sync_state='INIT' THEN\n","            UPDATE SET\n","                t.sync_state='COMMIT'\n","        \"\"\"\n","        spark.sql(sql)\n","\n","\n","    def run_sequential_schedule(self):\n","        df_schedule = self.get_schedule()\n","\n","        for row in df_schedule.collect():\n","            schedule = SyncSchedule(row)\n","\n","            self.sync_bq_table(schedule)\n","\n","            self.save_schedule_telemetry(schedule)  \n","\n","        self.process_load_group_telemetry()\n","        self.commit_table_configuration()\n","    \n","    def run_aync_schedule(self):\n","        dag = ScheduleDAG(timeout=self.UserConfig.Async.NotebookTimeout, \\\n","            concurrency=self.UserConfig.Async.Parallelism)\n","\n","        schedule = self.get_schedule()\n","\n","        load_grps = [i[\"priority\"] for i in schedule.select(\"priority\").distinct().orderBy(\"priority\").collect()]\n","\n","        grp_dependency = None\n","\n","        for grp in load_grps:\n","            checkpoint_dependencies = []\n","            grp_nm = \"GROUP_{0}\".format(grp)\n","            grp_df = schedule.where(f\"priority = '{grp}'\")\n","\n","            for tbl in grp_df.collect():\n","                nm = \"{0}.{1}\".format(tbl[\"dataset\"], tbl[\"table_name\"])\n","                dependencies = []\n","\n","                if tbl[\"partition_id\"] is not None:\n","                    nm = \"{0}${1}\".format(nm, tbl[\"partition_id\"])\n","\n","                if grp_dependency is not None:\n","                    dependencies.append(grp_dependency)\n","                \n","                dag.activities.append( \\\n","                    DAGActivity(nm, \"BQ_TBL_PART_LOADER\", \\\n","                        self.UserConfig.Async.CellTimeout, \\\n","                        None, None, \\\n","                        dependencies, \\\n","                        schedule_id=tbl[\"schedule_id\"], \\\n","                        project_id=tbl[\"project_id\"], \\\n","                        dataset=tbl[\"dataset\"], \\\n","                        table_name=tbl[\"table_name\"], \\\n","                        partition_id=tbl[\"partition_id\"], \\\n","                        config_json_path=config_json_path))\n","\n","                checkpoint_dependencies.append(nm)\n","                print(f\"Load Activity: {nm}\")\n","\n","            \n","            grp_dependency = grp_nm\n","            print(f\"Load Group Checkpoint: {grp_nm}\")\n","            dag.activities.append( \\\n","                DAGActivity(grp_nm, \"BQ_LOAD_GROUP_CHECKPOINT\", \\\n","                    self.UserConfig.Async.CellTimeout, \\\n","                    None, None, \\\n","                    checkpoint_dependencies, \\\n","                    load_group=grp, \\\n","                    config_json_path=self.ConfigPath))\n","        \n","        dag_json = json.dumps(dag, indent=4, cls=ScheduleDAGEncoder)\n","        #print(dag_json)\n","        schedule_dag = json.loads(dag_json)\n","\n","        dag_result = mssparkutils.notebook.runMultiple(schedule_dag, {\"displayDAGViaGraphviz\":True, \"DAGLayout\":\"spectral\", \"DAGSize\":8})\n","\n","        self.commit_table_configuration()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"188d4f89-f45c-455e-8263-facaf270c5df","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-12T13:48:56.460091Z","session_start_time":null,"execution_start_time":"2024-04-12T13:49:00.8709995Z","execution_finish_time":"2024-04-12T13:49:01.251321Z","parent_msg_id":"0ccd252d-59b8-4469-8176-4ebfb4e282dc"},"text/plain":"StatementMeta(, 188d4f89-f45c-455e-8263-facaf270c5df, 13, Finished, Available)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"86235e45-0c89-45e9-b017-9466ec0dc54a"},{"cell_type":"code","source":["class SyncSetup(ConfigBase):\n","    def __init__(self, config_path):\n","        if spark.catalog.tableExists(\"user_config_json\"):\n","            spark.catalog.dropTempView(\"user_config_json\")\n","\n","        super().__init__(config_path)\n","\n","    def get_fabric_lakehouse(self, nm):\n","        lakehouse = None\n","\n","        try:\n","            lakehouse = mssparkutils.lakehouse.get(nm)\n","        except Exception:\n","            print(\"Lakehouse not found: {0}\".format(nm))\n","\n","        return lakehouse\n","\n","    def create_fabric_lakehouse(self, nm):\n","        lakehouse = self.get_fabric_lakehouse(nm)\n","\n","        if (lakehouse is None):\n","            print(\"Creating Lakehouse {0}...\".format(nm))\n","            mssparkutils.lakehouse.create(nm)\n","\n","    def setup(self):\n","        self.create_fabric_lakehouse(self.UserConfig.MetadataLakehouse)\n","        self.create_fabric_lakehouse(self.UserConfig.TargetLakehouse)\n","        spark.sql(f\"USE {self.UserConfig.MetadataLakehouse}\")\n","        self.create_all_tables()\n","\n","    def drop_table(self, tbl):\n","        sql = f\"DROP TABLE IF EXISTS {tbl}\"\n","        spark.sql(sql)\n","\n","    def get_tbl_name(self, tbl):\n","        return self.UserConfig.get_lakehouse_tablename(self.UserConfig.MetadataLakehouse, tbl)\n","\n","    def create_data_type_map_tbl(self):\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_DATA_TYPE_MAP)\n","        self.drop_table(tbl_nm)\n","\n","        sql = f\"\"\"CREATE TABLE IF NOT EXISTS {tbl_nm} (data_type STRING, partition_type STRING, is_watermark STRING)\"\"\"\n","        spark.sql(sql)\n","\n","        df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/data/bq_data_types.csv\")\n","        df.write.mode(\"OVERWRITE\").saveAsTable(tbl_nm)\n","\n","    def create_sync_config_tbl(self):\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_SYNC_CONFIG)\n","        self.drop_table(tbl_nm)\n","\n","        sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {tbl_nm}\n","        (\n","            project_id STRING,\n","            dataset STRING,\n","            table_name STRING,\n","            enabled BOOLEAN,\n","            lakehouse STRING,\n","            lakehouse_table_name STRING,\n","            source_query STRING,\n","            priority INTEGER,\n","            load_strategy STRING,\n","            load_type STRING,\n","            interval STRING,\n","            primary_keys ARRAY<STRING>,\n","            is_partitioned BOOLEAN,\n","            partition_column STRING,\n","            partition_type STRING,\n","            partition_grain STRING,\n","            watermark_column STRING,\n","            autodetect BOOLEAN,\n","            config_override BOOLEAN,\n","            sync_state STRING,\n","            created_dt TIMESTAMP,\n","            last_updated_dt TIMESTAMP\n","        )\n","        \"\"\"\n","        spark.sql(sql)\n","    \n","    def create_sync_schedule_tbl(self):\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_SYNC_SCHEDULE)\n","        self.drop_table(tbl_nm)\n","\n","        sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {tbl_nm} (\n","            group_schedule_id STRING,\n","            schedule_id STRING,\n","            project_id STRING,\n","            dataset STRING,\n","            table_name STRING,\n","            scheduled TIMESTAMP,\n","            status STRING,\n","            started TIMESTAMP,\n","            completed TIMESTAMP,\n","            completed_activities INT,\n","            failed_activities INT,\n","            max_watermark STRING,\n","            priority INTEGER\n","        )\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def create_sync_schedule_telemetry_tbl(self):\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_SYNC_SCHEDULE_TELEMETRY)\n","        self.drop_table(tbl_nm)\n","\n","        sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {tbl_nm} (\n","            schedule_id STRING,\n","            project_id STRING,\n","            dataset STRING,\n","            table_name STRING,\n","            partition_id STRING,\n","            status STRING,\n","            started TIMESTAMP,\n","            completed TIMESTAMP,\n","            src_row_count BIGINT,\n","            dest_row_count BIGINT,\n","            inserted_row_count BIGINT,\n","            updated_row_count BIGINT,\n","            delta_version BIGINT,\n","            spark_application_id STRING,\n","            max_watermark STRING,\n","            summary_load STRING\n","        )\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def create_all_tables(self):\n","        self.create_data_type_map_tbl()\n","        self.create_sync_config_tbl()\n","        self.create_sync_schedule_tbl()\n","        self.create_sync_schedule_telemetry_tbl()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"188d4f89-f45c-455e-8263-facaf270c5df","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-12T13:48:56.8332918Z","session_start_time":null,"execution_start_time":"2024-04-12T13:49:01.7377599Z","execution_finish_time":"2024-04-12T13:49:02.0871756Z","parent_msg_id":"710506e8-b37e-4be7-9464-1cbe5d98cc21"},"text/plain":"StatementMeta(, 188d4f89-f45c-455e-8263-facaf270c5df, 14, Finished, Available)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e37812cf-b115-44e2-b629-64ae1fc6eb5c"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"9c7403b4-606d-412c-9224-64ccdbff9cc3","default_lakehouse_name":"BQ_Metadata","default_lakehouse_workspace_id":"0fb2d96e-53e7-4c49-a594-beb0891ac121"}}},"nbformat":4,"nbformat_minor":5}