{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\n","from delta.tables import *\n","from datetime import datetime, timezone\n","import json\n","import base64\n","from pathlib import Path\n","import os"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bab08844-21d0-4e24-a0eb-ae15aa15e06a","statement_id":35,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-15T12:35:07.2473621Z","session_start_time":null,"execution_start_time":"2024-04-15T12:35:07.6633907Z","execution_finish_time":"2024-04-15T12:35:08.0216359Z","parent_msg_id":"a594f063-685e-45e8-b975-a95851004a69"},"text/plain":"StatementMeta(, bab08844-21d0-4e24-a0eb-ae15aa15e06a, 35, Finished, Available)"},"metadata":{}}],"execution_count":33,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"51d29545-50c9-44b9-a5c6-7d1d674fee14"},{"cell_type":"code","source":["class SyncConstants:\n","    OVERWRITE = \"OVERWRITE\"\n","    APPEND = \"APPEND\"\n","    FULL = \"FULL\"\n","    PARTITION = \"PARTITION\"\n","    WATERMARK = \"WATERMARK\"\n","    AUTO = \"AUTO\"\n","    TIME = \"TIME\"\n","    INITIAL_FULL_OVERWRITE = \"INITIAL_FULL_OVERWRITE\"\n","    INFORMATION_SCHEMA_TABLES = \"INFORMATION_SCHEMA.TABLES\"\n","    INFORMATION_SCHEMA_PARTITIONS = \"INFORMATION_SCHEMA.PARTITIONS\"\n","    INFORMATION_SCHEMA_COLUMNS = \"INFORMATION_SCHEMA.COLUMNS\"\n","    INFORMATION_SCHEMA_TABLE_CONSTRAINTS = \"INFORMATION_SCHEMA.TABLE_CONSTRAINTS\"\n","    INFORMATION_SCHEMA_KEY_COLUMN_USAGE = \"INFORMATION_SCHEMA.KEY_COLUMN_USAGE\"\n","\n","    SQL_TBL_SYNC_SCHEDULE_PARTITION = \"bq_sync_schedule_partition\"\n","    SQL_TBL_SYNC_SCHEDULE = \"bq_sync_schedule\"\n","    SQL_TBL_SYNC_CONFIG = \"bq_sync_configuration\"\n","    SQL_TBL_DATA_TYPE_MAP = \"bq_data_type_map\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bab08844-21d0-4e24-a0eb-ae15aa15e06a","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-15T11:37:46.3041787Z","session_start_time":null,"execution_start_time":"2024-04-15T11:37:46.8004218Z","execution_finish_time":"2024-04-15T11:37:47.190507Z","parent_msg_id":"1ab187e6-e845-4ab8-9a2d-4527ba28297a"},"text/plain":"StatementMeta(, bab08844-21d0-4e24-a0eb-ae15aa15e06a, 14, Finished, Available)"},"metadata":{}}],"execution_count":12,"metadata":{},"id":"27ba34e9-1354-477a-9680-ee29cebba906"},{"cell_type":"code","source":["class ConfigBase():\n","    def __init__(self, config_path, gcp_credential):\n","        if config_path is None:\n","            raise ValueError(\"Missing Path to JSON User Config\")\n","        \n","        if gcp_credential is None:\n","            raise ValueError(\"Missing GCP Credentials\")\n","\n","        self.ConfigPath = config_path\n","        self.UserConfig = None\n","        self.GCPCredential = None\n","\n","        self.UserConfig = self.ensure_user_config()\n","        self.GCPCredential = self.load_gcp_credential(gcp_credential)\n","    \n","    def ensure_user_config(self):\n","        if self.UserConfig is None and self.ConfigPath is not None:\n","            config = self.load_user_config(self.ConfigPath)\n","\n","            cfg = ConfigDataset(config)\n","\n","            self.validate_user_config(cfg)\n","            \n","            return cfg\n","        else:\n","            return self.UserConfig\n","    \n","    def load_user_config(self, config_path):\n","        config_df = spark.read.option(\"multiline\",\"true\").json(config_path)\n","        config_df.createOrReplaceTempView(\"user_config_json\")\n","        config_df.cache()\n","        return json.loads(config_df.toJSON().first())\n","\n","    def validate_user_config(self, cfg):\n","        if cfg is None:\n","            raise RuntimeError(\"Invalid User Config\")    \n","        return True\n","\n","    def load_gcp_credential(self, credential):\n","        cred = None\n","\n","        if self.is_base64(credential):\n","            cred = credential\n","        else:\n","            if os.path.exists(credential):\n","                file_contents = self.read_credential_file(credential)\n","                cred = self.convert_to_base64string(file_contents)\n","            else:\n","                raise ValueError(\"Invalid GCP Credential path supplied.\")\n","\n","        return cred\n","\n","    def read_credential_file(self, credential_path):\n","        txt = Path(credential_path).read_text()\n","        txt = txt.replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n","\n","        return txt\n","\n","    def convert_to_base64string(self, credential_val):\n","        credential_val_bytes = credential_val.encode(\"ascii\") \n","        \n","        base64_bytes = base64.b64encode(credential_val_bytes) \n","        base64_string = base64_bytes.decode(\"ascii\") \n","\n","        return base64_string\n","\n","    def is_base64(self, val):\n","        try:\n","                if isinstance(val, str):\n","                        sb_bytes = bytes(val, 'ascii')\n","                elif isinstance(val, bytes):\n","                        sb_bytes = val\n","                else:\n","                        raise ValueError(\"Argument must be string or bytes\")\n","                return base64.b64encode(base64.b64decode(sb_bytes)) == sb_bytes\n","        except Exception:\n","                return False\n","\n","    def read_bq_to_dataframe(self, query, cache_results=False):\n","        df = spark.read \\\n","            .format(\"bigquery\") \\\n","            .option(\"parentProject\", self.UserConfig.ProjectID) \\\n","            .option(\"credentials\", self.GCPCredential) \\\n","            .option(\"viewsEnabled\", \"true\") \\\n","            .option(\"materializationDataset\", self.UserConfig.Dataset) \\\n","            .load(query)\n","        \n","        if cache_results:\n","            df.cache()\n","        \n","        return df\n","\n","    def write_lakehouse_table(self, df, lakehouse, tbl_nm, mode=SyncConstants.OVERWRITE):\n","        dest_table = self.UserConfig.get_lakehouse_tablename(lakehouse, tbl_nm)\n","\n","        df.write \\\n","            .mode(mode) \\\n","            .saveAsTable(dest_table)\n","    \n","    def create_infosys_proxy_view(self, trgt):\n","        clean_nm = trgt.replace(\".\", \"_\")\n","        tbl = self.UserConfig.flatten_3part_tablename(clean_nm)\n","        lakehouse_tbl = self.UserConfig.get_lakehouse_tablename(self.UserConfig.MetadataLakehouse, tbl)\n","\n","        sql = f\"\"\"\n","        CREATE OR REPLACE TEMPORARY VIEW BQ_{clean_nm}\n","        AS\n","        SELECT *\n","        FROM {lakehouse_tbl}\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def create_userconfig_tables_proxy_view(self):\n","        sql = \"\"\"\n","            CREATE OR REPLACE TEMPORARY VIEW user_config_tables\n","            AS\n","            SELECT\n","                project_id, dataset, tbl.table_name,\n","                tbl.enabled,tbl.load_priority,tbl.source_query,\n","                tbl.load_strategy,tbl.load_type,tbl.interval,\n","                tbl.watermark.column as watermark_column,\n","                tbl.partitioned.enabled as partition_enabled,\n","                tbl.partitioned.type as partition_type,\n","                tbl.partitioned.column as partition_column,\n","                tbl.partitioned.partition_grain,\n","                tbl.lakehouse_target.lakehouse,\n","                tbl.lakehouse_target.table_name AS lakehouse_target_table,\n","                tbl.keys\n","            FROM (SELECT project_id, dataset, EXPLODE(tables) AS tbl FROM user_config_json)\n","        \"\"\"\n","        spark.sql (sql)\n","\n","    def create_userconfig_tables_cols_proxy_view(self):\n","        sql = \"\"\"\n","            CREATE OR REPLACE TEMPORARY VIEW user_config_table_keys\n","            AS\n","            SELECT\n","                project_id, dataset, table_name, pkeys.column\n","            FROM (\n","                SELECT\n","                    project_id, dataset, tbl.table_name, EXPLODE(tbl.keys) AS pkeys\n","                FROM (SELECT project_id, dataset, EXPLODE(tables) AS tbl FROM user_config_json)\n","            )\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def create_proxy_views(self):\n","        self.create_userconfig_tables_proxy_view()\n","        self.create_userconfig_tables_cols_proxy_view()\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_TABLES)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_PARTITIONS)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_COLUMNS)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_TABLE_CONSTRAINTS)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_KEY_COLUMN_USAGE)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bab08844-21d0-4e24-a0eb-ae15aa15e06a","statement_id":43,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-15T12:40:54.9223564Z","session_start_time":null,"execution_start_time":"2024-04-15T12:40:55.3966627Z","execution_finish_time":"2024-04-15T12:40:55.7506098Z","parent_msg_id":"1756db3c-59e5-490b-8a0f-1e8ec7e35409"},"text/plain":"StatementMeta(, bab08844-21d0-4e24-a0eb-ae15aa15e06a, 43, Finished, Available)"},"metadata":{}}],"execution_count":41,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0c6a4adf-5b5a-44e1-840f-553c14cf5b8a"},{"cell_type":"code","source":["class Scheduler(ConfigBase):\n","    def __init__(self, config_path, gcp_credential):\n","        super().__init__(config_path, gcp_credential)\n","\n","    def run(self):\n","        sql = f\"\"\"\n","        WITH new_schedule AS ( \n","            SELECT CURRENT_TIMESTAMP() as scheduled\n","        ),\n","        last_bq_tbl_updates AS (\n","            SELECT table_catalog, table_schema, table_name, max(last_modified_time) as last_bq_tbl_update\n","            FROM bq_information_schema_partitions\n","            GROUP BY table_catalog, table_schema, table_name\n","        ),\n","        last_load AS (\n","            SELECT project_id, dataset, table_name, MAX(started) AS last_load_update\n","            FROM {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","            WHERE status='COMPLETE'\n","            GROUP BY project_id, dataset, table_name\n","        ),\n","        schedule AS (\n","            SELECT\n","                UUID() AS schedule_id,\n","                c.project_id,\n","                c.dataset,\n","                c.table_name,\n","                n.scheduled,\n","                CASE WHEN ((l.last_load_update IS NULL) OR\n","                     (b.last_bq_tbl_update >= l.last_load_update))\n","                    THEN 'SCHEDULED' ELSE 'SKIPPED' END as status,\n","                NULL as started,\n","                NULL as completed,\n","                NULL as src_row_count,\n","                NULL as dest_row_count,\n","                NULL as dest_inserted_row_count,\n","                NULL as dest_updated_row_count,\n","                NULL as delta_version,\n","                NULL as spark_application_id,\n","                NULL as max_watermark,\n","                NULL as summary_load,\n","                c.priority\n","            FROM {SyncConstants.SQL_TBL_SYNC_CONFIG} c \n","            LEFT JOIN {SyncConstants.SQL_TBL_SYNC_SCHEDULE} s ON \n","                c.project_id= s.project_id AND\n","                c.dataset = s.dataset AND\n","                c.table_name = s.table_name AND\n","                s.status = 'SCHEDULED'\n","            LEFT JOIN last_bq_tbl_updates b ON\n","                c.project_id= b.table_catalog AND\n","                c.dataset = b.table_schema AND\n","                c.table_name = b.table_name\n","            LEFT JOIN last_load l ON \n","                c.project_id= l.project_id AND\n","                c.dataset = l.dataset AND\n","                c.table_name = l.table_name\n","            CROSS JOIN new_schedule n\n","            WHERE s.schedule_id IS NULL\n","            AND c.enabled = TRUE\n","        )\n","\n","        INSERT INTO {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","        SELECT * FROM schedule s\n","        WHERE s.project_id = '{self.UserConfig.ProjectID}'\n","        AND s.dataset = '{self.UserConfig.Dataset}'\n","        \"\"\"\n","        spark.sql(sql)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bab08844-21d0-4e24-a0eb-ae15aa15e06a","statement_id":15,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-15T11:37:47.3149576Z","session_start_time":null,"execution_start_time":"2024-04-15T11:37:47.698973Z","execution_finish_time":"2024-04-15T11:37:48.0583334Z","parent_msg_id":"11d41a6b-1ba6-4c63-b0e7-d4daae540bdb"},"text/plain":"StatementMeta(, bab08844-21d0-4e24-a0eb-ae15aa15e06a, 15, Finished, Available)"},"metadata":{}}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fdb1ac1d-8bb9-4a40-94a2-87721b3844be"},{"cell_type":"code","source":["class SyncSchedule:\n","    EndTime = None\n","    SourceRows = 0\n","    DestRows = 0\n","    InsertedRows = 0\n","    UpdatedRows = 0\n","    DeltaVersion = None\n","    SparkAppId = None\n","    MaxWatermark = None\n","    Status = None\n","\n","    def __init__(self, row):\n","        self.Row = row\n","        self.StartTime = datetime.now(timezone.utc)\n","        self.ScheduleId = row[\"schedule_id\"]\n","        self.LoadStrategy = row[\"load_strategy\"]\n","        self.LoadType = row[\"load_type\"]\n","        self.InitialLoad = row[\"initial_load\"]\n","        self.ProjectId = row[\"project_id\"]\n","        self.Dataset = row[\"dataset\"]\n","        self.TableName = row[\"table_name\"]\n","        self.SourceQuery = row[\"source_query\"]\n","        self.MaxWatermark = row[\"max_watermark\"]\n","        self.IsPartitioned = row[\"is_partitioned\"]\n","        self.PartitionColumn = row[\"partition_column\"]\n","        self.PartitionType = row[\"partition_type\"]\n","        self.PartitionGrain = row[\"partition_grain\"]\n","        self.WatermarkColumn = row[\"watermark_column\"]\n","        self.LastScheduleLoadDate = row[\"last_schedule_dt\"]\n","        self.Lakehouse = row[\"lakehouse\"]\n","        self.DestinationTableName = row[\"lakehouse_table_name\"]\n","    \n","    @property\n","    def SummaryLoadType(self):\n","        if self.InitialLoad:\n","            return SyncConstants.INITIAL_FULL_OVERWRITE\n","        else:\n","            return \"{0}_{1}\".format(self.LoadStrategy, self.LoadType)\n","    \n","    @property\n","    def Mode(self):\n","        if self.InitialLoad:\n","            return SyncConstants.OVERWRITE\n","        else:\n","            return self.LoadType\n","    \n","    @property\n","    def PrimaryKey(self):\n","        if self.Row[\"primary_keys\"]:\n","            return self.Row[\"primary_keys\"][0]\n","        else:\n","            return None\n","    \n","    @property\n","    def LakehouseTableName(self):\n","        return \"{0}.{1}\".format(self.Lakehouse, self.DestinationTableName)\n","        \n","    @property\n","    def BQTableName(self):\n","        return \"{0}.{1}.{2}\".format(self.ProjectId, self.Dataset, self.TableName)\n","\n","    @property\n","    def IsTimeIngestionPartitioned(self):\n","        is_time = False\n","\n","        if self.PartitionColumn == \"_PARTITIONTIME\" or self.PartitionColumn == \"_PARTITIONDATE\":\n","            is_time = True;\n","\n","        return is_time;\n","\n","\n","    def UpdateRowCounts(self, src, dest, insert, update):\n","        self.SourceRows += src\n","        self.DestRows += dest\n","\n","        match self.LoadStrategy:\n","            case SyncConstants.WATERMARK:\n","                self.InsertedRows += src     \n","            case SyncConstants.PARTITION:\n","                self.InsertedRows += dest  \n","            case _:\n","                self.InsertedRows += dest\n","\n","        self.UpdatedRows = 0"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bab08844-21d0-4e24-a0eb-ae15aa15e06a","statement_id":16,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-15T11:37:48.8590137Z","session_start_time":null,"execution_start_time":"2024-04-15T11:37:49.2112828Z","execution_finish_time":"2024-04-15T11:37:49.5679739Z","parent_msg_id":"9fff4acf-bb42-4cd5-844d-4795a11077e4"},"text/plain":"StatementMeta(, bab08844-21d0-4e24-a0eb-ae15aa15e06a, 16, Finished, Available)"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f8235f42-25df-451d-8083-f8b4f7cc8876"},{"cell_type":"code","source":["class ConfigDataset:\n","    def __init__(self, json_config):\n","        self.ProjectID = self.get_json_conf_val(json_config, \"project_id\", None)\n","        self.Dataset = self.get_json_conf_val(json_config, \"dataset\", None)\n","        self.LoadAllTables = self.get_json_conf_val(json_config, \"load_all_tables\", True)\n","        self.Autodetect = self.get_json_conf_val(json_config, \"autodetect\", True)\n","        self.MasterReset = self.get_json_conf_val(json_config, \"master_reset\", False)\n","        self.MetadataLakehouse = self.get_json_conf_val(json_config, \"metadata_lakehouse\", None)\n","        self.TargetLakehouse = self.get_json_conf_val(json_config, \"target_lakehouse\", None)\n","\n","        self.Tables = []\n","\n","        if \"tables\" in json_config:\n","            for t in json_config[\"tables\"]:\n","                self.Tables.append(ConfigBQTable(t))\n","    \n","    def get_delimited_tables_list(self):\n","            return ''.join([str(x.TableName) for x in self.Tables])\n","\n","    def get_table_name_list(self):\n","        return [str(x.TableName) for x in self.Tables]\n","\n","    def get_bq_table_fullname(self, tbl_name):\n","        return f\"{self.ProjectID}.{self.Dataset}.{tbl_name}\"\n","\n","    def get_lakehouse_tablename(self, lakehouse, tbl_name):\n","        return f\"{lakehouse}.{tbl_name}\"\n","\n","    def flatten_3part_tablename(self, tbl_name):\n","        clean_project_id = self.ProjectID.replace(\"-\", \"_\")\n","        return f\"{clean_project_id}_{self.Dataset}_{tbl_name}\"\n","    \n","    def get_json_conf_val(self, json, config_key, default_val = None):\n","        if config_key in json:\n","            return json[config_key]\n","        else:\n","            return default_val\n","\n","class ConfigTableColumn:\n","    def __init__(self, col = \"\"):\n","        self.Column = col\n","\n","class ConfigLakehouseTarget:\n","    def __init__(self, lakehouse = \"\", table = \"\"):\n","        self.Lakehouse = lakehouse\n","        self.Table = table\n","\n","class ConfigPartition:\n","    def __init__(self, enabled = False, partition_type = \"\", col = ConfigTableColumn(), grain = \"\"):\n","        self.Enabled = enabled\n","        self.PartitionType = partition_type\n","        self.PartitionColumn = col\n","        self.Granularity = grain\n","\n","class ConfigBQTable:\n","    def __str__(self):\n","        return str(self.TableName)\n","\n","    def __init__(self, json_config):\n","        self.TableName = self.get_json_conf_val(json_config, \"table_name\", \"\")\n","        self.Priority = self.get_json_conf_val(json_config, \"priority\", 100)\n","        self.SourceQuery = self.get_json_conf_val(json_config, \"source_query\", \"\")\n","        self.LoadStrategy = self.get_json_conf_val(json_config, \"load_strategy\" , SyncConstants.FULL)\n","        self.LoadType = self.get_json_conf_val(json_config, \"load_type\", SyncConstants.OVERWRITE)\n","        self.Interval =  self.get_json_conf_val(json_config, \"interval\", SyncConstants.AUTO)\n","        self.Enabled =  self.get_json_conf_val(json_config, \"enabled\", True)\n","\n","        if \"lakehouse_target\" in json_config:\n","            self.LakehouseTarget = ConfigLakehouseTarget( \\\n","                self.get_json_conf_val(json_config[\"lakehouse_target\"], \"lakehouse\", \"\"), \\\n","                self.get_json_conf_val(json_config[\"lakehouse_target\"], \"table_name\", \"\"))\n","        else:\n","            self.LakehouseTarget = ConfigLakehouseTarget()\n","        \n","        if \"watermark\" in json_config:\n","            self.Watermark = ConfigTableColumn( \\\n","                self.get_json_conf_val(json_config[\"watermark\"], \"column\", \"\"))\n","        else:\n","            self.Watermark = ConfigTableColumn()\n","\n","        if \"partitioned\" in json_config:\n","            self.Partitioned = ConfigPartition( \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"enabled\", False), \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"type\", \"\"), \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"column\", \"\"), \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"partition_grain\", \"\"))\n","        else:\n","            self.Partitioned = ConfigPartition()\n","        \n","        self.Keys = []\n","\n","        if \"keys\" in json_config:\n","            for c in json_config[\"keys\"]:\n","                self.Keys.append(ConfigTableColumn( \\\n","                    self.get_json_conf_val(c, \"column\", \"\")))\n","        \n","    def get_json_conf_val(self, json, config_key, default_val = None):\n","        if config_key in json:\n","            return json[config_key]\n","        else:\n","            return default_val"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"bab08844-21d0-4e24-a0eb-ae15aa15e06a","statement_id":17,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-15T11:37:53.2307741Z","session_start_time":null,"execution_start_time":"2024-04-15T11:37:53.6142249Z","execution_finish_time":"2024-04-15T11:37:53.9746062Z","parent_msg_id":"fb26c6ad-c739-4d3b-a164-d70ef7a5f512"},"text/plain":"StatementMeta(, bab08844-21d0-4e24-a0eb-ae15aa15e06a, 17, Finished, Available)"},"metadata":{}}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e052e1da-4963-41ff-ac98-e56b3eb1ca5d"},{"cell_type":"code","source":["class ConfigMetadataLoader(ConfigBase):\n","    def __init__(self, config_path, gcp_credential):\n","        super().__init__(config_path, gcp_credential)\n","    \n","    def create_autodetect_view(self):\n","        sql = \"\"\"\n","        CREATE OR REPLACE TEMPORARY VIEW bq_table_metadata_autodetect\n","        AS\n","        WITH pkeys AS (    \n","            SELECT\n","                c.table_catalog, c.table_schema, c.table_name, \n","                k.column_name AS pk_col\n","            FROM bq_information_schema_table_constraints c\n","            JOIN bq_information_schema_key_column_usage k ON\n","                k.table_catalog = c.table_catalog AND\n","                k.table_schema = c.table_schema AND\n","                k.table_name = c.table_name AND\n","                k.constraint_name = c.constraint_name\n","            JOIN bq_information_schema_columns n ON\n","                n.table_catalog = k.table_catalog AND\n","                n.table_schema = k.table_schema AND\n","                n.table_name = k.table_name AND\n","                n.column_name = k.column_name\n","            JOIN bq_data_type_map m ON n.data_type = m.data_type\n","            WHERE c.constraint_type = 'PRIMARY KEY'\n","            AND m.is_watermark = 'YES'\n","        ),\n","        pkeys_cnt AS (\n","            SELECT \n","                table_catalog, table_schema, table_name, \n","                COUNT(*) as pk_cnt\n","            FROM pkeys\n","            GROUP BY table_catalog, table_schema, table_name\n","        ),\n","        watermark_cols AS (\n","            SELECT \n","                k.*\n","            FROM pkeys k\n","            JOIN pkeys_cnt c ON \n","                k.table_catalog = c.table_catalog AND\n","                k.table_schema = c.table_schema AND\n","                k.table_name = c.table_name\n","            WHERE c.pk_cnt = 1\n","        ),\n","        partitions AS (\n","            SELECT\n","                table_catalog, table_schema, table_name, \n","                count(*) as partition_count,\n","                avg(len(partition_id)) AS partition_id_len,\n","                sum(case when partition_id is NULL then 1 else 0 end) as null_partition_count\n","            FROM bq_information_schema_partitions\n","            GROUP BY table_catalog, table_schema, table_name\n","        ), \n","        partition_columns AS\n","        (\n","            SELECT\n","                table_catalog, table_schema, table_name,\n","                column_name, c.data_type,\n","                m.partition_type AS partitioning_type\n","            FROM bq_information_schema_columns c\n","            JOIN bq_data_type_map m ON c.data_type=m.data_type\n","            WHERE is_partitioning_column = 'YES'\n","        ),\n","        partition_cfg AS\n","        (\n","            SELECT\n","                p.*,\n","                CASE WHEN p.partition_count = 1 AND p.null_partition_count = 1 THEN FALSE ELSE TRUE END AS is_partitioned,\n","                c.column_name AS partition_col,\n","                c.data_type AS partition_data_type,\n","                c.partitioning_type,\n","                CASE WHEN (c.partitioning_type = 'TIME')\n","                    THEN \n","                        CASE WHEN (partition_id_len = 4) THEN 'YEAR'\n","                            WHEN (partition_id_len = 6) THEN 'MONTH'\n","                            WHEN (partition_id_len = 8) THEN 'DAY'\n","                            WHEN (partition_id_len = 10) THEN 'HOUR'\n","                            ELSE NULL END\n","                    ELSE NULL END AS partitioning_strategy\n","            FROM partitions p\n","            LEFT JOIN partition_columns c ON \n","                p.table_catalog = c.table_catalog AND\n","                p.table_schema = c.table_schema AND\n","                p.table_name = c.table_name\n","        )\n","\n","        SELECT \n","            t.table_catalog, t.table_schema, t.table_name, t.is_insertable_into,\n","            p.is_partitioned, p.partition_col, p.partition_data_type, p.partitioning_type, p.partitioning_strategy,\n","            w.pk_col\n","        FROM bq_information_schema_tables t\n","        LEFT JOIN watermark_cols w ON \n","            t.table_catalog = w.table_catalog AND\n","            t.table_schema = w.table_schema AND\n","            t.table_name = w.table_name\n","        LEFT JOIN partition_cfg p ON\n","            t.table_catalog = p.table_catalog AND\n","            t.table_schema = p.table_schema AND\n","            t.table_name = p.table_name\n","        \"\"\"\n","\n","        spark.sql(sql)\n","\n","    def sync_bq_information_schema_tables(self):\n","        bq_table = self.UserConfig.get_bq_table_fullname(SyncConstants.INFORMATION_SCHEMA_TABLES)\n","        tbl_nm = self.UserConfig.flatten_3part_tablename(SyncConstants.INFORMATION_SCHEMA_TABLES.replace(\".\", \"_\"))\n","\n","        bql = f\"\"\"\n","        SELECT *\n","        FROM {bq_table}\n","        WHERE table_type='BASE TABLE'\n","        AND table_name NOT LIKE '_bqc_%'\n","        \"\"\"\n","\n","        df = self.read_bq_to_dataframe(bql)\n","\n","        if not self.UserConfig.LoadAllTables:\n","            filter_list = self.UserConfig.get_table_name_list()\n","            df = df.filter(col(\"table_name\").isin(filter_list))    \n","\n","        self.write_lakehouse_table(df, self.UserConfig.MetadataLakehouse, tbl_nm)\n","\n","    def sync_bq_information_schema_table_dependent(self, dependent_tbl):\n","        bq_table = self.UserConfig.get_bq_table_fullname(SyncConstants.INFORMATION_SCHEMA_TABLES)\n","        bq_dependent_tbl = self.UserConfig.get_bq_table_fullname(dependent_tbl)\n","        tbl_nm = self.UserConfig.flatten_3part_tablename(dependent_tbl.replace(\".\", \"_\"))\n","\n","        bql = f\"\"\"\n","        SELECT c.*\n","        FROM {bq_dependent_tbl} c\n","        JOIN {bq_table} t ON \n","        t.table_catalog=c.table_catalog AND\n","        t.table_schema=c.table_schema AND\n","        t.table_name=c.table_name\n","        WHERE t.table_type='BASE TABLE'\n","        AND t.table_name NOT LIKE '_bqc_%'\n","        \"\"\"\n","\n","        df = self.read_bq_to_dataframe(bql)\n","\n","        if not self.UserConfig.LoadAllTables:\n","            filter_list = self.UserConfig.get_table_name_list()\n","            df = df.filter(col(\"table_name\").isin(filter_list)) \n","\n","        self.write_lakehouse_table(df, self.UserConfig.MetadataLakehouse, tbl_nm)\n","\n","    def sync_bq_metadata(self):\n","        self.sync_bq_information_schema_tables()\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_PARTITIONS)\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_COLUMNS)\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_TABLE_CONSTRAINTS)\n","        self.sync_bq_information_schema_table_dependent(SyncConstants.INFORMATION_SCHEMA_KEY_COLUMN_USAGE)\n","\n","    def create_proxy_views(self):\n","        super().create_proxy_views()\n","        self.create_autodetect_view()\n","\n","    def auto_detect_table_profiles(self):\n","        self.create_proxy_views()\n","        \n","        sql = f\"\"\"\n","        WITH default_config AS (\n","            SELECT autodetect, target_lakehouse FROM user_config_json\n","        ),\n","        pk AS (\n","            SELECT\n","            a.table_catalog, a.table_schema, a.table_name, array_agg(COALESCE(a.pk_col, u.column)) as pk\n","            FROM bq_table_metadata_autodetect a\n","            LEFT JOIN user_config_table_keys u ON\n","                a.table_catalog = u.project_id AND\n","                a.table_schema = u.dataset AND\n","                a.table_name = u.table_name\n","            GROUP BY a.table_catalog, a.table_schema, a.table_name\n","        ),\n","        source AS (\n","            SELECT\n","                a.table_catalog as project_id,\n","                a.table_schema as dataset,\n","                a.table_name as table_name,\n","                COALESCE(u.enabled, TRUE) AS enabled,\n","                COALESCE(u.lakehouse, d.target_lakehouse) AS lakehouse,\n","                COALESCE(u.lakehouse_target_table, a.table_name) AS lakehouse_table_name,\n","                COALESCE(u.source_query, '') AS source_query,\n","                COALESCE(u.load_priority, '100') AS priority,\n","                CASE WHEN (COALESCE(u.watermark_column, a.pk_col) IS NOT NULL AND\n","                        COALESCE(u.watermark_column, a.pk_col) <> '') THEN 'WATERMARK' \n","                    WHEN (COALESCE(u.partition_enabled, a.is_partitioned) = TRUE) \n","                        AND COALESCE(u.partition_column, a.partition_col, '') NOT IN \n","                            ('_PARTITIONTIME', '_PARTITIONDATE')\n","                    THEN 'PARTITION' ELSE 'FULL' END AS load_strategy,\n","                CASE WHEN (COALESCE(u.watermark_column, a.pk_col) IS NOT NULL AND\n","                        COALESCE(u.watermark_column, a.pk_col) <> '') THEN 'APPEND' ELSE\n","                    'OVERWRITE' END AS load_type,\n","                COALESCE(u.interval, 'AUTO') AS interval,\n","                p.pk AS primary_keys,\n","                COALESCE(u.partition_enabled, a.is_partitioned) AS is_partitioned,\n","                COALESCE(u.partition_column, a.partition_col, '') AS partition_column,\n","                COALESCE(u.partition_type, a.partitioning_type, '') AS partition_type,\n","                COALESCE(u.partition_grain, a.partitioning_strategy, '') AS partition_grain,\n","                COALESCE(u.watermark_column, a.pk_col, '') AS watermark_column, \n","                d.autodetect,\n","                CASE WHEN u.table_name IS NULL THEN FALSE ELSE TRUE END AS config_override,\n","                'INIT' AS sync_state,\n","                CURRENT_TIMESTAMP() as created_dt,\n","                NULL as last_updated_dt\n","            FROM bq_table_metadata_autodetect a\n","            JOIN pk p ON\n","                a.table_catalog = p.table_catalog AND\n","                a.table_schema = p.table_schema AND\n","                a.table_name = p.table_name\n","            LEFT JOIN user_config_tables u ON \n","                a.table_catalog = u.project_id AND\n","                a.table_schema = u.dataset AND\n","                a.table_name = u.table_name\n","            CROSS JOIN default_config d\n","        )\n","\n","        MERGE INTO {SyncConstants.SQL_TBL_SYNC_CONFIG} t\n","        USING source s\n","        ON t.project_id = s.project_id AND\n","            t.dataset = s.dataset AND\n","            t.table_name = s.table_name\n","        WHEN MATCHED AND t.sync_state <> 'INIT' THEN\n","            UPDATE SET\n","                t.enabled = s.enabled,\n","                t.interval = s.interval,\n","                t.priority = s.priority,\n","                t.last_updated_dt = CURRENT_TIMESTAMP()\n","        WHEN MATCHED AND t.sync_state = 'INIT' THEN\n","            UPDATE SET *\n","        WHEN NOT MATCHED THEN\n","            INSERT *\n","        \"\"\"\n","\n","        spark.sql(sql)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2024-04-15T13:40:03.4191873Z","session_start_time":"2024-04-15T13:40:03.6376313Z","execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"e4a78244-0eb0-40d1-aa47-ff0968647c9a"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a30a6ece-4ffa-47aa-bb2f-08d5dac28f83"},{"cell_type":"code","source":["class BQScheduleLoader(ConfigBase):\n","    def __init__(self, config_path, gcp_credential):\n","        super().__init__(config_path, gcp_credential)\n","        super().create_proxy_views()\n","\n","    def save_telemetry(self, telemetry: SyncSchedule):\n","        df = spark.table(SyncConstants.SQL_TBL_SYNC_SCHEDULE) \\\n","            .filter(\"schedule_id=='{0}'\".format(telemetry.ScheduleId)) \\\n","            .withColumn(\"started\", lit(telemetry.StartTime)) \\\n","            .withColumn(\"src_row_count\", lit(telemetry.SourceRows)) \\\n","            .withColumn(\"dest_row_count\", lit(telemetry.DestRows)) \\\n","            .withColumn(\"dest_inserted_row_count\", lit(telemetry.InsertedRows)) \\\n","            .withColumn(\"dest_updated_row_count\", lit(telemetry.UpdatedRows)) \\\n","            .withColumn(\"delta_version\", lit(telemetry.DeltaVersion)) \\\n","            .withColumn(\"spark_application_id\", lit(telemetry.SparkAppId)) \\\n","            .withColumn(\"max_watermark\", lit(telemetry.MaxWatermark)) \\\n","            .withColumn(\"summary_load\", lit(telemetry.SummaryLoadType)) \\\n","            .withColumn(\"status\", lit(\"COMPLETE\")) \\\n","            .withColumn(\"completed\", lit(telemetry.EndTime))\n","\n","        schedule_df = DeltaTable.forName(spark, SyncConstants.SQL_TBL_SYNC_SCHEDULE).alias(\"t\")\n","\n","        schedule_df.merge( \\\n","            df.alias('s'), 't.schedule_id = s.schedule_id') \\\n","            .whenMatchedUpdate(set = \\\n","            { \\\n","            \"status\": \"s.status\", \\\n","            \"started\": \"s.started\", \\\n","            \"completed\": \"s.completed\", \\\n","            \"src_row_count\": \"s.src_row_count\", \\\n","            \"dest_row_count\": \"s.dest_row_count\", \\\n","            \"dest_inserted_row_count\": \"s.dest_inserted_row_count\", \\\n","            \"dest_updated_row_count\": \"s.dest_updated_row_count\", \\\n","            \"delta_version\": \"s.delta_version\", \\\n","            \"spark_application_id\": \"s.spark_application_id\", \\\n","            \"max_watermark\": \"s.max_watermark\", \\\n","            \"summary_load\": \"s.summary_load\" \\\n","            } \\\n","        ).execute()\n","\n","        if telemetry.LoadStrategy == SyncConstants.PARTITION and not telemetry.InitialLoad:\n","            self.save_partition_telemtry(telemetry)\n","\n","    def save_partition_telemtry(self, telemetry: SyncSchedule):\n","        sql = f\"\"\"\n","        SELECT \n","            s.schedule_id, s.project_id, s.dataset, s.table_name, \n","            p.partition_id, p.total_rows AS bq_total_rows, \n","            p.last_modified_time AS bq_last_modified, p.storage_tier as bq_storage_tier,\n","            s.started, s.completed\n","        FROM {SyncConstants.SQL_TBL_SYNC_SCHEDULE_PARTITION} s\n","        JOIN bq_information_schema_partitions p ON\n","            s.project_id=p.table_catalog AND\n","            s.dataset=p.table_schema AND \n","            s.table_name=p.table_name\n","        WHERE s.schedule_id='{telemetry.ScheduleId}'\n","        \"\"\"\n","        df = spark.sql(sql)\n","\n","        df.write.mode(SyncConstants.APPEND).saveAsTable(SyncConstants.SQL_TBL_SYNC_SCHEDULE_PARTITION)\n","\n","    def get_table_delta_version(self, tbl):\n","        sql = f\"DESCRIBE HISTORY {tbl}\"\n","        df = spark.sql(sql) \\\n","            .select(max(col(\"version\")).alias(\"delta_version\"))\n","\n","        for row in df.collect():\n","            return row[\"delta_version\"]\n","\n","    def update_sync_config_state(self, project_id, dataset, table_name):\n","        sql = f\"\"\"\n","        UPDATE {SyncConstants.SQL_TBL_SYNC_CONFIG} \n","        SET sync_state='COMMIT' \n","        WHERE\n","            project_id='{project_id}' AND\n","            dataset='{dataset}' AND\n","            table_name='{table_name}'\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def get_table_partition_metadata(self, schedule):\n","        sql = f\"\"\"\n","        WITH last_load AS (\n","            SELECT project_id, dataset, table_name, MAX(started) AS last_load_update\n","            FROM {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","            WHERE status='COMPLETE'\n","            GROUP BY project_id, dataset, table_name\n","        )\n","\n","        SELECT\n","            sp.table_name, sp.partition_id, sp.total_rows, sp.last_modified_time, sp.storage_tier,\n","            s.last_load_update AS last_part_load\n","        FROM bq_information_schema_partitions sp\n","        LEFT JOIN last_load s ON \n","            sp.table_catalog = s.project_id AND \n","            sp.table_schema = s.dataset AND\n","            sp.table_name = s.table_name\n","        WHERE sp.table_catalog = '{schedule.ProjectId}'\n","        AND sp.table_schema = '{schedule.Dataset}'\n","        AND sp.table_name = '{schedule.TableName}'\n","        AND sp.last_modified_time >= s.last_load_update\n","        \"\"\"\n","        return spark.sql(sql)\n","\n","    def get_schedule(self):\n","        sql = f\"\"\"\n","        WITH last_completed_schedule AS (\n","            SELECT schedule_id, project_id, dataset, table_name, max_watermark, started AS last_schedule_dt\n","            FROM (\n","                SELECT schedule_id, project_id, dataset, table_name, started, max_watermark,\n","                ROW_NUMBER() OVER(PARTITION BY project_id, dataset, table_name ORDER BY scheduled DESC) AS row_num\n","                FROM {SyncConstants.SQL_TBL_SYNC_SCHEDULE}\n","                WHERE status='COMPLETE'\n","            )\n","            WHERE row_num = 1\n","        )\n","\n","        SELECT c.*, \n","            s.schedule_id,\n","            h.max_watermark,\n","            h.last_schedule_dt,\n","            CASE WHEN (h.schedule_id IS NULL) THEN TRUE ELSE FALSE END AS initial_load\n","        FROM {SyncConstants.SQL_TBL_SYNC_CONFIG} c\n","        JOIN {SyncConstants.SQL_TBL_SYNC_SCHEDULE} s ON \n","            c.project_id = s.project_id AND\n","            c.dataset = s.dataset AND\n","            c.table_name = s.table_name\n","        LEFT JOIN last_completed_schedule h ON\n","            c.project_id = h.project_id AND\n","            c.dataset = h.dataset AND\n","            c.table_name = h.table_name\n","        WHERE s.status = 'SCHEDULED'\n","            AND s.priority = 100\n","            AND c.enabled = TRUE\n","            AND c.project_id = '{self.UserConfig.ProjectID}' \n","            AND c.dataset = '{self.UserConfig.Dataset}'\n","        \"\"\"\n","        df = spark.sql(sql)\n","        df.cache()\n","\n","        return df\n","\n","    def build_bq_partition_query(self, schedule, predicate = []):\n","        query = f\"SELECT * FROM {schedule.BQTableName}\"\n","\n","        if predicate:\n","            query += \" WHERE \"\n","            query += \" OR \".join(predicate)\n","        \n","        print(query)\n","        return query\n","\n","    def get_max_watermark(self, lakehouse_tbl, watermark_col):\n","        df = spark.table(lakehouse_tbl) \\\n","            .select(max(col(watermark_col)).alias(\"watermark\"))\n","\n","        for row in df.collect():\n","            return row[\"watermark\"]\n","\n","    def sync_bq_table(self, row):\n","        schedule = SyncSchedule(row)\n","\n","        print(\"{0} {1}...\".format(schedule.SummaryLoadType, schedule.TableName))\n","\n","        if schedule.LoadStrategy == SyncConstants.PARTITION and not schedule.InitialLoad:\n","            print(\"Load by partition...\")\n","            df_partitions = self.get_table_partition_metadata(schedule)\n","\n","            predicate = []\n","            partitions_to_write = []\n","\n","            for p in df_partitions.collect():\n","                part_id = p[\"partition_id\"]\n","\n","                if p[\"last_modified_time\"] > p[\"last_part_load\"]:\n","                    print(\"Partition {0} has changes...\".format(part_id))\n","\n","                    partitions_to_write.append(part_id)\n","                    \n","                    match schedule.PartitionGrain:\n","                        case \"DAY\":\n","                            part_format = \"%Y%m%d\"\n","                        case \"MONTH\":\n","                            part_format = \"%Y%m\"\n","                        case \"YEAR\":\n","                            part_format = \"%Y\"\n","                        case \"HOUR\":\n","                            part_format = \"%Y%m%d%H\"\n","                        case _:\n","                            raise Exception(\"Unsupported Partition Grain in Table Config\")\n","                                \n","                    predicate.append(f\"date_trunc({schedule.PartitionColumn}, {schedule.PartitionGrain}) = PARSE_DATETIME('{part_format}', '{part_id}')\")\n","                else:\n","                    print(\"Partition {0} is up to date...\".format(part_id))\n","\n","            src = self.build_bq_partition_query(schedule, predicate)\n","        else:\n","            src = schedule.BQTableName     \n","\n","            if schedule.SourceQuery != \"\":\n","                src = schedule.SourceQuery\n","\n","        df_bq = super().read_bq_to_dataframe(src)\n","\n","        predicate = None\n","\n","        if schedule.LoadStrategy == SyncConstants.WATERMARK and not schedule.InitialLoad:\n","            pk = schedule.PrimaryKey\n","            max_watermark = schedule.MaxWatermark\n","\n","            if max_watermark.isdigit():\n","                predicate = f\"{pk} > {max_watermark}\"\n","            else:\n","                predicate = f\"{pk} > '{max_watermark}'\"\n","            \n","        if predicate is not None:\n","            df_bq = df_bq.where(predicate)\n","\n","        df_bq.cache()\n","\n","        partition = None\n","\n","        if schedule.IsPartitioned and not schedule.IsTimeIngestionPartitioned:\n","            print('Resolving Fabric partitioning...')\n","            if schedule.PartitionType == SyncConstants.TIME:\n","                partition_col = schedule.PartitionColumn\n","                part_format = \"\"\n","                part_col_name = f\"__bq_part_{partition_col}\"\n","                use_proxy_col = False\n","\n","                match schedule.PartitionGrain:\n","                    case \"DAY\":\n","                        part_format = \"yyyyMMdd\"\n","\n","                        if dict(df_bq.dtypes)[partition_col] == \"date\":\n","                            partition = partition_col\n","                        else:\n","                            partition = f\"{part_col_name}_DAY\"\n","                            use_proxy_col = True\n","                    case \"MONTH\":\n","                        part_format = \"yyyyMM\"\n","                        partition = f\"{part_col_name}_MONTH\"\n","                        use_proxy_col = True\n","                    case \"YEAR\":\n","                        part_format = \"yyyy\"\n","                        partition = f\"{part_col_name}_YEAR\"\n","                        use_proxy_col = True\n","                    case \"HOUR\":\n","                        part_format = \"yyyyMMddHH\"\n","                        partition = f\"{part_col_name}_HOUR\"\n","                        use_proxy_col = True\n","                    case _:\n","                        print('Unsupported partition grain...')\n","                \n","                print(\"{0} partitioning - partitioned by {1} (Requires Proxy Column: {2})\".format( \\\n","                    row[\"partition_grain\"], \\\n","                    partition, \\\n","                    use_proxy_col))\n","                \n","                if use_proxy_col:\n","                    df_bq = df_bq.withColumn(partition, date_format(col(partition_col), part_format))\n","\n","        if schedule.LoadStrategy == SyncConstants.PARTITION and not schedule.InitialLoad:\n","            if partitions_to_write:\n","                for part in partitions_to_write:\n","                    print(f\"Writing {schedule.TableName}${part} partition...\")\n","                    part_filter = f\"{partition} = '{part}'\"\n","                    \n","                    pdf = df_bq.where(part_filter)\n","                    part_cnt = pdf.count()\n","                    \n","                    schedule.UpdateRowCounts(0, part_cnt, 0, 0)\n","\n","                    pdf.write \\\n","                        .mode(SyncConstants.OVERWRITE) \\\n","                        .option(\"replaceWhere\", part_filter) \\\n","                        .saveAsTable(schedule.LakehouseTableName)\n","        else:\n","            if partition is None:\n","                df_bq.write \\\n","                    .mode(schedule.Mode) \\\n","                    .saveAsTable(schedule.LakehouseTableName)\n","            else:\n","                df_bq.write \\\n","                    .partitionBy(partition) \\\n","                    .mode(schedule.Mode) \\\n","                    .saveAsTable(schedule.LakehouseTableName)\n","\n","        if schedule.LoadStrategy == SyncConstants.WATERMARK:\n","            schedule.MaxWatermark = self.get_max_watermark(schedule.LakehouseTableName, schedule.PrimaryKey)\n","\n","        src_cnt = df_bq.count()\n","\n","        if schedule.LoadStrategy != SyncConstants.PARTITION or schedule.InitialLoad:\n","            dest_cnt = spark.table(schedule.LakehouseTableName).count()\n","        else:\n","            dest_cnt = 0\n","\n","        schedule.UpdateRowCounts(src_cnt, dest_cnt, 0, 0)    \n","        schedule.SparkAppId = spark.sparkContext.applicationId\n","        schedule.DeltaVersion = self.get_table_delta_version(schedule.LakehouseTableName)\n","        schedule.EndTime = datetime.now(timezone.utc)\n","\n","        df_bq.unpersist()\n","\n","        self.save_telemetry(schedule)\n","\n","        if schedule.InitialLoad:\n","            self.update_sync_config_state(schedule.ProjectId, schedule.Dataset, schedule.TableName)\n","\n","    def run_schedule(self):\n","        df_schedule = self.get_schedule()\n","\n","        for row in df_schedule.collect():\n","            self.sync_bq_table(row)  "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"188d4f89-f45c-455e-8263-facaf270c5df","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-12T13:48:56.460091Z","session_start_time":null,"execution_start_time":"2024-04-12T13:49:00.8709995Z","execution_finish_time":"2024-04-12T13:49:01.251321Z","parent_msg_id":"0ccd252d-59b8-4469-8176-4ebfb4e282dc"},"text/plain":"StatementMeta(, 188d4f89-f45c-455e-8263-facaf270c5df, 13, Finished, Available)"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"86235e45-0c89-45e9-b017-9466ec0dc54a"},{"cell_type":"code","source":["class SyncSetup(ConfigBase):\n","    def __init__(self, config_path, gcp_credential):\n","        super().__init__(config_path, gcp_credential)\n","\n","    def get_fabric_lakehouse(self, nm):\n","        lakehouse = None\n","\n","        try:\n","            lakehouse = mssparkutils.lakehouse.get(nm)\n","        except Exception:\n","            print(\"Lakehouse not found\")\n","\n","        return lakehouse\n","\n","    def create_fabric_lakehouse(self, nm):\n","        lakehouse = get_fabric_lakehouse(nm)\n","\n","        if (lakehouse is None):\n","            mssparkutils.lakehouse.create(nm)\n","\n","    def setup(self):\n","        self.create_fabric_lakehouse(self.UserConfig.MetadataLakehouse)\n","        self.create_fabric_lakehouse(self.UserConfig.TargetLakehouse)\n","        self.create_all_tables()\n","\n","    def drop_table(self, tbl):\n","        sql = f\"DROP TABLE IF EXISTS {tbl_nm}\"\n","        spark.sql(sql)\n","\n","    def get_tbl_name(self, tbl):\n","        return self.UserConfig.get_lakehouse_tablename(self.UserConfig.MetadataLakehouse, tbl)\n","\n","    def create_data_type_map_tbl(self):\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_DATA_TYPE_MAP)\n","        self.drop_table(tbl_name)\n","\n","        sql = f\"\"\"CREATE TABLE IF NOT EXISTS {tbl_nm} (data_type STRING, partition_type STRING, is_watermark STRING)\"\"\"\n","        spark.sql(sql)\n","\n","        df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/data/bq_data_types.csv\")\n","        df.write.mode(\"OVERWRITE\").saveAsTable(tbl_nm)\n","\n","    def create_sync_config_tbl(self):\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_SYNC_CONFIG)\n","        self.drop_table(tbl_name)\n","\n","        sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {tbl_nm}\n","        (\n","            project_id STRING,\n","            dataset STRING,\n","            table_name STRING,\n","            enabled BOOLEAN,\n","            lakehouse STRING,\n","            lakehouse_table_name STRING,\n","            source_query STRING,\n","            priority INTEGER,\n","            load_strategy STRING,\n","            load_type STRING,\n","            interval STRING,\n","            primary_keys ARRAY<STRING>,\n","            is_partitioned BOOLEAN,\n","            partition_column STRING,\n","            partition_type STRING,\n","            partition_grain STRING,\n","            watermark_column STRING,\n","            autodetect BOOLEAN,\n","            config_override BOOLEAN,\n","            sync_state STRING,\n","            created_dt TIMESTAMP,\n","            last_updated_dt TIMESTAMP\n","        )\n","        \"\"\"\n","        spark.sql(sql)\n","    \n","    def create_sync_schedule_tbl(self):\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_SYNC_SCHEDULE)\n","        self.drop_table(tbl_name)\n","\n","        sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {tbl_nm} (\n","            schedule_id STRING,\n","            project_id STRING,\n","            dataset STRING,\n","            table_name STRING,\n","            scheduled TIMESTAMP,\n","            status STRING,\n","            started TIMESTAMP,\n","            completed TIMESTAMP,\n","            src_row_count BIGINT,\n","            dest_row_count BIGINT,\n","            dest_inserted_row_count BIGINT,\n","            dest_updated_row_count BIGINT,\n","            delta_version BIGINT,\n","            spark_application_id STRING,\n","            max_watermark STRING,\n","            summary_load STRING,\n","            priority INTEGER\n","        )\n","        PARTITIONED BY (priority)\n","        \"\"\"\n","        spark.sql(sql)\n","    \n","    def create_sync_schedule_partition_tbl(self):\n","        tbl_nm = self.get_tbl_name(SyncConstants.SQL_TBL_SYNC_SCHEDULE_PARTITION)\n","        self.drop_table(tbl_name)\n","\n","        sql = f\"\"\"\n","        CREATE TABLE IF NOT EXISTS {tbl_nm} (\n","            schedule_id STRING,\n","            project_id STRING,\n","            dataset STRING,\n","            table_name STRING,\n","            partition_id STRING,\n","            bq_total_rows BIGINT,\n","            bq_last_modified TIMESTAMP,\n","            bq_storage_tier STRING,\n","            started TIMESTAMP,\n","            completed TIMESTAMP\n","        )\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def create_all_tables(self):\n","        self.create_data_type_map_tbl()\n","        self.create_sync_config_tbl()\n","        self.create_sync_schedule_tbl()\n","        self.create_sync_schedule_partition_tbl()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"188d4f89-f45c-455e-8263-facaf270c5df","statement_id":14,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-12T13:48:56.8332918Z","session_start_time":null,"execution_start_time":"2024-04-12T13:49:01.7377599Z","execution_finish_time":"2024-04-12T13:49:02.0871756Z","parent_msg_id":"710506e8-b37e-4be7-9464-1cbe5d98cc21"},"text/plain":"StatementMeta(, 188d4f89-f45c-455e-8263-facaf270c5df, 14, Finished, Available)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e37812cf-b115-44e2-b629-64ae1fc6eb5c"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"9c7403b4-606d-412c-9224-64ccdbff9cc3","default_lakehouse_name":"BQ_Metadata","default_lakehouse_workspace_id":"0fb2d96e-53e7-4c49-a594-beb0891ac121"}}},"nbformat":4,"nbformat_minor":5}