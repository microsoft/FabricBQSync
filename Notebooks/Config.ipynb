{"cells":[{"cell_type":"code","source":["class SyncConstants:\n","    '''\n","    Class representing various string constants used through-out\n","    '''\n","    OVERWRITE = \"OVERWRITE\"\n","    APPEND = \"APPEND\"\n","    FULL = \"FULL\"\n","    PARTITION = \"PARTITION\"\n","    WATERMARK = \"WATERMARK\"\n","    TIME_INGESTION = \"TIME_INGESTION\"\n","    MERGE = \"MERGE\"\n","    AUTO = \"AUTO\"\n","    TIME = \"TIME\"    \n","    YEAR = \"YEAR\"\n","    MONTH = \"MONTH\"\n","    DAY = \"DAY\"\n","    HOUR = \"HOUR\"\n","\n","    INITIAL_FULL_OVERWRITE = \"INITIAL_FULL_OVERWRITE\"\n","    INFORMATION_SCHEMA_TABLES = \"INFORMATION_SCHEMA.TABLES\"\n","    INFORMATION_SCHEMA_PARTITIONS = \"INFORMATION_SCHEMA.PARTITIONS\"\n","    INFORMATION_SCHEMA_COLUMNS = \"INFORMATION_SCHEMA.COLUMNS\"\n","    INFORMATION_SCHEMA_TABLE_CONSTRAINTS = \"INFORMATION_SCHEMA.TABLE_CONSTRAINTS\"\n","    INFORMATION_SCHEMA_KEY_COLUMN_USAGE = \"INFORMATION_SCHEMA.KEY_COLUMN_USAGE\"\n","\n","    SQL_TBL_SYNC_SCHEDULE = \"bq_sync_schedule\"\n","    SQL_TBL_SYNC_CONFIG = \"bq_sync_configuration\"\n","    SQL_TBL_DATA_TYPE_MAP = \"bq_data_type_map\"\n","    SQL_TBL_SYNC_SCHEDULE_TELEMETRY = \"bq_sync_schedule_telemetry\"\n","\n","    def get_load_strategies () -> List[str]:\n","        return [SyncConstants.FULL, SyncConstants.PARTITION, SyncConstants.WATERMARK, SyncConstants.TIME_INGESTION]\n","\n","    def get_load_types() -> List[str]:\n","        return [SyncConstants.OVERWRITE, SyncConstants.APPEND, SyncConstants.MERGE]\n","\n","    def get_partition_types() -> List[str]:\n","        return [SyncConstants.TIME, SyncConstants.TIME_INGESTION]\n","\n","    def get_partition_grains() -> List[str]:\n","        return [SyncConstants.YEAR, SyncConstants.MONTH, SyncConstants.DAY, SyncConstants.HOUR]\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"62248c3f-83de-419e-847f-409e1ddda63b","statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:17:28.271484Z","session_start_time":null,"execution_start_time":"2024-04-18T13:17:35.9396462Z","execution_finish_time":"2024-04-18T13:17:36.3257594Z","parent_msg_id":"0bc4dc0f-c668-4278-bb1c-f42c7f7823a0"},"text/plain":"StatementMeta(, 62248c3f-83de-419e-847f-409e1ddda63b, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9f22c096-d250-49f3-8dec-5a370541be51"},{"cell_type":"code","source":["class ScheduleDAG:\n","    \"\"\"\n","    Schedule DAG for Run Multiple Notebook implementation\n","    \"\"\"\n","    def __init__(\n","            self, \n","            timeout:int=7200, \n","            concurrency:int=5):\n","        \"\"\"\n","        Schedule DAG configuration. Maps DAG dependencies and sets paralellism concurrency for load\n","        \"\"\"\n","        self.activities:list[DAGActivity] = []\n","        self.timeoutInSeconds:int = timeout\n","        self.concurrency:int = concurrency\n","\n","class ScheduleDAGEncoder(JSONEncoder):\n","        \"\"\"\n","        JSON Encoder for Schedule DAG\n","        \"\"\"\n","        def default(self, o):\n","            return o.__dict__\n","            \n","class DAGActivity:\n","    \"\"\"\n","    DAG Activity for Run Multiple Notebook implementation\n","    \"\"\"\n","    def __init__(\n","            self, \n","            name:str, \n","            path:str, \n","            timeout:int = 3600, \n","            retry:int =  None, \n","            retryInterval:int = None, \n","            dependencies:list[str] = [], \n","            **keyword_args):\n","        \"\"\"\n","        DAG activity configuration. Keyword args are used to pass notebook params\n","        \"\"\"\n","        self.name = name\n","        self.path = path\n","        self.timeoutPerCellInSeconds = timeout\n","        self.retry = retry\n","        self.retryIntervalInSeconds = retryInterval\n","        self.dependencies = dependencies\n","        self.args = keyword_args"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"62248c3f-83de-419e-847f-409e1ddda63b","statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:17:28.4027284Z","session_start_time":null,"execution_start_time":"2024-04-18T13:17:36.7937917Z","execution_finish_time":"2024-04-18T13:17:37.2014824Z","parent_msg_id":"c9919198-12df-4279-b47b-804f373f3890"},"text/plain":"StatementMeta(, 62248c3f-83de-419e-847f-409e1ddda63b, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2cdba948-156c-434a-988a-2da7f4424a6c"},{"cell_type":"code","source":["class SyncSchedule:\n","    \"\"\"\n","    Scheduled configuration object that also is used to track and store telemetry from load process\n","    \"\"\"\n","    EndTime:datetime = None\n","    SourceRows:int = 0\n","    DestRows:int = 0\n","    InsertedRows:int = 0\n","    UpdatedRows:int = 0\n","    DeltaVersion:str = None\n","    SparkAppId:str = None\n","    MaxWatermark:str = None\n","    Status:str = None\n","    FabricPartitionColumn:str = None\n","\n","    def __init__(\n","                self, \n","                row: Row):\n","        \"\"\"\n","        Scheduled load Configuration load from Data Row\n","        \"\"\"\n","        self.Row = row\n","        self.StartTime = datetime.now(timezone.utc)\n","        self.GroupScheduleId = row[\"group_schedule_id\"]\n","        self.ScheduleId = row[\"schedule_id\"]\n","        self.LoadStrategy = row[\"load_strategy\"]\n","        self.LoadType = row[\"load_type\"]\n","        self.InitialLoad = row[\"initial_load\"]\n","        self.ProjectId = row[\"project_id\"]\n","        self.Dataset = row[\"dataset\"]\n","        self.TableName = row[\"table_name\"]\n","        self.SourceQuery = row[\"source_query\"]\n","        self.MaxWatermark = row[\"max_watermark\"]\n","        self.IsPartitioned = row[\"is_partitioned\"]\n","        self.PartitionColumn = row[\"partition_column\"]\n","        self.PartitionType = row[\"partition_type\"]\n","        self.PartitionGrain = row[\"partition_grain\"]\n","        self.WatermarkColumn = row[\"watermark_column\"]\n","        self.LastScheduleLoadDate = row[\"last_schedule_dt\"]\n","        self.Lakehouse = row[\"lakehouse\"]\n","        self.DestinationTableName = row[\"lakehouse_table_name\"]\n","        self.PartitionId = row[\"partition_id\"]\n","    \n","    @property\n","    def SummaryLoadType(self) -> str:\n","        \"\"\"\n","        Summarized the load strategy based on context\n","        \"\"\"\n","        if self.InitialLoad:\n","            return SyncConstants.INITIAL_FULL_OVERWRITE\n","        else:\n","            return \"{0}_{1}\".format(self.LoadStrategy, self.LoadType)\n","    \n","    @property\n","    def Mode(self) -> str:\n","        \"\"\"\n","        Returns the write mode based on context\n","        \"\"\"\n","        if self.InitialLoad:\n","            return SyncConstants.OVERWRITE\n","        else:\n","            return self.LoadType\n","    \n","    @property\n","    def Keys(self) -> list[str]:\n","        \"\"\"\n","        Returns list of keys\n","        \"\"\"        \n","        if self.Row[\"primary_keys\"]:\n","            return [k for k in self.Row[\"primary_keys\"]]\n","        else:\n","            return None\n","        \n","    @property\n","    def PrimaryKey(self) -> str:\n","        \"\"\"\n","        Returns the first instance of primary key. Only used for tables with a single primary key\n","        \"\"\"        \n","        if self.Row[\"primary_keys\"]:\n","            return self.Row[\"primary_keys\"][0]\n","        else:\n","            return None\n","    \n","    @property\n","    def LakehouseTableName(self) -> str:\n","        \"\"\"\n","        Returns the two-part Lakehouse table name\n","        \"\"\"\n","        return \"{0}.{1}\".format(self.Lakehouse, self.DestinationTableName)\n","        \n","    @property\n","    def BQTableName(self) -> str:\n","        \"\"\"\n","        Returns the three-part BigQuery table name\n","        \"\"\"\n","        return \"{0}.{1}.{2}\".format(self.ProjectId, self.Dataset, self.TableName)\n","\n","    @property\n","    def IsTimeIngestionPartitioned(self) -> bool:\n","        \"\"\"\n","        Bool indicator for time ingestion tables\n","        \"\"\"\n","        return self.LoadStrategy == SyncConstants.TIME_INGESTION\n","\n","    @property\n","    def IsTimePartitionedStrategy(self) -> bool:\n","         \"\"\"\n","         Bool indicator for the two time partitioned strategies\n","         \"\"\"\n","         return (self.LoadStrategy == SyncConstants.PARTITION or \\\n","                self.LoadStrategy == SyncConstants.TIME_INGESTION)\n","    \n","    def UpdateRowCounts(\n","            self, \n","            src:int, \n","            dest:int, \n","            insert:int = 0, \n","            update:int = 0):\n","        \"\"\"\n","        Updates the telemetry row counts based on table configuration\n","        \"\"\"\n","        self.SourceRows += src\n","        self.DestRows += dest\n","\n","        if not self.LoadType == SyncConstants.MERGE:\n","            match self.LoadStrategy:\n","                case SyncConstants.WATERMARK:\n","                    self.InsertedRows += src     \n","                case SyncConstants.PARTITION:\n","                    self.InsertedRows += dest  \n","                case _:\n","                    self.InsertedRows += dest\n","            \n","            self.UpdatedRows = 0\n","        else:\n","            self.InsertedRows += insert\n","            self.UpdatedRows += update"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"62248c3f-83de-419e-847f-409e1ddda63b","statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:17:28.5422506Z","session_start_time":null,"execution_start_time":"2024-04-18T13:17:37.6669127Z","execution_finish_time":"2024-04-18T13:17:38.0397458Z","parent_msg_id":"7efcc3de-7783-41ea-ac0d-3c2d8fde6c56"},"text/plain":"StatementMeta(, 62248c3f-83de-419e-847f-409e1ddda63b, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8d13feff-25f1-417c-b31c-cda7efe40150"},{"cell_type":"code","source":["class ConfigDataset:\n","    \"\"\"\n","    User Config class for Big Query project/dataset configuration\n","    \"\"\"\n","    def __init__(\n","            self, \n","            json_config:str):\n","        \"\"\"\n","        Loads from use config JSON\n","        \"\"\"\n","        self.ProjectID = self.get_json_conf_val(json_config, \"project_id\", None)\n","        self.Dataset = self.get_json_conf_val(json_config, \"dataset\", None)\n","        self.LoadAllTables = self.get_json_conf_val(json_config, \"load_all_tables\", True)\n","        self.Autodetect = self.get_json_conf_val(json_config, \"autodetect\", True)\n","        self.MasterReset = self.get_json_conf_val(json_config, \"master_reset\", False)\n","        self.MetadataLakehouse = self.get_json_conf_val(json_config, \"metadata_lakehouse\", None)\n","        self.TargetLakehouse = self.get_json_conf_val(json_config, \"target_lakehouse\", None)\n","        self.Tables = []\n","\n","        if \"gcp_credentials\" in json_config:\n","            self.GCPCredential = ConfigGCPCredential(\n","                self.get_json_conf_val(json_config[\"gcp_credentials\"], \"credential_path\", None),\n","                self.get_json_conf_val(json_config[\"gcp_credentials\"], \"access_token\", None),\n","                self.get_json_conf_val(json_config[\"gcp_credentials\"], \"credential\", None)\n","            )\n","        else:\n","            self.GCPCredential = ConfigGCPCredential()\n","\n","        if \"async\" in json_config:\n","            self.Async = ConfigAsync(\n","                self.get_json_conf_val(json_config[\"async\"], \"enabled\", False),\n","                self.get_json_conf_val(json_config[\"async\"], \"parallelism\", None),\n","                self.get_json_conf_val(json_config[\"async\"], \"notebook_timeout\", None),\n","                self.get_json_conf_val(json_config[\"async\"], \"cell_timeout\", None)\n","            )\n","        else:\n","            self.Async = ConfigAsync()\n","\n","        if \"tables\" in json_config:\n","            for t in json_config[\"tables\"]:\n","                self.Tables.append(ConfigBQTable(t))\n","\n","    @property\n","    def GCPCredentialPath(self):\n","        return self.GCPCredential.CredentialPath\n","    \n","    def get_table_name_list(self) -> list[str]:\n","        \"\"\"\n","        Returns a list of table names from the user configuration\n","        \"\"\"\n","        return [str(x.TableName) for x in self.Tables]\n","\n","    def get_bq_table_fullname(\n","            self, \n","            tbl_name:str) -> str:\n","        \"\"\"\n","        Returns three-part BigQuery table name\n","        \"\"\"\n","        return f\"{self.ProjectID}.{self.Dataset}.{tbl_name}\"\n","\n","    def get_lakehouse_tablename(\n","            self, \n","            lakehouse:str, \n","            tbl_name:str) -> str:\n","        \"\"\"\n","        Reurns two-part Lakehouse table name\n","        \"\"\"\n","        return f\"{lakehouse}.{tbl_name}\"\n","\n","    def flatten_3part_tablename(\n","            self, \n","            tbl_name:str) -> str:\n","        \"\"\"\n","        Replaces special characters in the GCP project name and returns three-part\n","        name with underscores\n","        \"\"\"\n","        clean_project_id = self.ProjectID.replace(\"-\", \"_\")\n","        return f\"{clean_project_id}_{self.Dataset}_{tbl_name}\"\n","    \n","    def get_json_conf_val(\n","            self, \n","            json:str, \n","            config_key:str, \n","            default_val = None):\n","        \"\"\"\n","        Extracts a value from the user config JSON doc by key. If it doesn't\n","        exist the default value is returned\n","        \"\"\"\n","        if config_key in json:\n","            return json[config_key]\n","        else:\n","            return default_val\n","\n","class ConfigGCPCredential:\n","    \"\"\"\n","    GCP Credential model\n","    \"\"\"\n","    def __init__(self, path:str = None, token:str = None, credential:str = None):\n","        self.CredentialPath = path\n","        self.AccessToken = token\n","        self.Credential = credential\n","\n","class ConfigTableMaintenance:\n","    \"\"\"\n","    User Config class for table maintenance\n","    \"\"\"\n","    def __init__(\n","            self, \n","            enabled:bool = False, \n","            interval:str = None):\n","        self.Enabled = enabled\n","        self.Interval = interval\n","\n","class ConfigAsync:\n","    \"\"\"\n","    User Config class for parallelized async loading configuration\n","    \"\"\"\n","    def __init__(\n","            self, \n","            enabled:bool = False, \n","            parallelism:int = 5, \n","            notebook_timeout:int = 1800, \n","            cell_timeout:int = 300):\n","        self.Enabled = enabled\n","        self.Parallelism = parallelism\n","        self.NotebookTimeout = notebook_timeout\n","        self.CellTimeout = cell_timeout\n","\n","class ConfigTableColumn:\n","    \"\"\"\n","    User Config class for Big Query Table table column mapping configuration\n","    \"\"\"\n","    def __init__(\n","            self, \n","            col:str = \"\"):\n","        self.Column = col\n","\n","class ConfigLakehouseTarget:\n","    \"\"\"\n","    User Config class for Big Query Table Lakehouse target mapping configuration\n","    \"\"\"\n","    def __init__(\n","            self, \n","            lakehouse:str = \"\", \n","            table:str = \"\"):\n","        self.Lakehouse = lakehouse\n","        self.Table = table\n","\n","class ConfigPartition:\n","    \"\"\"\n","    User Config class for Big Query Table partition configuration\n","    \"\"\"\n","    def __init__(\n","            self, \n","            enabled:bool = False, \n","            partition_type:str = \"\", \n","            col:ConfigTableColumn = ConfigTableColumn(), \n","            grain:str = \"\"):\n","        self.Enabled = enabled\n","        self.PartitionType = partition_type\n","        self.PartitionColumn = col\n","        self.Granularity = grain\n","\n","class ConfigBQTable:\n","    \"\"\"\n","    User Config class for Big Query Table mapping configuration\n","    \"\"\"\n","    def __str__(self):\n","        return str(self.TableName)\n","\n","    def __init__(\n","            self, \n","            json_config:str):\n","        \"\"\"\n","        Loads from user config JSON object\n","        \"\"\"\n","        self.TableName = self.get_json_conf_val(json_config, \"table_name\", \"\")\n","        self.Priority = self.get_json_conf_val(json_config, \"priority\", 100)\n","        self.SourceQuery = self.get_json_conf_val(json_config, \"source_query\", \"\")\n","        self.LoadStrategy = self.get_json_conf_val(json_config, \"load_strategy\" , SyncConstants.FULL)\n","        self.LoadType = self.get_json_conf_val(json_config, \"load_type\", SyncConstants.OVERWRITE)\n","        self.Interval =  self.get_json_conf_val(json_config, \"interval\", SyncConstants.AUTO)\n","        self.Enabled =  self.get_json_conf_val(json_config, \"enabled\", True)\n","        self.EnforcePartitionExpiration = self.get_json_conf_val(json_config, \"enforce_partition_expiration\", False)\n","        self.EnableDeletionVectors = self.get_json_conf_val(json_config, \"enable_deletion_vectors\", False)\n","        self.AllowSchemaEvolution = self.get_json_conf_val(json_config, \"allow_schema_evoluton\", False)\n","        \n","        if \"lakehouse_target\" in json_config:\n","            self.LakehouseTarget = ConfigLakehouseTarget( \\\n","                self.get_json_conf_val(json_config[\"lakehouse_target\"], \"lakehouse\", \"\"), \\\n","                self.get_json_conf_val(json_config[\"lakehouse_target\"], \"table_name\", \"\"))\n","        else:\n","            self.LakehouseTarget = ConfigLakehouseTarget()\n","        \n","        if \"watermark\" in json_config:\n","            self.Watermark = ConfigTableColumn( \\\n","                self.get_json_conf_val(json_config[\"watermark\"], \"column\", \"\"))\n","        else:\n","            self.Watermark = ConfigTableColumn()\n","\n","        if \"partitioned\" in json_config:\n","            self.Partitioned = ConfigPartition( \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"enabled\", False), \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"type\", \"\"), \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"column\", \"\"), \\\n","                self.get_json_conf_val(json_config[\"partitioned\"], \"partition_grain\", \"\"))\n","        else:\n","            self.Partitioned = ConfigPartition()\n","        \n","        if \"table_maintenance\" in json_config:\n","            self.TableMaintenance = ConfigTableMaintenance( \\\n","                self.get_json_conf_val(json_config[\"table_maintenance\"], \"enabled\", False), \\\n","                self.get_json_conf_val(json_config[\"table_maintenance\"], \"interval\", \"MONTH\"))\n","        else:\n","            self.TableMaintenance = ConfigTableMaintenance()\n","\n","        self.Keys = []\n","\n","        if \"keys\" in json_config:\n","            for c in json_config[\"keys\"]:\n","                self.Keys.append(ConfigTableColumn( \\\n","                    self.get_json_conf_val(c, \"column\", \"\")))\n","        \n","    def get_json_conf_val(\n","            self, \n","            json:str, \n","            config_key:str, \n","            default_val = None):\n","        \"\"\"\n","        Extracts a value from the user config JSON doc by key. If it doesn't\n","        exist the default value is returned\n","        \"\"\"\n","        if config_key in json:\n","            return json[config_key]\n","        else:\n","            return default_val\n","        "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"62248c3f-83de-419e-847f-409e1ddda63b","statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:17:28.6635799Z","session_start_time":null,"execution_start_time":"2024-04-18T13:17:38.5357108Z","execution_finish_time":"2024-04-18T13:17:38.9741394Z","parent_msg_id":"d3473069-c5bc-451d-b980-f7472386c42d"},"text/plain":"StatementMeta(, 62248c3f-83de-419e-847f-409e1ddda63b, 7, Finished, Available)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"73ebafd6-a34b-4cbc-a273-aa933ae3f407"},{"cell_type":"code","source":["class ConfigBase():\n","    '''\n","    Base class for sync objects that require access to user-supplied configuration\n","    '''\n","    def __init__(\n","              self, \n","              config_path:str, \n","              force_reload_config:bool = False):\n","        \"\"\"\n","        Init method loads the user JSON config from the supplied path.\n","        \"\"\"\n","        if config_path is None:\n","            raise ValueError(\"Missing Path to JSON User Config\")\n","\n","        self.ConfigPath = config_path\n","        self.UserConfig = None\n","        self.GCPCredential = None\n","\n","        self.UserConfig = self.ensure_user_config(force_reload_config)\n","\n","        self.GCPCredential = self.load_gcp_credential()\n","    \n","    def ensure_user_config(\n","              self, \n","              reload_config:bool) -> ConfigDataset:\n","        \"\"\"\n","        Load the user JSON config if it hasn't been loaded or \n","        returns the local user config as an ConfigDataset object\n","        \"\"\"\n","        if (self.UserConfig is None or reload_config) and self.ConfigPath is not None:\n","            config = self.load_user_config(self.ConfigPath, reload_config)\n","\n","            cfg = ConfigDataset(config)\n","\n","            self.validate_user_config(cfg)\n","            \n","            return cfg\n","        else:\n","            return self.UserConfig\n","    \n","    def load_user_config(\n","            self, \n","            config_path:str, \n","            reload_config:bool)->str:\n","        \"\"\"\n","        If the spark dataframe is not cached, loads the user config JSON to a dataframe,\n","        caches it, creates a temporary session view and then returns a JSON object\n","        \"\"\"\n","        config_df = None\n","\n","        if not spark.catalog.tableExists(\"user_config_json\") or reload_config:\n","            config_df = spark.read.option(\"multiline\",\"true\").json(config_path)\n","            config_df.createOrReplaceTempView(\"user_config_json\")\n","            config_df.cache()\n","        else:\n","            config_df = spark.table(\"user_config_json\")\n","            \n","        return json.loads(config_df.toJSON().first())\n","\n","    def validate_user_config(\n","            self, \n","            cfg:ConfigDataset) -> bool:\n","        \"\"\"\n","        Validates the user config JSON to make sure all required config is supplied\n","        \"\"\"\n","        if cfg is None:\n","            raise RuntimeError(\"Invalid User Config\")    \n","        \n","        validation_errors = []\n","\n","        if not cfg.ProjectID:\n","            validation_errors.append(\"GCP Project ID missing or empty\")\n","        \n","        if not cfg.Dataset:\n","            validation_errors.append(\"GCP Dataset missing or empty\")\n","\n","        if not cfg.MetadataLakehouse:\n","            validation_errors.append(\"Metadata Lakehouse missing or empty\")\n","        \n","        if not cfg.TargetLakehouse:\n","            validation_errors.append(\"Target Lakehouse missing or empty\")\n","\n","        if not cfg.GCPCredential.CredentialPath and not cfg.GCPCredential.Credential:\n","            validation_errors.append(\"GCP Credentials Path and GCP Credentials cannot both be empty\")\n","        \n","        for t in cfg.Tables:\n","            if not t.TableName:\n","                validation_errors.append(\"Unknown table, table with missing or empty Table Name\")\n","                continue\n","\n","            if t.LoadStrategy and not t.LoadStrategy in SyncConstants.get_load_strategies():\n","                validation_errors.append(f\"Table {t.TableName} has a missing or invalid load strategy\")\n","\n","            if t.LoadType and not t.LoadType in SyncConstants.get_load_types():\n","                validation_errors.append(f\"Table {t.TableName} has a missing or invalid load type\")\n","            \n","            if t.LoadStrategy == SyncConstants.WATERMARK:\n","                if t.Watermark is None or not t.Watermark.Column:\n","                    validation_errors.append(f\"Table {t.TableName} is configured for Watermark but is missing the Watermark column\")\n","\n","    \n","        if not validation_errors:\n","            config_errors = \"\\r\\n\".join(validation_errors)\n","            raise ValueError(f\"Errors in User Config JSON File:\\r\\n{config_errors}\")\n","        \n","        return True\n","\n","    def load_gcp_credential(self) -> str:\n","        \"\"\"\n","        GCP credentials can be supplied as a base64 encoded string or as a path to \n","        the GCP service account JSON credentials. If a path is supplied, the JSON file \n","        is loaded and the contents serialized to a base64 string\n","        \"\"\"\n","        cred = None\n","\n","        if self.is_base64(self.UserConfig.GCPCredential.Credential):\n","            cred = self.UserConfig.GCPCredential.Credential\n","        else:\n","            file_contents = self.read_credential_file()\n","            cred = self.convert_to_base64string(file_contents)\n","            \n","        return cred\n","\n","    def read_credential_file(self) -> str:\n","        \"\"\"\n","        Reads credential file from the Notebook Resource file path\n","        \"\"\"\n","        credential = f\"{mssparkutils.nbResPath}{self.UserConfig.GCPCredential.CredentialPath}\"\n","\n","        if not os.path.exists(credential):\n","           raise ValueError(\"Invalid GCP Credential path supplied.\")\n","        \n","        txt = Path(credential).read_text()\n","        txt = txt.replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n","\n","        return txt\n","\n","    def convert_to_base64string(\n","            self, \n","            credential_val:str) -> str:\n","        \"\"\"\n","        Converts string to base64 encoding, returns ascii value of bytes\n","        \"\"\"\n","        credential_val_bytes = credential_val.encode(\"ascii\") \n","        \n","        base64_bytes = base64.b64encode(credential_val_bytes) \n","        base64_string = base64_bytes.decode(\"ascii\") \n","\n","        return base64_string\n","\n","    def is_base64(\n","            self, \n","            val:str) -> str:\n","        \"\"\"\n","        Evaluates a string to determine if its base64 encoded\n","        \"\"\"\n","        try:\n","                if isinstance(val, str):\n","                        sb_bytes = bytes(val, 'ascii')\n","                elif isinstance(val, bytes):\n","                        sb_bytes = val\n","                else:\n","                        raise ValueError(\"Argument must be string or bytes\")\n","                return base64.b64encode(base64.b64decode(sb_bytes)) == sb_bytes\n","        except Exception:\n","                return False\n","\n","    def read_bq_partition_to_dataframe(\n","            self, \n","            table:str, \n","            partition_filter:str, \n","            cache_results:bool=False) -> DataFrame:\n","        \"\"\"\n","        Reads a specific partition using the BigQuery spark connector.\n","        BigQuery does not support table decorator so the table and partition info \n","        is passed using options\n","        \"\"\"\n","        df = spark.read \\\n","            .format(\"bigquery\") \\\n","            .option(\"parentProject\", self.UserConfig.ProjectID) \\\n","            .option(\"credentials\", self.GCPCredential) \\\n","            .option(\"viewsEnabled\", \"true\") \\\n","            .option(\"materializationDataset\", self.UserConfig.Dataset) \\\n","            .option(\"table\", table) \\\n","            .option(\"filter\", partition_filter) \\\n","            .load()\n","        \n","        if cache_results:\n","            df.cache()\n","        \n","        return df\n","\n","    def read_bq_to_dataframe(\n","            self, \n","            query:str, \n","            cache_results:bool=False) -> DataFrame:\n","        \"\"\"\n","        Reads a BigQuery table using the BigQuery spark connector\n","        \"\"\"\n","        df = spark.read \\\n","            .format(\"bigquery\") \\\n","            .option(\"parentProject\", self.UserConfig.ProjectID) \\\n","            .option(\"credentials\", self.GCPCredential) \\\n","            .option(\"viewsEnabled\", \"true\") \\\n","            .option(\"materializationDataset\", self.UserConfig.Dataset) \\\n","            .load(query)\n","        \n","        if cache_results:\n","            df.cache()\n","        \n","        return df\n","\n","    def write_lakehouse_table(\n","            self, \n","            df:DataFrame, \n","            lakehouse:str, \n","            tbl_nm:str, \n","            mode:str=SyncConstants.OVERWRITE):\n","        \"\"\"\n","        Write a DataFrame to the lakehouse using the Lakehouse.TableName notation\n","        \"\"\"\n","        dest_table = self.UserConfig.get_lakehouse_tablename(lakehouse, tbl_nm)\n","\n","        df.write \\\n","            .mode(mode) \\\n","            .saveAsTable(dest_table)\n","    \n","    def create_infosys_proxy_view(\n","            self, \n","            trgt:str,\n","            refresh:bool = False):\n","        \"\"\"\n","        Creates a covering temporary view over top of the Big Query metadata tables\n","        \"\"\"\n","        clean_nm = trgt.replace(\".\", \"_\")\n","        vw_nm = f\"BQ_{clean_nm}\"\n","\n","        if not spark.catalog.tableExists(vw_nm) or refresh:\n","            tbl = self.UserConfig.flatten_3part_tablename(clean_nm)\n","            lakehouse_tbl = self.UserConfig.get_lakehouse_tablename(self.UserConfig.MetadataLakehouse, tbl)\n","\n","            sql = f\"\"\"\n","            CREATE OR REPLACE TEMPORARY VIEW {vw_nm}\n","            AS\n","            SELECT *\n","            FROM {lakehouse_tbl}\n","            \"\"\"\n","            spark.sql(sql)\n","\n","    def create_userconfig_tables_proxy_view(self):\n","        \"\"\"\n","        Explodes the User Config table configuration into a temporary view\n","        \"\"\"\n","        sql = \"\"\"\n","            CREATE OR REPLACE TEMPORARY VIEW user_config_tables\n","            AS\n","            SELECT\n","                project_id, dataset, tbl.table_name,\n","                tbl.enabled,tbl.load_priority,tbl.source_query,\n","                tbl.load_strategy,tbl.load_type,tbl.interval,\n","                tbl.watermark.column as watermark_column,\n","                tbl.partitioned.enabled as partition_enabled,\n","                tbl.partitioned.type as partition_type,\n","                tbl.partitioned.column as partition_column,\n","                tbl.partitioned.partition_grain,\n","                tbl.lakehouse_target.lakehouse,\n","                tbl.lakehouse_target.table_name AS lakehouse_target_table,\n","                tbl.keys\n","            FROM (SELECT project_id, dataset, EXPLODE(tables) AS tbl FROM user_config_json)\n","        \"\"\"\n","        spark.sql (sql)\n","\n","    def create_userconfig_tables_cols_proxy_view(self):\n","        \"\"\"\n","        Explodes the User Config table primary keys into a temporary view\n","        \"\"\"\n","        sql = \"\"\"\n","            CREATE OR REPLACE TEMPORARY VIEW user_config_table_keys\n","            AS\n","            SELECT\n","                project_id, dataset, table_name, pkeys.column\n","            FROM (\n","                SELECT\n","                    project_id, dataset, tbl.table_name, EXPLODE(tbl.keys) AS pkeys\n","                FROM (SELECT project_id, dataset, EXPLODE(tables) AS tbl FROM user_config_json)\n","            )\n","        \"\"\"\n","        spark.sql(sql)\n","\n","    def create_proxy_views(\n","            self, \n","            refresh:bool = False):\n","        \"\"\"\n","        Create the user config and covering BQ information schema views\n","        \"\"\"\n","        if not spark.catalog.tableExists(\"user_config_tables\") or refresh:\n","            self.create_userconfig_tables_proxy_view()\n","        \n","        if not spark.catalog.tableExists(\"user_config_table_keys\") or refresh:\n","            self.create_userconfig_tables_cols_proxy_view()\n","\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_TABLES, refresh)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_PARTITIONS, refresh)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_COLUMNS, refresh)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_TABLE_CONSTRAINTS, refresh)\n","        self.create_infosys_proxy_view(SyncConstants.INFORMATION_SCHEMA_KEY_COLUMN_USAGE, refresh)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"62248c3f-83de-419e-847f-409e1ddda63b","statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","queued_time":"2024-04-18T13:22:44.9096307Z","session_start_time":null,"execution_start_time":"2024-04-18T13:22:45.3517779Z","execution_finish_time":"2024-04-18T13:22:45.7218481Z","parent_msg_id":"b5fd4dd4-9b1f-4696-8c76-2bad79c4c47e"},"text/plain":"StatementMeta(, 62248c3f-83de-419e-847f-409e1ddda63b, 14, Finished, Available)"},"metadata":{}}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1c49f796-9eba-4acf-8fe6-e82c5b01907f"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"}},"nbformat":4,"nbformat_minor":5}